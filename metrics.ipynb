{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gigaam-rnnt to instantiate a model of type gigaam. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# load audio\n",
    "wav, sr = torchaudio.load(\"audio.wav\")\n",
    "# resample if necessary\n",
    "wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "# load model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"waveletdeboshir/gigaam-rnnt\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"waveletdeboshir/gigaam-rnnt\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "input_features = processor(wav[0], sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "# greedy prediction\n",
    "with torch.no_grad():\n",
    "    pred_ids = model.generate(**input_features)\n",
    "\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(pred_ids)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print(f\"Размер модели (вычислено): {size_all_mb:.2f} MB\")\n",
    "    return size_all_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер модели (вычислено): 893.62 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "893.6179275512695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка процессора waveletdeboshir/gigaam-rnnt...\n",
      "Загрузка модели waveletdeboshir/gigaam-rnnt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gigaam-rnnt to instantiate a model of type gigaam. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель перемещена на устройство: cuda\n",
      "Метрики WER и CER загружены.\n",
      "Загрузка датасета mozilla-foundation/common_voice_16_1, конфигурация: ru, сплит: test...\n",
      "Загружено 100 сэмплов для оценки.\n",
      "Аудио в датасете преобразовано к 16000 Hz.\n",
      "\n",
      "Начало оценки качества на 100 сэмплах для waveletdeboshir/gigaam-rnnt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка качества: 100%|██████████| 100/100 [00:27<00:00,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Качество модели (Word Error Rate - WER) для waveletdeboshir/gigaam-rnnt: 0.3041\n",
      "Качество модели (Character Error Rate - CER) для waveletdeboshir/gigaam-rnnt: 0.0882\n",
      "\n",
      "Примеры транскрипций:\n",
      "Эталон  : масштабы финансово-экономического кризиса и темпы его распространения застали самых опытных специалистов мира врасплох.\n",
      "Предсказание: масштабы финансово экономического кризиса и темпы его распространения застали самых опытных специалистов мира врасплох\n",
      "\n",
      "Эталон  : к сожалению, эти предложения не нашли отражения в тексте.\n",
      "Предсказание: к сожалению эти предложения не нашли отражения в тексте\n",
      "\n",
      "Эталон  : мы настоятельно призываем посла танина незамедлительно провести неофициальное пленарное заседание для обсуждения реформы совета.\n",
      "Предсказание: мы настоятельно призываем посла танина незамедлительно провести неофициальное пленарное заседание для обсуждения реформы совета\n",
      "\n",
      "Эталон  : толпа озвереет, будет тереться, ощетинит ножки стоглавая вошь.\n",
      "Предсказание: толпа зверет будет тереться ощетинит ножки стоглавая вошь\n",
      "\n",
      "Эталон  : за это время международное сообщество стало свидетелем окончания периода деспотии и репрессий в ливии.\n",
      "Предсказание: за это время международное собщество стало свидетелем окончания периода деспоти и репресий в ливи\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import time\n",
    "import psutil\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate # Для метрик WER и CER\n",
    "import os\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Конфигурация ---\n",
    "MODEL_ID = \"waveletdeboshir/gigaam-rnnt\" # <--- ИЗМЕНЕНО: ID новой модели\n",
    "DATASET_ID = \"mozilla-foundation/common_voice_16_1\"\n",
    "DATASET_NAME = \"ru\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "NUM_SAMPLES_FOR_QUALITY_TEST = 100\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "\n",
    "# --- 1. Загрузка модели и процессора ---\n",
    "print(f\"Загрузка процессора {MODEL_ID}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "print(f\"Загрузка модели {MODEL_ID}...\")\n",
    "model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# --- 2. Определение устройства и перемещение модели ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() # Переводим модель в режим оценки\n",
    "print(f\"Модель перемещена на устройство: {device}\")\n",
    "\n",
    "# --- 3. Загрузка метрик WER и CER ---\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "print(\"Метрики WER и CER загружены.\")\n",
    "\n",
    "# --- 4. Загрузка и подготовка датасета ---\n",
    "print(f\"Загрузка датасета {DATASET_ID}, конфигурация: {DATASET_NAME}, сплит: {DATASET_SPLIT}...\")\n",
    "dataset = load_dataset(DATASET_ID, DATASET_NAME, split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_QUALITY_TEST}]\", trust_remote_code=True)\n",
    "print(f\"Загружено {len(dataset)} сэмплов для оценки.\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "print(f\"Аудио в датасете преобразовано к {TARGET_SAMPLE_RATE} Hz.\")\n",
    "\n",
    "# --- 5. Функция для инференса и декодирования (АДАПТИРОВАНА ДЛЯ RNN-T) ---\n",
    "def transcribe_audio(batch):\n",
    "    audio_input = batch[\"audio\"]\n",
    "    raw_audio = audio_input[\"array\"]\n",
    "    sampling_rate = audio_input[\"sampling_rate\"]\n",
    "\n",
    "    if len(raw_audio) == 0:\n",
    "        return {\"reference_transcription\": batch[\"sentence\"], \"predicted_transcription\": \"\"}\n",
    "\n",
    "    waveform_tensor = torch.tensor(raw_audio).unsqueeze(0)\n",
    "    \n",
    "    # Входные данные для RNN-T модели готовятся процессором и сразу перемещаются на нужное устройство\n",
    "    input_features = processor(waveform_tensor[0], sampling_rate=sampling_rate, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Используем model.generate() для RNN-T моделей\n",
    "        # input_features уже на нужном устройстве (device)\n",
    "        pred_ids = model.generate(**input_features) \n",
    "    \n",
    "    # Декодируем ID токенов в текст\n",
    "    # ВАЖНО: используем skip_special_tokens=True, чтобы убрать специальные токены (например, <pad>, <s>, </s>)\n",
    "    # pred_ids обычно имеет форму (batch_size, sequence_length), для одного сэмпла (1, sequence_length)\n",
    "    transcription = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return {\"reference_transcription\": batch[\"sentence\"], \"predicted_transcription\": transcription.strip()} # Добавил .strip() для удаления возможных пробелов по краям\n",
    "\n",
    "# --- 6. Оценка качества на датасете ---\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"\\nНачало оценки качества на {NUM_SAMPLES_FOR_QUALITY_TEST} сэмплах для {MODEL_ID}...\")\n",
    "for i in tqdm(range(len(dataset)), desc=\"Оценка качества\"):\n",
    "    sample = dataset[i]\n",
    "    \n",
    "    if not sample[\"sentence\"] or sample[\"sentence\"].strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        result = transcribe_audio(sample)\n",
    "        \n",
    "        predictions.append(result[\"predicted_transcription\"].lower())\n",
    "        references.append(result[\"reference_transcription\"].lower())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке сэмпла {i} ({sample.get('path', 'N/A')}): {e}\")\n",
    "        # Можно добавить пустые строки или пропустить, чтобы не сломать расчет метрики\n",
    "        # predictions.append(\"\") \n",
    "        # references.append(sample[\"sentence\"].lower() if sample[\"sentence\"] else \"ошибка_при_обработке\")\n",
    "\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# --- 7. Расчет и вывод метрик WER и CER ---\n",
    "if predictions and references:\n",
    "    # Перед расчетом метрик убедимся, что у нас одинаковое количество предсказаний и эталонов\n",
    "    if len(predictions) != len(references):\n",
    "        print(f\"Внимание: Количество предсказаний ({len(predictions)}) не совпадает с количеством эталонов ({len(references)}). Проверьте наличие ошибок.\")\n",
    "        # Можно попытаться выровнять, но лучше разобраться в причине\n",
    "        min_len = min(len(predictions), len(references))\n",
    "        predictions = predictions[:min_len]\n",
    "        references = references[:min_len]\n",
    "\n",
    "    if not predictions: # Если после фильтрации ничего не осталось\n",
    "        print(\"\\nНе удалось собрать предсказания для расчета WER/CER.\")\n",
    "    else:\n",
    "        wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "        cer_score = cer_metric.compute(predictions=predictions, references=references)\n",
    "        print(f\"\\nКачество модели (Word Error Rate - WER) для {MODEL_ID}: {wer_score:.4f}\")\n",
    "        print(f\"Качество модели (Character Error Rate - CER) для {MODEL_ID}: {cer_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nНе удалось собрать предсказания и/или эталоны для расчета WER/CER.\")\n",
    "\n",
    "print(\"\\nПримеры транскрипций:\")\n",
    "for i in range(min(5, len(predictions))): # Убедимся, что predictions не пустой\n",
    "    print(f\"Эталон  : {references[i] if i < len(references) else 'N/A'}\")\n",
    "    print(f\"Предсказание: {predictions[i] if i < len(predictions) else 'N/A'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка процессора waveletdeboshir/gigaam-rnnt...\n",
      "Загрузка модели waveletdeboshir/gigaam-rnnt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gigaam-rnnt to instantiate a model of type gigaam. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель и процессор загружены.\n",
      "Подготовка 25 сэмплов для замеров...\n",
      "Сэмплы подготовлены.\n",
      "\n",
      "--- Измерение на GPU ---\n",
      "Прогрев GPU (5 запусков)...\n",
      "Замеры времени инференса GPU (20 запусков)...\n",
      "Среднее время инференса на GPU: 88.79 ms\n",
      "Пиковое использование VRAM (измерено на первом реальном запуске): 2158.08 MB\n",
      "\n",
      "--- Измерение на CPU ---\n",
      "Прогрев CPU (5 запусков)...\n",
      "Замеры времени инференса CPU (20 запусков)...\n",
      "Среднее время инференса на CPU: 836.99 ms\n",
      "Использование RAM (после инференсов на CPU): 3056.15 MB\n",
      "(RAM до прогрева CPU: 3036.06 MB, после прогрева CPU: 3056.30 MB)\n",
      "\n",
      "--- Итоговые результаты для таблицы ---\n",
      "Модель: waveletdeboshir/gigaam-rnnt\n",
      "Время инференса (CPU, ms): 836.99\n",
      "Время инференса (GPU, ms): 88.79\n",
      "Использование RAM (MB): 3056.15\n",
      "Использование VRAM (MB): 2158.08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# --- Конфигурация ---\n",
    "MODEL_ID = \"waveletdeboshir/gigaam-rnnt\" # <--- ИЗМЕНЕНО: ID новой модели\n",
    "DATASET_ID = \"mozilla-foundation/common_voice_16_1\"\n",
    "DATASET_NAME = \"ru\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "NUM_SAMPLES_FOR_TIME_TEST = 20\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "NUM_WARMUP_RUNS = 5\n",
    "\n",
    "# --- 1. Загрузка модели и процессора ---\n",
    "print(f\"Загрузка процессора {MODEL_ID}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "print(f\"Загрузка модели {MODEL_ID}...\")\n",
    "model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model.eval() \n",
    "print(\"Модель и процессор загружены.\")\n",
    "\n",
    "# --- 2. Подготовка сэмплов ---\n",
    "print(f\"Подготовка {NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS} сэмплов для замеров...\")\n",
    "dataset_for_timing = load_dataset(\n",
    "    DATASET_ID, \n",
    "    DATASET_NAME, \n",
    "    split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS}]\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "dataset_for_timing = dataset_for_timing.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "\n",
    "# Сохраняем уже обработанные процессором данные (на CPU)\n",
    "processed_inputs_list = []\n",
    "for i in range(NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS):\n",
    "    sample_audio = dataset_for_timing[i][\"audio\"][\"array\"]\n",
    "    waveform_tensor = torch.tensor(sample_audio).unsqueeze(0) # (1, num_samples)\n",
    "    # Процессор вызывается один раз для каждого сэмпла\n",
    "    input_features = processor(waveform_tensor[0], sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\")\n",
    "    processed_inputs_list.append(input_features)\n",
    "print(\"Сэмплы подготовлены.\")\n",
    "\n",
    "# --- Переменные для результатов ---\n",
    "time_gpu_ms_avg = \"N/A\"\n",
    "vram_usage_mb = \"N/A\"\n",
    "\n",
    "# --- 3. Измерение на GPU (если доступно) ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n--- Измерение на GPU ---\")\n",
    "    device_gpu = torch.device(\"cuda\")\n",
    "    model.to(device_gpu)\n",
    "    \n",
    "    print(f\"Прогрев GPU ({NUM_WARMUP_RUNS} запусков)...\")\n",
    "    for i in range(NUM_WARMUP_RUNS):\n",
    "        # Перемещаем конкретный input_features на GPU перед использованием\n",
    "        warmup_input = {k: v.to(device_gpu) for k, v in processed_inputs_list[i].items()}\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**warmup_input) # <--- ИЗМЕНЕНО: используем model.generate()\n",
    "            \n",
    "    torch.cuda.synchronize(device_gpu) # Убедимся, что прогрев завершен\n",
    "    torch.cuda.reset_peak_memory_stats(device_gpu) # Сбрасываем счетчик пиковой памяти ПОСЛЕ прогрева\n",
    "    \n",
    "    gpu_times = []\n",
    "    print(f\"Замеры времени инференса GPU ({NUM_SAMPLES_FOR_TIME_TEST} запусков)...\")\n",
    "    # Память измеряется для первого реального замера, т.к. reset_peak_memory_stats был до цикла\n",
    "    # Если нужно измерять для каждого, то reset и get нужно делать внутри цикла (но это обычно не требуется)\n",
    "    \n",
    "    # Первый замер для VRAM\n",
    "    current_input_for_vram = {k: v.to(device_gpu) for k, v in processed_inputs_list[NUM_WARMUP_RUNS].items()}\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**current_input_for_vram)\n",
    "    torch.cuda.synchronize(device_gpu)\n",
    "    vram_usage_mb = torch.cuda.max_memory_allocated(device_gpu) / (1024 * 1024)\n",
    "    del current_input_for_vram # Освобождаем память от этого инпута\n",
    "    if NUM_SAMPLES_FOR_TIME_TEST == 0: # Если только прогрев\n",
    "         time_gpu_ms_avg = 0\n",
    "\n",
    "    for i in range(NUM_WARMUP_RUNS, NUM_WARMUP_RUNS + NUM_SAMPLES_FOR_TIME_TEST):\n",
    "        current_input = {k: v.to(device_gpu) for k, v in processed_inputs_list[i].items()}\n",
    "        torch.cuda.synchronize(device_gpu) # Синхронизация перед стартом таймера\n",
    "        start_time = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**current_input) # <--- ИЗМЕНЕНО: используем model.generate()\n",
    "        torch.cuda.synchronize(device_gpu) # Синхронизация после завершения операций на GPU\n",
    "        end_time = time.perf_counter()\n",
    "        gpu_times.append((end_time - start_time) * 1000) # Время в миллисекундах\n",
    "        \n",
    "    if gpu_times:\n",
    "        time_gpu_ms_avg = sum(gpu_times) / len(gpu_times)\n",
    "    \n",
    "    print(f\"Среднее время инференса на GPU: {time_gpu_ms_avg if isinstance(time_gpu_ms_avg, str) else f'{time_gpu_ms_avg:.2f}'} ms\")\n",
    "    print(f\"Пиковое использование VRAM (измерено на первом реальном запуске): {vram_usage_mb:.2f} MB\")\n",
    "    \n",
    "    del warmup_input, current_input # Очистка переменных\n",
    "    model.to(\"cpu\") # Перемещаем модель обратно на CPU\n",
    "    torch.cuda.empty_cache() # Очищаем кэш CUDA\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"CUDA (GPU) не доступна. Пропуск измерений на GPU.\")\n",
    "\n",
    "# --- 4. Измерение на CPU ---\n",
    "print(\"\\n--- Измерение на CPU ---\")\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "model.to(device_cpu) # Убедимся, что модель на CPU\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "ram_before_cpu_warmup_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Прогрев CPU ({NUM_WARMUP_RUNS} запусков)...\")\n",
    "for i in range(NUM_WARMUP_RUNS):\n",
    "    # processed_inputs_list уже на CPU\n",
    "    warmup_input = processed_inputs_list[i] \n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**warmup_input) # <--- ИЗМЕНЕНО: используем model.generate()\n",
    "\n",
    "cpu_times = []\n",
    "print(f\"Замеры времени инференса CPU ({NUM_SAMPLES_FOR_TIME_TEST} запусков)...\")\n",
    "# Измеряем RAM после прогрева, перед основным циклом замеров\n",
    "ram_after_cpu_warmup_mb = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "for i in range(NUM_WARMUP_RUNS, NUM_WARMUP_RUNS + NUM_SAMPLES_FOR_TIME_TEST):\n",
    "    current_input = processed_inputs_list[i]\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**current_input) # <--- ИЗМЕНЕНО: используем model.generate()\n",
    "    end_time = time.perf_counter()\n",
    "    cpu_times.append((end_time - start_time) * 1000)\n",
    "\n",
    "if cpu_times:\n",
    "    time_cpu_ms_avg = sum(cpu_times) / len(cpu_times)\n",
    "else:\n",
    "    time_cpu_ms_avg = \"N/A\" # или 0, если NUM_SAMPLES_FOR_TIME_TEST = 0\n",
    "\n",
    "# Финальное измерение RAM после всех операций\n",
    "ram_usage_mb = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "print(f\"Среднее время инференса на CPU: {time_cpu_ms_avg if isinstance(time_cpu_ms_avg, str) else f'{time_cpu_ms_avg:.2f}'} ms\")\n",
    "print(f\"Использование RAM (после инференсов на CPU): {ram_usage_mb:.2f} MB\")\n",
    "print(f\"(RAM до прогрева CPU: {ram_before_cpu_warmup_mb:.2f} MB, после прогрева CPU: {ram_after_cpu_warmup_mb:.2f} MB)\")\n",
    "\n",
    "\n",
    "# --- 5. Итоговые результаты для таблицы ---\n",
    "print(\"\\n--- Итоговые результаты для таблицы ---\")\n",
    "print(f\"Модель: {MODEL_ID}\")\n",
    "print(f\"Время инференса (CPU, ms): {time_cpu_ms_avg if isinstance(time_cpu_ms_avg, str) else f'{time_cpu_ms_avg:.2f}'}\")\n",
    "print(f\"Время инференса (GPU, ms): {time_gpu_ms_avg if isinstance(time_gpu_ms_avg, str) else f'{time_gpu_ms_avg:.2f}'}\")\n",
    "print(f\"Использование RAM (MB): {ram_usage_mb:.2f}\") # Финальное значение\n",
    "print(f\"Использование VRAM (MB): {vram_usage_mb if isinstance(vram_usage_mb, str) else f'{vram_usage_mb:.2f}'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
