{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Возможно ли, что супергерои появятся в реальной жизни? Мог бы это и быть? Супергерои? Что имеется ввиду под супергероями? Это же генетическая тема, ну то есть... Типа то, что скрафтят усиленных людей? Ну конечно это возможно. Ну типа да, это возможно. Грубо говоря, усиленные люди это будут те, у кого скорее всего будет возможность купить это. Ну прям как в кино супергерои, ну блин, прям такого не будет конечно. Ну я думаю, в будущем могут развить там какую-то у кого-то усиленную регенерацию Или усиленную скорость, силу, блять Так даже сейчас же, ну вот есть те же самые астероиды и прочее Ты видел, как некоторые люди выглядят? То есть они выглядят нереалистично абсолютно То есть такого представить нельзя, такого быть в принципе не может То, как некоторые выглядят То есть если посмотреть, там какие-то фото атлетов, допустим, лет 50 или 100 назад То есть, ну, обманывают природу Прямо сейчас уже Субтитры сделал DimaTorzok\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "mp3_file_path = \"sample.wav\"\n",
    "\n",
    "\n",
    "result = pipe(mp3_file_path)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print(f\"Размер модели (вычислено): {size_all_mb:.2f} MB\")\n",
    "    return size_all_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер модели (вычислено): 2943.97 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2943.974609375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda, тип данных: torch.float16\n",
      "Загрузка процессора для openai/whisper-large-v3...\n",
      "Загрузка модели openai/whisper-large-v3...\n",
      "Модель и процессор загружены.\n",
      "Метрики WER и CER загружены.\n",
      "Загрузка датасета mozilla-foundation/common_voice_16_1, конфигурация: ru, сплит: test...\n",
      "Загружено 100 сэмплов для оценки.\n",
      "Аудио в датасете преобразовано к 16000 Hz.\n",
      "\n",
      "Начало оценки качества на 100 сэмплах для openai/whisper-large-v3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка качества:   0%|          | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Оценка качества: 100%|██████████| 100/100 [00:59<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Расчет метрик для 100 успешно обработанных сэмплов.\n",
      "Качество модели (Word Error Rate - WER) для openai/whisper-large-v3: 0.1714\n",
      "Качество модели (Character Error Rate - CER) для openai/whisper-large-v3: 0.0713\n",
      "\n",
      "Примеры транскрипций:\n",
      "Эталон     : масштабы финансово-экономического кризиса и темпы его распространения застали самых опытных специалистов мира врасплох.\n",
      "Предсказание: масштабы финансово-экономического кризиса и темпового распространения застали самых опытных специалистов мира врасплох.\n",
      "\n",
      "Эталон     : к сожалению, эти предложения не нашли отражения в тексте.\n",
      "Предсказание: к сожалению, эти предложения не носили отражения в тексте.\n",
      "\n",
      "Эталон     : мы настоятельно призываем посла танина незамедлительно провести неофициальное пленарное заседание для обсуждения реформы совета.\n",
      "Предсказание: мы настоятельно призываем посла тонино незамедлительно провести неофициальное пленарное заседание для обсуждения реформы совета.\n",
      "\n",
      "Эталон     : толпа озвереет, будет тереться, ощетинит ножки стоглавая вошь.\n",
      "Предсказание: толпа звереет, будет тереться, щетинит ножки стоглавая вошь.\n",
      "\n",
      "Эталон     : за это время международное сообщество стало свидетелем окончания периода деспотии и репрессий в ливии.\n",
      "Предсказание: за это время международное сообщество стало свидетелем окончания периода господии и репрессий в ливии.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor # Изменено AutoModel\n",
    "import time\n",
    "import psutil\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "import os\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Конфигурация ---\n",
    "MODEL_ID = \"openai/whisper-large-v3\" # <--- ИЗМЕНЕНО: ID вашей новой модели\n",
    "DATASET_ID = \"mozilla-foundation/common_voice_16_1\" # Датасет для оценки\n",
    "DATASET_NAME = \"ru\" # Используем русскую часть Common Voice\n",
    "DATASET_SPLIT = \"test\"\n",
    "NUM_SAMPLES_FOR_QUALITY_TEST = 100 # Количество сэмплов для быстрой оценки, можно увеличить\n",
    "TARGET_SAMPLE_RATE = 16000 # Whisper ожидает 16kHz\n",
    "\n",
    "# --- Определение устройства и типа данных ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Whisper v3 хорошо работает с float16 на GPU для ускорения и экономии памяти\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Используемое устройство: {device}, тип данных: {torch_dtype}\")\n",
    "\n",
    "# --- 1. Загрузка модели и процессора ---\n",
    "print(f\"Загрузка процессора для {MODEL_ID}...\")\n",
    "# AutoProcessor для Whisper загрузит и feature_extractor, и tokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(f\"Загрузка модели {MODEL_ID}...\")\n",
    "# Используем AutoModelForSpeechSeq2Seq для моделей Whisper\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True, # Полезно для больших моделей\n",
    "    use_safetensors=True   # Более безопасный и часто быстрый формат загрузки\n",
    ")\n",
    "model.to(device) # Перемещаем модель на выбранное устройство\n",
    "model.eval() # Переводим модель в режим оценки\n",
    "print(\"Модель и процессор загружены.\")\n",
    "\n",
    "# --- 2. Загрузка метрик WER и CER ---\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "print(\"Метрики WER и CER загружены.\")\n",
    "\n",
    "# --- 3. Загрузка и подготовка датасета ---\n",
    "print(f\"Загрузка датасета {DATASET_ID}, конфигурация: {DATASET_NAME}, сплит: {DATASET_SPLIT}...\")\n",
    "# Загружаем указанное количество сэмплов\n",
    "dataset = load_dataset(DATASET_ID, DATASET_NAME, split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_QUALITY_TEST}]\", trust_remote_code=True)\n",
    "print(f\"Загружено {len(dataset)} сэмплов для оценки.\")\n",
    "# Приводим аудио в датасете к нужной частоте дискретизации (16kHz для Whisper)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "print(f\"Аудио в датасете преобразовано к {TARGET_SAMPLE_RATE} Hz.\")\n",
    "\n",
    "# --- 4. Функция для инференса и декодирования (АДАПТИРОВАНА ДЛЯ WHISPER) ---\n",
    "def transcribe_audio_whisper(batch_audio_sample):\n",
    "    raw_audio = batch_audio_sample[\"array\"]\n",
    "    sampling_rate = batch_audio_sample[\"sampling_rate\"] # Должно быть TARGET_SAMPLE_RATE\n",
    "\n",
    "    if len(raw_audio) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    # Процессор Whisper готовит input_features из сырого аудио\n",
    "    # Передаем сырой аудио массив и его частоту дискретизации\n",
    "    input_features = processor(raw_audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    \n",
    "    # Перемещаем input_features на то же устройство и с тем же dtype, что и модель\n",
    "    input_features = input_features.to(device, dtype=torch_dtype)\n",
    "\n",
    "    # Для многоязычных моделей Whisper важно указать язык для декодирования,\n",
    "    # чтобы получить наилучшее качество на конкретном языке.\n",
    "    # `processor.get_decoder_prompt_ids` помогает сформировать нужные токены для `generate`.\n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Генерируем последовательность токенов\n",
    "        predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    \n",
    "    # Декодируем ID токенов в текст.\n",
    "    # `skip_special_tokens=True` убирает специальные токены (например, токены языка, тишины).\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription.strip()\n",
    "\n",
    "# --- 5. Оценка качества на датасете ---\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"\\nНачало оценки качества на {NUM_SAMPLES_FOR_QUALITY_TEST} сэмплах для {MODEL_ID}...\")\n",
    "for i in tqdm(range(len(dataset)), desc=\"Оценка качества\"):\n",
    "    sample = dataset[i]\n",
    "    reference_text = sample[\"sentence\"]\n",
    "\n",
    "    # Пропускаем сэмплы с пустыми эталонными транскрипциями\n",
    "    if not reference_text or reference_text.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Используем функцию, адаптированную для Whisper\n",
    "        predicted_text = transcribe_audio_whisper(sample[\"audio\"])\n",
    "        \n",
    "        # Приводим к нижнему регистру для консистентного сравнения\n",
    "        predictions.append(predicted_text.lower())\n",
    "        references.append(reference_text.lower())\n",
    "\n",
    "    except Exception as e:\n",
    "        audio_path_info = sample.get('path', f'индекс {i}') # Пытаемся получить путь, если есть\n",
    "        print(f\"Ошибка при обработке сэмпла {audio_path_info}: {e}\")\n",
    "        # В случае ошибки можно добавить пустую строку или специальный маркер,\n",
    "        # но лучше пропустить, чтобы не искажать сильно метрику, если ошибок мало.\n",
    "        # Для данного скрипта, если ошибка, пара не добавится, и количество обработанных будет меньше.\n",
    "\n",
    "    # Периодическая очистка памяти GPU, если используется CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# --- 6. Расчет и вывод метрик WER и CER ---\n",
    "if predictions and references:\n",
    "    # Убедимся, что количество предсказаний и эталонов совпадает\n",
    "    # (на случай, если какие-то сэмплы были пропущены из-за ошибок или пустых эталонов)\n",
    "    num_evaluated_samples = len(predictions)\n",
    "    print(f\"\\nРасчет метрик для {num_evaluated_samples} успешно обработанных сэмплов.\")\n",
    "\n",
    "    if num_evaluated_samples > 0:\n",
    "        wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "        cer_score = cer_metric.compute(predictions=predictions, references=references)\n",
    "        print(f\"Качество модели (Word Error Rate - WER) для {MODEL_ID}: {wer_score:.4f}\")\n",
    "        print(f\"Качество модели (Character Error Rate - CER) для {MODEL_ID}: {cer_score:.4f}\")\n",
    "    else:\n",
    "        print(\"Не было собрано ни одного валидного предсказания для расчета метрик.\")\n",
    "else:\n",
    "    print(\"\\nНе удалось собрать предсказания и/или эталоны для расчета WER/CER.\")\n",
    "\n",
    "# Вывод нескольких примеров для визуальной оценки\n",
    "print(\"\\nПримеры транскрипций:\")\n",
    "for i in range(min(5, len(predictions))):\n",
    "    print(f\"Эталон     : {references[i]}\")\n",
    "    print(f\"Предсказание: {predictions[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda, тип данных: torch.float16\n",
      "Загрузка процессора для openai/whisper-large-v3...\n",
      "Загрузка модели openai/whisper-large-v3...\n",
      "Модель и процессор загружены.\n",
      "Загрузка 25 аудио сэмплов для замеров...\n",
      "Подготовка аудиоданных...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Подготовка аудио: 100%|██████████| 25/25 [00:00<00:00, 161.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аудио сэмплы подготовлены.\n",
      "\n",
      "--- Измерение на GPU ---\n",
      "Прогрев GPU (5 запусков)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Замеры времени инференса GPU (20 запусков)...\n",
      "Среднее время инференса на GPU: 383.90 ms\n",
      "Пиковое использование VRAM (измерено на первом реальном запуске): 6146.68 MB\n",
      "\n",
      "--- Измерение на CPU ---\n",
      "Модель переведена в float32 для CPU-тестирования.\n",
      "Модель на CPU с dtype: torch.float32\n",
      "Прогрев CPU (5 запусков)...\n",
      "Замеры времени инференса CPU (20 запусков)...\n",
      "Среднее время инференса на CPU: 4504.25 ms\n",
      "Использование RAM (после инференсов на CPU): 7772.48 MB\n",
      "(RAM до прогрева CPU: 7679.34 MB, после прогрева CPU: 7772.51 MB)\n",
      "\n",
      "--- Итоговые результаты для таблицы ---\n",
      "Модель: openai/whisper-large-v3\n",
      "Время инференса (CPU, ms): 4504.25\n",
      "Время инференса (GPU, ms): 383.90\n",
      "Использование RAM (MB): 7772.48\n",
      "Использование VRAM (MB): 6146.68\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor # Изменено AutoModel\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from datasets import load_dataset, Audio\n",
    "from tqdm.auto import tqdm # Добавим для наглядности подготовки сэмплов\n",
    "\n",
    "# --- Конфигурация ---\n",
    "MODEL_ID = \"openai/whisper-large-v3\" # <--- ИЗМЕНЕНО: ID вашей новой модели\n",
    "DATASET_ID = \"mozilla-foundation/common_voice_16_1\"\n",
    "DATASET_NAME = \"ru\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "NUM_SAMPLES_FOR_TIME_TEST = 20\n",
    "TARGET_SAMPLE_RATE = 16000 # Whisper ожидает 16kHz\n",
    "NUM_WARMUP_RUNS = 5\n",
    "\n",
    "# --- Определение устройства и типа данных ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 # float16 для GPU\n",
    "print(f\"Используемое устройство: {device}, тип данных: {torch_dtype}\")\n",
    "\n",
    "# --- 1. Загрузка модели и процессора ---\n",
    "print(f\"Загрузка процессора для {MODEL_ID}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(f\"Загрузка модели {MODEL_ID}...\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device) # Сразу перемещаем на основное устройство (GPU или CPU)\n",
    "model.eval()\n",
    "print(\"Модель и процессор загружены.\")\n",
    "\n",
    "# Подготовка forced_decoder_ids для русского языка (для GPU и CPU инференса)\n",
    "# Это нужно делать один раз, так как они не зависят от конкретного аудиосэмпла\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "\n",
    "# --- 2. Подготовка сэмплов ---\n",
    "# Сэмплы будут готовиться как сырые аудио массивы,\n",
    "# так как Whisper процессор принимает их напрямую.\n",
    "# Предобработка в input_features будет происходить непосредственно перед инференсом.\n",
    "print(f\"Загрузка {NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS} аудио сэмплов для замеров...\")\n",
    "dataset_for_timing = load_dataset(\n",
    "    DATASET_ID,\n",
    "    DATASET_NAME,\n",
    "    split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS}]\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "dataset_for_timing = dataset_for_timing.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "\n",
    "# Сохраняем сырые аудиоданные\n",
    "raw_audio_list = []\n",
    "print(f\"Подготовка аудиоданных...\")\n",
    "for i in tqdm(range(NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS), desc=\"Подготовка аудио\"):\n",
    "    raw_audio_list.append(dataset_for_timing[i][\"audio\"][\"array\"])\n",
    "print(\"Аудио сэмплы подготовлены.\")\n",
    "\n",
    "\n",
    "# --- Переменные для результатов ---\n",
    "time_gpu_ms_avg = \"N/A\"\n",
    "vram_usage_mb = \"N/A\"\n",
    "\n",
    "# --- 3. Измерение на GPU (если доступно) ---\n",
    "if device.type == 'cuda': # Проверяем основное устройство\n",
    "    print(\"\\n--- Измерение на GPU ---\")\n",
    "    # Модель уже на GPU и в нужном dtype\n",
    "    \n",
    "    print(f\"Прогрев GPU ({NUM_WARMUP_RUNS} запусков)...\")\n",
    "    for i in range(NUM_WARMUP_RUNS):\n",
    "        raw_audio = raw_audio_list[i]\n",
    "        # Предобработка процессором и перемещение на GPU\n",
    "        input_features = processor(raw_audio, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(device, dtype=torch_dtype)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "            \n",
    "    torch.cuda.synchronize(device) # Убедимся, что прогрев завершен\n",
    "    torch.cuda.reset_peak_memory_stats(device) # Сбрасываем счетчик пиковой памяти ПОСЛЕ прогрева\n",
    "    \n",
    "    gpu_times = []\n",
    "    print(f\"Замеры времени инференса GPU ({NUM_SAMPLES_FOR_TIME_TEST} запусков)...\")\n",
    "    \n",
    "    # Первый реальный замер для VRAM\n",
    "    raw_audio_for_vram = raw_audio_list[NUM_WARMUP_RUNS]\n",
    "    input_features_for_vram = processor(raw_audio_for_vram, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(device, dtype=torch_dtype)\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_features_for_vram, forced_decoder_ids=forced_decoder_ids)\n",
    "    torch.cuda.synchronize(device)\n",
    "    vram_usage_mb = torch.cuda.max_memory_allocated(device) / (1024 * 1024)\n",
    "    del input_features_for_vram # Освобождаем память\n",
    "    if NUM_SAMPLES_FOR_TIME_TEST == 0:\n",
    "         time_gpu_ms_avg = 0 # или \"N/A\"\n",
    "\n",
    "    for i in range(NUM_WARMUP_RUNS, NUM_WARMUP_RUNS + NUM_SAMPLES_FOR_TIME_TEST):\n",
    "        raw_audio = raw_audio_list[i]\n",
    "        input_features = processor(raw_audio, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(device, dtype=torch_dtype)\n",
    "        \n",
    "        torch.cuda.synchronize(device)\n",
    "        start_time = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "        torch.cuda.synchronize(device)\n",
    "        end_time = time.perf_counter()\n",
    "        gpu_times.append((end_time - start_time) * 1000)\n",
    "        del input_features # Освобождаем память после каждого инференса\n",
    "        \n",
    "    if gpu_times:\n",
    "        time_gpu_ms_avg = sum(gpu_times) / len(gpu_times)\n",
    "    \n",
    "    print(f\"Среднее время инференса на GPU: {time_gpu_ms_avg if isinstance(time_gpu_ms_avg, str) else f'{time_gpu_ms_avg:.2f}'} ms\")\n",
    "    print(f\"Пиковое использование VRAM (измерено на первом реальном запуске): {vram_usage_mb if isinstance(vram_usage_mb, str) else f'{vram_usage_mb:.2f}'} MB\")\n",
    "    \n",
    "    # Модель остается на GPU для возможного CPU-тестирования, если torch_dtype был изменен для CPU\n",
    "    # Но если CPU-тест идет после GPU-теста, модель нужно переместить на CPU\n",
    "    # gc.collect() и torch.cuda.empty_cache() полезны\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"CUDA (GPU) не доступна. Пропуск измерений на GPU.\")\n",
    "\n",
    "# --- 4. Измерение на CPU ---\n",
    "print(\"\\n--- Измерение на CPU ---\")\n",
    "# Перемещаем модель на CPU и устанавливаем dtype=float32, если она была на GPU с float16\n",
    "# Если изначально device был 'cpu', то модель уже там и в float32 (или как загрузилась)\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.to(cpu_device) # Гарантированно перемещаем на CPU\n",
    "if torch_dtype == torch.float16 and device.type == \"cuda\": # Если модель была на GPU в float16\n",
    "    model = model.to(torch.float32) # Переводим в float32 для CPU, т.к. CPU float16 медленный\n",
    "    print(\"Модель переведена в float32 для CPU-тестирования.\")\n",
    "current_cpu_dtype = model.dtype # Узнаем текущий dtype модели на CPU\n",
    "print(f\"Модель на CPU с dtype: {current_cpu_dtype}\")\n",
    "\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "ram_before_cpu_warmup_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Прогрев CPU ({NUM_WARMUP_RUNS} запусков)...\")\n",
    "for i in range(NUM_WARMUP_RUNS):\n",
    "    raw_audio = raw_audio_list[i]\n",
    "    # Предобработка процессором (результат на CPU) и приведение к dtype модели\n",
    "    input_features = processor(raw_audio, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(dtype=current_cpu_dtype)\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "cpu_times = []\n",
    "print(f\"Замеры времени инференса CPU ({NUM_SAMPLES_FOR_TIME_TEST} запусков)...\")\n",
    "ram_after_cpu_warmup_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "for i in range(NUM_WARMUP_RUNS, NUM_WARMUP_RUNS + NUM_SAMPLES_FOR_TIME_TEST):\n",
    "    raw_audio = raw_audio_list[i]\n",
    "    input_features = processor(raw_audio, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(dtype=current_cpu_dtype)\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    end_time = time.perf_counter()\n",
    "    cpu_times.append((end_time - start_time) * 1000)\n",
    "    del input_features # Освобождаем память\n",
    "\n",
    "if cpu_times:\n",
    "    time_cpu_ms_avg = sum(cpu_times) / len(cpu_times)\n",
    "else:\n",
    "    time_cpu_ms_avg = \"N/A\"\n",
    "\n",
    "ram_usage_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Среднее время инференса на CPU: {time_cpu_ms_avg if isinstance(time_cpu_ms_avg, str) else f'{time_cpu_ms_avg:.2f}'} ms\")\n",
    "print(f\"Использование RAM (после инференсов на CPU): {ram_usage_mb:.2f} MB\")\n",
    "print(f\"(RAM до прогрева CPU: {ram_before_cpu_warmup_mb:.2f} MB, после прогрева CPU: {ram_after_cpu_warmup_mb:.2f} MB)\")\n",
    "\n",
    "\n",
    "# --- 5. Итоговые результаты для таблицы ---\n",
    "print(\"\\n--- Итоговые результаты для таблицы ---\")\n",
    "print(f\"Модель: {MODEL_ID}\")\n",
    "print(f\"Время инференса (CPU, ms): {time_cpu_ms_avg if isinstance(time_cpu_ms_avg, str) else f'{time_cpu_ms_avg:.2f}'}\")\n",
    "print(f\"Время инференса (GPU, ms): {time_gpu_ms_avg if isinstance(time_gpu_ms_avg, str) else f'{time_gpu_ms_avg:.2f}'}\")\n",
    "print(f\"Использование RAM (MB): {ram_usage_mb:.2f}\")\n",
    "print(f\"Использование VRAM (MB): {vram_usage_mb if isinstance(vram_usage_mb, str) else f'{vram_usage_mb:.2f}'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
