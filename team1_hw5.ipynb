{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e735a4d",
      "metadata": {
        "id": "8e735a4d"
      },
      "outputs": [],
      "source": [
        "# !pip install torch transformers datasets evaluate onnx onnxruntime openvino-dev psutil pandas\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18c52196",
      "metadata": {
        "id": "18c52196"
      },
      "outputs": [],
      "source": [
        "import os, time, psutil, json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import onnxruntime as ort\n",
        "\n",
        "# CUDA‑флаг\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "DEVICE_CPU = torch.device('cpu')\n",
        "DEVICE_GPU = torch.device('cuda' if USE_GPU else 'cpu')\n",
        "\n",
        "# для повтора эксперимента\n",
        "N_WARMUP, N_RUNS = 10, 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3be24b7",
      "metadata": {
        "id": "c3be24b7"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, device='cpu', n_runs=N_RUNS):\n",
        "    model.eval()\n",
        "    dummy = torch.randn(1, 3, 224, 224, device=device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(N_WARMUP):\n",
        "            _ = model(dummy)\n",
        "        if device != 'cpu':\n",
        "            torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(n_runs):\n",
        "            _ = model(dummy)\n",
        "        if device != 'cpu':\n",
        "            torch.cuda.synchronize()\n",
        "    return (time.time() - start) * 1e3 / n_runs  # ms\n",
        "\n",
        "_process = psutil.Process(os.getpid())\n",
        "\n",
        "def rss_mb() -> float:\n",
        "    return _process.memory_info().rss / 1024 ** 2\n",
        "\n",
        "def vram_mb() -> float:\n",
        "    return torch.cuda.max_memory_allocated() / 1024 ** 2\n",
        "\n",
        "def file_mb(path: str | Path) -> float:\n",
        "    return Path(path).stat().st_size / 1024 ** 2\n",
        "\n",
        "def reset_vram():\n",
        "    if USE_GPU:\n",
        "        torch.cuda.reset_max_memory_allocated()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d0d195",
      "metadata": {
        "id": "14d0d195",
        "outputId": "713e3cf6-5ed0-4c73-fa1a-f2d4b112420a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/kazanplova/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 370MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f08e738",
      "metadata": {
        "id": "9f08e738"
      },
      "source": [
        "#### TorchScript\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc730fe8",
      "metadata": {
        "id": "bc730fe8"
      },
      "outputs": [],
      "source": [
        "ts_path = \"resnet50_ts.pt\"\n",
        "ts_model = torch.jit.trace(model.to(DEVICE_CPU), torch.randn(1, 3, 224, 224))\n",
        "ts_model.save(ts_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c137e6de",
      "metadata": {
        "id": "c137e6de"
      },
      "source": [
        "#### ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df45b502",
      "metadata": {
        "id": "df45b502"
      },
      "outputs": [],
      "source": [
        "onnx_path = \"resnet50.onnx\"\n",
        "torch.onnx.export(\n",
        "    model.to(DEVICE_CPU), torch.randn(1, 3, 224, 224),\n",
        "    onnx_path, input_names=['input'], output_names=['output'], opset_version=13\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a471f20a",
      "metadata": {
        "id": "a471f20a"
      },
      "source": [
        "#### TensorRT (fp16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89e61b4",
      "metadata": {
        "id": "f89e61b4"
      },
      "outputs": [],
      "source": [
        "from torch2trt import torch2trt\n",
        "trt_path = \"resnet50_trt.pth\"\n",
        "trt_model = torch2trt(\n",
        "    model.to(DEVICE_GPU), [torch.randn(1, 3, 224, 224, device=DEVICE_GPU)],\n",
        "    fp16_mode=True\n",
        ")\n",
        "torch.save(trt_model.state_dict(), trt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32762394",
      "metadata": {
        "id": "32762394"
      },
      "source": [
        "#### OpenVINO (из ONNX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1030a0",
      "metadata": {
        "id": "4f1030a0"
      },
      "outputs": [],
      "source": [
        "from openvino.runtime import Core, serialize\n",
        "\n",
        "os.makedirs(\"openvino_model\", exist_ok=True)\n",
        "ie = Core()\n",
        "ov = ie.read_model(onnx_path)\n",
        "serialize(ov, \"openvino_model/model.xml\", \"openvino_model/model.bin\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ff9911",
      "metadata": {
        "id": "a6ff9911"
      },
      "source": [
        "### benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f4ebe3",
      "metadata": {
        "id": "b9f4ebe3"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "pt_cpu_time = measure_inference_time(model.to(DEVICE_CPU), 'cpu')\n",
        "results.append([\"PyTorch-CPU\", pt_cpu_time, None, rss_mb(), None, \"n/a\"])\n",
        "\n",
        "if USE_GPU:\n",
        "    reset_vram()\n",
        "    pt_gpu_time = measure_inference_time(model.to(DEVICE_GPU), 'cuda')\n",
        "    results.append([\"PyTorch-GPU\", None, pt_gpu_time, None, vram_mb(), \"n/a\"])\n",
        "\n",
        "m = torch.jit.load(ts_path).to(DEVICE_CPU)\n",
        "ts_cpu_time = measure_inference_time(m, 'cpu')\n",
        "results.append([\"TorchScript-CPU\", ts_cpu_time, None, rss_mb(), None, file_mb(ts_path)])\n",
        "\n",
        "if USE_GPU:\n",
        "    reset_vram()\n",
        "    ts_gpu_time = measure_inference_time(m.to(DEVICE_GPU), 'cuda')\n",
        "    results.append([\"TorchScript-GPU\", None, ts_gpu_time, None, vram_mb(), file_mb(ts_path)])\n",
        "\n",
        "def onnx_run(sess):\n",
        "    data = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
        "    sess.run(None, {sess.get_inputs()[0].name: data})\n",
        "\n",
        "sess_cpu = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "for _ in range(N_WARMUP): onnx_run(sess_cpu)\n",
        "start = time.time()\n",
        "for _ in range(N_RUNS): onnx_run(sess_cpu)\n",
        "onnx_cpu_time = (time.time() - start) * 1e3 / N_RUNS\n",
        "results.append([\"ONNX-CPU\", onnx_cpu_time, None, rss_mb(), None, file_mb(onnx_path)])\n",
        "\n",
        "if USE_GPU:\n",
        "    sess_gpu = ort.InferenceSession(onnx_path, providers=[\"CUDAExecutionProvider\"])\n",
        "    reset_vram()\n",
        "    for _ in range(N_WARMUP): onnx_run(sess_gpu)\n",
        "    start = time.time()\n",
        "    for _ in range(N_RUNS): onnx_run(sess_gpu)\n",
        "    onnx_gpu_time = (time.time() - start) * 1e3 / N_RUNS\n",
        "    results.append([\"ONNX-GPU\", None, onnx_gpu_time, None, vram_mb(), file_mb(onnx_path)])\n",
        "\n",
        "reset_vram()\n",
        "trt_time = measure_inference_time(trt_model, 'cuda')\n",
        "results.append([\"TensorRT-GPU\", None, trt_time, None, vram_mb(), file_mb(trt_path)])\n",
        "\n",
        "compiled = ie.compile_model(ov, \"CPU\")\n",
        "def ov_run():\n",
        "    compiled([np.random.randn(1, 3, 224, 224).astype(np.float32)])\n",
        "\n",
        "for _ in range(N_WARMUP): ov_run()\n",
        "start = time.time()\n",
        "for _ in range(N_RUNS): ov_run()\n",
        "ov_time = (time.time() - start) * 1e3 / N_RUNS\n",
        "results.append([\n",
        "    \"OpenVINO-CPU\", ov_time, None, rss_mb(), None,\n",
        "    file_mb(\"openvino_model/model.xml\") + file_mb(\"openvino_model/model.bin\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cccd6bd",
      "metadata": {
        "id": "8cccd6bd"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6b704f",
      "metadata": {
        "id": "ec6b704f",
        "outputId": "fa58f9d5-1b74-4d67-c995-d305ce1cf32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"Format\": \"PyTorch-CPU\",\n",
            "    \"Inf_CPU_ms\": 23.090620040893555,\n",
            "    \"Inf_GPU_ms\": null,\n",
            "    \"RAM_MB\": 7664.28515625,\n",
            "    \"VRAM_MB\": null,\n",
            "    \"File_MB\": \"n/a\"\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"PyTorch-GPU\",\n",
            "    \"Inf_CPU_ms\": null,\n",
            "    \"Inf_GPU_ms\": 3.9195919036865234,\n",
            "    \"RAM_MB\": null,\n",
            "    \"VRAM_MB\": 119.12890625,\n",
            "    \"File_MB\": \"n/a\"\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"TorchScript-CPU\",\n",
            "    \"Inf_CPU_ms\": 23.486652374267578,\n",
            "    \"Inf_GPU_ms\": null,\n",
            "    \"RAM_MB\": 7683.74609375,\n",
            "    \"VRAM_MB\": null,\n",
            "    \"File_MB\": 98.06046485900879\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"TorchScript-GPU\",\n",
            "    \"Inf_CPU_ms\": null,\n",
            "    \"Inf_GPU_ms\": 2.9832839965820312,\n",
            "    \"RAM_MB\": null,\n",
            "    \"VRAM_MB\": 216.85693359375,\n",
            "    \"File_MB\": 98.06046485900879\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"ONNX-CPU\",\n",
            "    \"Inf_CPU_ms\": 8.958115577697754,\n",
            "    \"Inf_GPU_ms\": null,\n",
            "    \"RAM_MB\": 7869.72265625,\n",
            "    \"VRAM_MB\": null,\n",
            "    \"File_MB\": 97.41437149047852\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"ONNX-GPU\",\n",
            "    \"Inf_CPU_ms\": null,\n",
            "    \"Inf_GPU_ms\": 8.751931190490723,\n",
            "    \"RAM_MB\": null,\n",
            "    \"VRAM_MB\": 205.5302734375,\n",
            "    \"File_MB\": 97.41437149047852\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"TensorRT-GPU\",\n",
            "    \"Inf_CPU_ms\": null,\n",
            "    \"Inf_GPU_ms\": 0.5286788940429688,\n",
            "    \"RAM_MB\": null,\n",
            "    \"VRAM_MB\": 206.1083984375,\n",
            "    \"File_MB\": 75.17973709106445\n",
            "  },\n",
            "  {\n",
            "    \"Format\": \"OpenVINO-CPU\",\n",
            "    \"Inf_CPU_ms\": 10.177736282348633,\n",
            "    \"Inf_GPU_ms\": null,\n",
            "    \"RAM_MB\": 8131.95703125,\n",
            "    \"VRAM_MB\": null,\n",
            "    \"File_MB\": 97.7283582687378\n",
            "  }\n",
            "]\n",
            "Самый быстрый GPU‑вариант: TensorRT-GPU\n",
            "Самый быстрый CPU‑вариант: ONNX-CPU\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "cols = [\"Format\", \"Inf_CPU_ms\", \"Inf_GPU_ms\", \"RAM_MB\", \"VRAM_MB\", \"File_MB\"]\n",
        "json_results = [dict(zip(cols, row)) for row in results]\n",
        "\n",
        "print(json.dumps(json_results, indent=2, ensure_ascii=False))\n",
        "\n",
        "gpu_candidates = [d for d in json_results if d[\"Inf_GPU_ms\"]]\n",
        "cpu_candidates = [d for d in json_results if d[\"Inf_CPU_ms\"]]\n",
        "\n",
        "best_gpu = min(gpu_candidates, key=lambda d: d[\"Inf_GPU_ms\"])[\"Format\"] if gpu_candidates else \"—\"\n",
        "best_cpu = min(cpu_candidates, key=lambda d: d[\"Inf_CPU_ms\"])[\"Format\"] if cpu_candidates else \"—\"\n",
        "\n",
        "print(f\"Самый быстрый GPU‑вариант: {best_gpu}\")\n",
        "print(f\"Самый быстрый CPU‑вариант: {best_cpu}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comfyui",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}