# Дистилляция знаний модели

В рамках данной ДЗ были взяты 2 модели одного семейства:
- efficientvit_b3 - модель учитель
- efficientvit_b1 - модель ученик

Основная цель данной дз была в сохранении максимального качества модели при снижении её размера и получения ускорения от этого.

## Сравнение моделей (учитель vs ученик)

| Параметр              | Учитель | Ученик | Изменение   |
|-----------------------|---------|--------|-------------|
| Размер (mB)           | 185.75  | 34.77  | в 5.34 раза |
| Скорость (ms)         | 21.26   | 11.10  | в 1.92 раза |
| Качество (pretrained) | 82.39   | 78.74  | -3.65       |

Как видно из данной таблицы оптимизация модели получается неплохая при смене модели в том же семействе.

## Процесс дистилляции
Для дистилляции моделей был выбран Response-Based метод, тк он не предполагает модификации архитектуры. В качестве функции потерь использовалась дивергенция Кульбака-Лейблера, которая применялась к логитам после softmax'a с температурой.

Тренировочная часть исходного датасета была разбита на 2 части, одна из которых использовалась для обучения, а другая для валидации модели.

### Проблемы при дистиляции
Была проведена серия экспериментов для улучшения качества исходной модели ученика, но основная проблема, которая возникала при обучении - переобучение модели (лосс на трейне падал, а на валидации выходил на плато)

Чтобы избежать переобучения были опробованы подходы ([ноутбук с обучением](notebooks/efficient-vit-distillation_version_3.ipynb)):
- Добавление регуляризации
- Добавление сильных аугментаций (повороты, блюр, чб, вырезание кропов)
- Добавление слоя dropout в голову классификации
- Регулирование кол-ва эпох и разный lr
- Изменение баланса лосса дистилляции и классификации

*P.S. также был опробован подход с обучением модели с нуля, а не с pretrain'а, но это оказалось долго и безрезультатно* ([ноутбук с обучением](notebooks/efficient-vit-distillation_full_train.ipynb))

**По итогам экспериментов не получилось увеличить качество ученика.** *Основная гипотеза, которая выдвигается по этому поводу*: претрейн был сделан на всём ImageNet и там из моделей уже выжали максимум под этот датасет. В текущих попытках мы используем подмножество исходного датасета, то есть показываем ему часть данных, которые он и так уже видел при обучении, что заставляет модель забывать те общие знания (который обобщали датасет), которые она успела накопить и концентрироваться на маленькой части данных, что вызывает переобучение.
