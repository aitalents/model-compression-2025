{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda, тип данных: torch.float16\n",
      "Загрузка модели openai/whisper-large-v3...\n",
      "Модель загружена.\n",
      "Загрузка процессора для openai/whisper-large-v3...\n",
      "Процессор загружен.\n",
      "Метрики WER и CER загружены.\n",
      "forced_decoder_ids подготовлены.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor \n",
    "import time\n",
    "import psutil\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "import os\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "# --- Конфигурация (основные параметры остаются) ---\n",
    "MODEL_ID = \"openai/whisper-large-v3\"\n",
    "DATASET_ID = \"mozilla-foundation/common_voice_16_1\"\n",
    "DATASET_NAME = \"ru\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "CALIBRATION_SPLIT = \"train\" \n",
    "NUM_SAMPLES_FOR_QUALITY_TEST = 50\n",
    "NUM_SAMPLES_FOR_TIME_TEST = 10\n",
    "NUM_WARMUP_RUNS = 3\n",
    "NUM_CALIBRATION_SAMPLES = 20 \n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Используемое устройство: {device}, тип данных: {torch_dtype}\")\n",
    "\n",
    "\n",
    "print(f\"Загрузка модели {MODEL_ID}...\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Модель загружена.\")\n",
    "\n",
    "\n",
    "print(f\"Загрузка процессора для {MODEL_ID}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"Процессор загружен.\")\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "print(\"Метрики WER и CER загружены.\")\n",
    "\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "print(\"forced_decoder_ids подготовлены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Оценка для openai/whisper-large-v3 на cuda:0 с dtype torch.float16 ---\n",
      "Загрузка датасета mozilla-foundation/common_voice_16_1 для оценки качества (test split)...\n",
      "Загрузка датасета mozilla-foundation/common_voice_16_1 для замеров производительности (test split)...\n",
      "\n",
      "Проведение оценки качества...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка качества:   0%|          | 0/50 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Оценка качества: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество (WER): 0.1468\n",
      "Качество (CER): 0.0490\n",
      "\n",
      "Проведение замеров производительности...\n",
      "\n",
      "--- Результаты FP16 (или FP32 на CPU) ---\n",
      "Модель: openai/whisper-large-v3\n",
      "Dtype: torch.float16\n",
      "Устройство: cuda:0\n",
      "Размер модели (прибл. MB): 2940.31\n",
      "Время инференса (CUDA, ms): 536.19\n",
      "Использование VRAM (MB): 3204.42\n",
      "Использование RAM (MB): 1707.11\n",
      "Качество (WER): 0.1468\n",
      "Качество (CER): 0.0490\n",
      "\n",
      "Результаты для таблицы (FP16):\n",
      "{'method': 'FP16', 'model_id': 'openai/whisper-large-v3', 'dtype': 'torch.float16', 'device': 'cuda:0', 'model_size_mb': '2940.31', 'time_ms': '536.19', 'vram_mb': '3204.42', 'ram_mb': '1707.11', 'wer': '0.1468', 'cer': '0.0490'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_model_dtype = model.dtype\n",
    "current_model_device = model.device\n",
    "print(f\"--- Оценка для {MODEL_ID} на {current_model_device} с dtype {current_model_dtype} ---\")\n",
    "\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "\n",
    "\n",
    "def evaluate_quality_whisper(model_to_eval, processor_to_use, dataset_to_eval, num_samples, device_to_use, dtype_to_use, forced_decoder_ids_for_eval):\n",
    "    model_to_eval.eval()\n",
    "    _predictions = []\n",
    "    _references = []\n",
    "    \n",
    "    for i in tqdm(range(min(num_samples, len(dataset_to_eval))), desc=\"Оценка качества\"):\n",
    "        sample = dataset_to_eval[i]\n",
    "        reference_text = sample[\"sentence\"]\n",
    "        if not reference_text or reference_text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        raw_audio = sample[\"audio\"][\"array\"]\n",
    "        sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "        if len(raw_audio) == 0:\n",
    "            _predictions.append(\"\")\n",
    "            _references.append(reference_text.lower())\n",
    "            continue\n",
    "        \n",
    "        input_features = processor_to_use(raw_audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        input_features = input_features.to(device_to_use, dtype=dtype_to_use)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model_to_eval.generate(input_features, forced_decoder_ids=forced_decoder_ids_for_eval)\n",
    "        \n",
    "        predicted_text = processor_to_use.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        _predictions.append(predicted_text.strip().lower())\n",
    "        _references.append(reference_text.lower())\n",
    "        del input_features, predicted_ids\n",
    "        if device_to_use.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    _wer, _cer = \"N/A\", \"N/A\"\n",
    "    if _predictions and _references:\n",
    "        _wer = wer_metric.compute(predictions=_predictions, references=_references)\n",
    "        _cer = cer_metric.compute(predictions=_predictions, references=_references)\n",
    "    return _wer, _cer, _predictions, _references\n",
    "\n",
    "\n",
    "def evaluate_performance_whisper(model_to_eval, processor_to_use, audio_list_for_timing, num_warmup, num_test, device_to_use, dtype_to_use, forced_decoder_ids_for_eval):\n",
    "    model_to_eval.eval()\n",
    "    _times = []\n",
    "    _vram_usage_mb, _ram_usage_mb = \"N/A\", \"N/A\"\n",
    "\n",
    "    for i in range(min(num_warmup, len(audio_list_for_timing))):\n",
    "        raw_audio = audio_list_for_timing[i]\n",
    "        input_features = processor_to_use(raw_audio, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(device_to_use, dtype=dtype_to_use)\n",
    "        with torch.no_grad():\n",
    "            _ = model_to_eval.generate(input_features, forced_decoder_ids=forced_decoder_ids_for_eval)\n",
    "        del input_features\n",
    "        if device_to_use.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "    if device_to_use.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats(device_to_use)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    \n",
    "    ps_process = psutil.Process(os.getpid())\n",
    "    ram_before_mb = ps_process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    for i in range(min(num_test, len(audio_list_for_timing))):\n",
    "        raw_audio = audio_list_for_timing[i]\n",
    "        input_features = processor_to_use(raw_audio, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\").input_features.to(device_to_use, dtype=dtype_to_use)\n",
    "        \n",
    "        if device_to_use.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model_to_eval.generate(input_features, forced_decoder_ids=forced_decoder_ids_for_eval)\n",
    "        \n",
    "        if device_to_use.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        _times.append((end_time - start_time) * 1000) # ms\n",
    "        \n",
    "        if i == 0 and device_to_use.type == 'cuda': \n",
    "            _vram_usage_mb = torch.cuda.max_memory_allocated(device_to_use) / (1024 * 1024)\n",
    "        del input_features\n",
    "        if device_to_use.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    _avg_time_ms = sum(_times) / len(_times) if _times else \"N/A\"\n",
    "    _ram_usage_mb = ps_process.memory_info().rss / (1024 * 1024)\n",
    "    \n",
    "    return _avg_time_ms, _vram_usage_mb if device_to_use.type == 'cuda' else \"N/A\", _ram_usage_mb\n",
    "\n",
    "print(f\"Загрузка датасета {DATASET_ID} для оценки качества ({DATASET_SPLIT} split)...\")\n",
    "quality_dataset = load_dataset(DATASET_ID, DATASET_NAME, split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_QUALITY_TEST}]\", trust_remote_code=True)\n",
    "quality_dataset = quality_dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "\n",
    "print(f\"Загрузка датасета {DATASET_ID} для замеров производительности ({DATASET_SPLIT} split)...\")\n",
    "timing_dataset_raw = load_dataset(DATASET_ID, DATASET_NAME, split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS}]\", trust_remote_code=True)\n",
    "timing_dataset_raw = timing_dataset_raw.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "raw_audio_list_for_timing = [sample[\"audio\"][\"array\"] for sample in timing_dataset_raw]\n",
    "\n",
    "\n",
    "print(\"\\nПроведение оценки качества...\")\n",
    "fp16_wer, fp16_cer, _, _ = evaluate_quality_whisper(model, processor, quality_dataset, NUM_SAMPLES_FOR_QUALITY_TEST, current_model_device, current_model_dtype, forced_decoder_ids)\n",
    "print(f\"Качество (WER): {fp16_wer if isinstance(fp16_wer, str) else fp16_wer:.4f}\")\n",
    "print(f\"Качество (CER): {fp16_cer if isinstance(fp16_cer, str) else fp16_cer:.4f}\")\n",
    "\n",
    "print(\"\\nПроведение замеров производительности...\")\n",
    "fp16_time_ms, fp16_vram_mb, fp16_ram_mb = evaluate_performance_whisper(model, processor, raw_audio_list_for_timing, NUM_WARMUP_RUNS, NUM_SAMPLES_FOR_TIME_TEST, current_model_device, current_model_dtype, forced_decoder_ids)\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "bytes_per_param = 2 if current_model_dtype == torch.float16 else 4 \n",
    "model_size_mb = (num_params * bytes_per_param) / (1024 * 1024)\n",
    "\n",
    "print(\"\\n--- Результаты FP16 (или FP32 на CPU) ---\")\n",
    "print(f\"Модель: {MODEL_ID}\")\n",
    "print(f\"Dtype: {current_model_dtype}\")\n",
    "print(f\"Устройство: {current_model_device}\")\n",
    "print(f\"Размер модели (прибл. MB): {model_size_mb:.2f}\")\n",
    "print(f\"Время инференса ({current_model_device.type.upper()}, ms): {fp16_time_ms if isinstance(fp16_time_ms, str) else f'{fp16_time_ms:.2f}'}\")\n",
    "if current_model_device.type == 'cuda':\n",
    "    print(f\"Использование VRAM (MB): {fp16_vram_mb if isinstance(fp16_vram_mb, str) else f'{fp16_vram_mb:.2f}'}\")\n",
    "print(f\"Использование RAM (MB): {fp16_ram_mb if isinstance(fp16_ram_mb, str) else f'{fp16_ram_mb:.2f}'}\") \n",
    "print(f\"Качество (WER): {fp16_wer if isinstance(fp16_wer, str) else f'{fp16_wer:.4f}'}\")\n",
    "print(f\"Качество (CER): {fp16_cer if isinstance(fp16_cer, str) else f'{fp16_cer:.4f}'}\")\n",
    "\n",
    "\n",
    "results_fp16 = {\n",
    "    \"method\": \"FP16\" if current_model_device.type == 'cuda' else \"FP32 (Baseline CPU)\",\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"dtype\": str(current_model_dtype),\n",
    "    \"device\": str(current_model_device),\n",
    "    \"model_size_mb\": f\"{model_size_mb:.2f}\",\n",
    "    \"time_ms\": f\"{fp16_time_ms:.2f}\" if isinstance(fp16_time_ms, (int, float)) else fp16_time_ms,\n",
    "    \"vram_mb\": f\"{fp16_vram_mb:.2f}\" if isinstance(fp16_vram_mb, (int, float)) else fp16_vram_mb,\n",
    "    \"ram_mb\": f\"{fp16_ram_mb:.2f}\" if isinstance(fp16_ram_mb, (int, float)) else fp16_ram_mb,\n",
    "    \"wer\": f\"{fp16_wer:.4f}\" if isinstance(fp16_wer, (int, float)) else fp16_wer,\n",
    "    \"cer\": f\"{fp16_cer:.4f}\" if isinstance(fp16_cer, (int, float)) else fp16_cer,\n",
    "}\n",
    "\n",
    "print(\"\\nРезультаты для таблицы (FP16):\")\n",
    "print(results_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INT8 Dynamic quantization for openai/whisper-large-v3 (CPU) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка качества: 100%|██████████| 50/50 [01:48<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INT8 Dynamic results ---\n",
      "WER:   0.1519\n",
      "CER:   0.0500\n",
      "Time:  2394.85 ms (CPU)\n",
      "RAM:   71362.42 MB\n",
      "Size:  1752.11 MB\n",
      "\n",
      "Dict for table:\n",
      "{'method': 'INT8 Dynamic (CPU)', 'model_id': 'openai/whisper-large-v3', 'dtype': 'torch.qint8 (dynamic)', 'device': 'cpu', 'model_size_mb': '1752.11', 'time_ms': '2394.85', 'vram_mb': 'N/A', 'ram_mb': '71362.42', 'wer': '0.1519', 'cer': '0.0500'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11702"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- INT8 динамическая квантизация Whisper-large-v3 (CPU) ---\n",
    "print(f\"--- INT8 Dynamic quantization for {MODEL_ID} (CPU) ---\")\n",
    "\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(device_cpu),\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "model_int8.eval()\n",
    "\n",
    "processor_int8 = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "forced_decoder_ids_int8 = processor_int8.get_decoder_prompt_ids(\n",
    "    language=\"russian\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "if \"quality_dataset\" not in locals() or len(quality_dataset) != NUM_SAMPLES_FOR_QUALITY_TEST:\n",
    "    quality_dataset = load_dataset(\n",
    "        DATASET_ID, DATASET_NAME,\n",
    "        split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_QUALITY_TEST}]\",\n",
    "        trust_remote_code=True\n",
    "    ).cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "\n",
    "wer_int8, cer_int8, _, _ = evaluate_quality_whisper(\n",
    "    model_int8,\n",
    "    processor_int8,\n",
    "    quality_dataset,\n",
    "    NUM_SAMPLES_FOR_QUALITY_TEST,\n",
    "    device_cpu,\n",
    "    torch.float32,\n",
    "    forced_decoder_ids_int8\n",
    ")\n",
    "\n",
    "if \"raw_audio_list_for_timing\" not in locals():\n",
    "    timing_dataset_raw = load_dataset(\n",
    "        DATASET_ID, DATASET_NAME,\n",
    "        split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS}]\",\n",
    "        trust_remote_code=True\n",
    "    ).cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "    raw_audio_list_for_timing = [s[\"audio\"][\"array\"] for s in timing_dataset_raw]\n",
    "\n",
    "time_int8_ms, _, ram_int8_mb = evaluate_performance_whisper(\n",
    "    model_int8,\n",
    "    processor_int8,\n",
    "    raw_audio_list_for_timing,\n",
    "    NUM_WARMUP_RUNS,\n",
    "    NUM_SAMPLES_FOR_TIME_TEST,\n",
    "    device_cpu,\n",
    "    torch.float32,\n",
    "    forced_decoder_ids_int8\n",
    ")\n",
    "\n",
    "tmp_path = \"temp_int8_dynamic.pth\"\n",
    "torch.save(model_int8.state_dict(), tmp_path)\n",
    "size_int8_mb = os.path.getsize(tmp_path) / (1024 * 1024)\n",
    "os.remove(tmp_path)\n",
    "\n",
    "print(\"\\n--- INT8 Dynamic results ---\")\n",
    "print(f\"WER:   {wer_int8:.4f}\")\n",
    "print(f\"CER:   {cer_int8:.4f}\")\n",
    "print(f\"Time:  {time_int8_ms:.2f} ms (CPU)\")\n",
    "print(f\"RAM:   {ram_int8_mb:.2f} MB\")\n",
    "print(f\"Size:  {size_int8_mb:.2f} MB\")\n",
    "\n",
    "results_int8_dynamic = {\n",
    "    \"method\": \"INT8 Dynamic (CPU)\",\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"dtype\": \"torch.qint8 (dynamic)\",\n",
    "    \"device\": str(device_cpu),\n",
    "    \"model_size_mb\": f\"{size_int8_mb:.2f}\",\n",
    "    \"time_ms\": f\"{time_int8_ms:.2f}\",\n",
    "    \"vram_mb\": \"N/A\",\n",
    "    \"ram_mb\": f\"{ram_int8_mb:.2f}\",\n",
    "    \"wer\": f\"{wer_int8:.4f}\",\n",
    "    \"cer\": f\"{cer_int8:.4f}\",\n",
    "}\n",
    "print(\"\\nDict for table:\")\n",
    "print(results_int8_dynamic)\n",
    "\n",
    "del model_int8\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Неструктурированный прунинг для openai/whisper-large-v3 ---\n",
      "Загрузка модели openai/whisper-large-v3 для прунинга на cuda с dtype torch.float16...\n",
      "Модель для прунинга загружена.\n",
      "Применение неструктурированного прунинга (amount=0.2)...\n",
      "Прунинг применен и сделан постоянным.\n",
      "\n",
      "Проведение оценки качества (Прореженная модель)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка качества: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество (WER) Прореженная модель: 0.1215\n",
      "Качество (CER) Прореженная модель: 0.0497\n",
      "\n",
      "Проведение замеров производительности (Прореженная модель)...\n",
      "\n",
      "--- Результаты: Неструктурированный прунинг ---\n",
      "Модель: openai/whisper-large-v3\n",
      "Метод: Прунинг (l1_unstructured, amount=0.2)\n",
      "Dtype: torch.float16\n",
      "Устройство: cuda\n",
      "Размер модели (общий, MB): 2940.31\n",
      "Размер модели (эффективный, MB, на осн. ненулевых): 2481.61\n",
      "Время инференса (CUDA, ms): 538.42\n",
      "Использование VRAM (MB): 6151.46\n",
      "Использование RAM (MB): 70791.42\n",
      "Качество (WER): 0.1215\n",
      "Качество (CER): 0.0497\n",
      "\n",
      "Результаты для таблицы (Прунинг):\n",
      "{'method': 'Pruning L1Unstr. (20.0%)', 'model_id': 'openai/whisper-large-v3', 'dtype': 'torch.float16', 'device': 'cuda', 'model_size_mb': '2481.61 (eff.) / 2940.31 (total)', 'time_ms': '538.42', 'vram_mb': '6151.46', 'ram_mb': '70791.42', 'wer': '0.1215', 'cer': '0.0497'}\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "print(f\"--- Неструктурированный прунинг для {MODEL_ID} ---\")\n",
    "\n",
    "\n",
    "PRUNING_AMOUNT = 0.2 \n",
    "LAYERS_TO_PRUNE = [torch.nn.Linear]\n",
    "\n",
    "\n",
    "\n",
    "pruning_device = device \n",
    "pruning_dtype = torch_dtype \n",
    "\n",
    "print(f\"Загрузка модели {MODEL_ID} для прунинга на {pruning_device} с dtype {pruning_dtype}...\")\n",
    "model_to_prune = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=pruning_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(pruning_device)\n",
    "model_to_prune.eval()\n",
    "print(\"Модель для прунинга загружена.\")\n",
    "\n",
    "print(f\"Применение неструктурированного прунинга (amount={PRUNING_AMOUNT})...\")\n",
    "for module_name, module in model_to_prune.named_modules():\n",
    "    for layer_type in LAYERS_TO_PRUNE:\n",
    "        if isinstance(module, layer_type):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=PRUNING_AMOUNT)\n",
    "           \n",
    "            prune.remove(module, 'weight')\n",
    "print(\"Прунинг применен и сделан постоянным.\")\n",
    "\n",
    "if 'quality_dataset' not in locals() or 'raw_audio_list_for_timing' not in locals():\n",
    "    print(\"Перезагрузка датасетов, так как они не найдены в текущем контексте...\")\n",
    "    quality_dataset = load_dataset(DATASET_ID, DATASET_NAME, split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_QUALITY_TEST}]\", trust_remote_code=True)\n",
    "    quality_dataset = quality_dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "    timing_dataset_raw = load_dataset(DATASET_ID, DATASET_NAME, split=f\"{DATASET_SPLIT}[:{NUM_SAMPLES_FOR_TIME_TEST + NUM_WARMUP_RUNS}]\", trust_remote_code=True)\n",
    "    timing_dataset_raw = timing_dataset_raw.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "    raw_audio_list_for_timing = [sample[\"audio\"][\"array\"] for sample in timing_dataset_raw]\n",
    "    print(\"Датасеты перезагружены.\")\n",
    "\n",
    "\n",
    "print(\"\\nПроведение оценки качества (Прореженная модель)...\")\n",
    "pruned_wer, pruned_cer, _, _ = evaluate_quality_whisper(\n",
    "    model_to_prune, processor, quality_dataset, NUM_SAMPLES_FOR_QUALITY_TEST,\n",
    "    pruning_device, pruning_dtype, forced_decoder_ids\n",
    ")\n",
    "print(f\"Качество (WER) Прореженная модель: {pruned_wer if isinstance(pruned_wer, str) else pruned_wer:.4f}\")\n",
    "print(f\"Качество (CER) Прореженная модель: {pruned_cer if isinstance(pruned_cer, str) else pruned_cer:.4f}\")\n",
    "\n",
    "print(\"\\nПроведение замеров производительности (Прореженная модель)...\")\n",
    "pruned_time_ms, pruned_vram_mb, pruned_ram_mb = evaluate_performance_whisper(\n",
    "    model_to_prune, processor, raw_audio_list_for_timing, NUM_WARMUP_RUNS, NUM_SAMPLES_FOR_TIME_TEST,\n",
    "    pruning_device, pruning_dtype, forced_decoder_ids\n",
    ")\n",
    "\n",
    "pruned_num_params = sum(p.numel() for p in model_to_prune.parameters() if p.requires_grad)\n",
    "\n",
    "pruned_nnz_params = 0\n",
    "for name, module in model_to_prune.named_modules():\n",
    "    for layer_type in LAYERS_TO_PRUNE:\n",
    "        if isinstance(module, layer_type):\n",
    "            pruned_nnz_params += torch.count_nonzero(module.weight).item()\n",
    "\n",
    "        elif not any(isinstance(module, lt) for lt in LAYERS_TO_PRUNE) and hasattr(module, 'weight') and module.weight is not None and module.weight.requires_grad:\n",
    "             pruned_nnz_params += module.weight.numel() \n",
    "        if hasattr(module, 'bias') and module.bias is not None and module.bias.requires_grad:\n",
    "            pruned_nnz_params += module.bias.numel()\n",
    "\n",
    "\n",
    "bytes_per_param_pruning = 2 if pruning_dtype == torch.float16 else 4\n",
    "\n",
    "pruned_model_size_mb_effective = (pruned_nnz_params * bytes_per_param_pruning) / (1024 * 1024)\n",
    "\n",
    "pruned_model_size_mb_total = (pruned_num_params * bytes_per_param_pruning) / (1024 * 1024)\n",
    "\n",
    "\n",
    "print(\"\\n--- Результаты: Неструктурированный прунинг ---\")\n",
    "print(f\"Модель: {MODEL_ID}\")\n",
    "print(f\"Метод: Прунинг (l1_unstructured, amount={PRUNING_AMOUNT})\")\n",
    "print(f\"Dtype: {pruning_dtype}\")\n",
    "print(f\"Устройство: {pruning_device}\")\n",
    "print(f\"Размер модели (общий, MB): {pruned_model_size_mb_total:.2f}\")\n",
    "print(f\"Размер модели (эффективный, MB, на осн. ненулевых): {pruned_model_size_mb_effective:.2f}\")\n",
    "print(f\"Время инференса ({pruning_device.type.upper()}, ms): {pruned_time_ms if isinstance(pruned_time_ms, str) else f'{pruned_time_ms:.2f}'}\")\n",
    "if pruning_device.type == 'cuda':\n",
    "    print(f\"Использование VRAM (MB): {pruned_vram_mb if isinstance(pruned_vram_mb, str) else f'{pruned_vram_mb:.2f}'}\")\n",
    "print(f\"Использование RAM (MB): {pruned_ram_mb if isinstance(pruned_ram_mb, str) else f'{pruned_ram_mb:.2f}'}\")\n",
    "print(f\"Качество (WER): {pruned_wer if isinstance(pruned_wer, str) else f'{pruned_wer:.4f}'}\")\n",
    "print(f\"Качество (CER): {pruned_cer if isinstance(pruned_cer, str) else f'{pruned_cer:.4f}'}\")\n",
    "\n",
    "results_pruning = {\n",
    "    \"method\": f\"Pruning L1Unstr. ({PRUNING_AMOUNT*100}%)\",\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"dtype\": str(pruning_dtype),\n",
    "    \"device\": str(pruning_device),\n",
    "    \"model_size_mb\": f\"{pruned_model_size_mb_effective:.2f} (eff.) / {pruned_model_size_mb_total:.2f} (total)\",\n",
    "    \"time_ms\": f\"{pruned_time_ms:.2f}\" if isinstance(pruned_time_ms, (int, float)) else pruned_time_ms,\n",
    "    \"vram_mb\": f\"{pruned_vram_mb:.2f}\" if isinstance(pruned_vram_mb, (int, float)) else pruned_vram_mb,\n",
    "    \"ram_mb\": f\"{pruned_ram_mb:.2f}\" if isinstance(pruned_ram_mb, (int, float)) else pruned_ram_mb,\n",
    "    \"wer\": f\"{pruned_wer:.4f}\" if isinstance(pruned_wer, (int, float)) else pruned_wer,\n",
    "    \"cer\": f\"{pruned_cer:.4f}\" if isinstance(pruned_cer, (int, float)) else pruned_cer,\n",
    "}\n",
    "print(\"\\nРезультаты для таблицы (Прунинг):\")\n",
    "print(results_pruning)\n",
    "\n",
    "# Очистка\n",
    "del model_to_prune\n",
    "gc.collect()\n",
    "if pruning_device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
