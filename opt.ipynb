{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Импорты и «константы»\n",
    "from pathlib import Path, PurePosixPath\n",
    "import time, torch, json, os, gc\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline)\n",
    "from optimum.onnxruntime import (ORTModelForSpeechSeq2Seq,\n",
    "                                 ORTQuantizer, AutoQuantizationConfig)\n",
    "\n",
    "BASE_ID   = \"openai/whisper-large-v3\"      # исходная модель\n",
    "ONNX_DIR  = Path(\"whisper_large_v3_onnx\")   # куда сохраняем export\n",
    "QN_DIR    = Path(\"whisper_large_v3_int8\")   # куда сохраняем INT8\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.float16          # fp16 на 4090 самое выгодное\n",
    "SAMPLES = 10                    # сколько аудио берём для WER/тайминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.7.0+cu126 | GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 2: базовые импорты и GPU инфо ────────────────────────────────────────\n",
    "import torch, time, gc, psutil, os, json, numpy as np\n",
    "from transformers import (AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline,\n",
    "                          GenerationConfig)\n",
    "from datasets import load_dataset, Audio, disable_caching\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "device = \"cuda:0\"\n",
    "print(\"Torch:\", torch.__version__, \"| GPU:\", torch.cuda.get_device_name(0))\n",
    "disable_caching()  # экономим место в ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Взято 30 примеров\n",
      "dict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'])\n"
     ]
    }
   ],
   "source": [
    "# ── Новый кусочек вместо предыдущей Cell-3 ────────────────────────────────\n",
    "from datasets import load_dataset, DownloadConfig, Audio\n",
    "\n",
    "download_config = DownloadConfig(max_retries=10, resume_download=True)\n",
    "\n",
    "stream_ds = load_dataset(\n",
    "    \"librispeech_asr\",\n",
    "    \"clean\",\n",
    "    split=\"test\",\n",
    "    streaming=True,\n",
    "    download_config=download_config,\n",
    ")\n",
    "\n",
    "stream_ds = stream_ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "ds = list(stream_ds.take(30))          # превращаем в список обычных dict’ов\n",
    "print(f\"Взято {len(ds)} примеров\")\n",
    "print(ds[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: helper-функции замера (ИСПРАВЛЕНО v2) ───────────────────────────\n",
    "import time, numpy as np, torch, gc, psutil, os, json\n",
    "from evaluate import load as load_metric\n",
    "from transformers import pipeline\n",
    "\n",
    "# Убедитесь, что 'device' определен глобально перед вызовом asr_pipe\n",
    "# Например: device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# или как в вашей ячейке 2: device = \"cuda:0\"\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def asr_pipe(model, processor, dtype=torch.float16):\n",
    "    # Используем device, определенный для GPU (или CPU, если нет GPU)\n",
    "    current_device = device # Захватываем значение device\n",
    "    print(f\"Создание пайплайна на устройстве: {current_device}\")\n",
    "    return pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        device=current_device,\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "\n",
    "# def evaluate_model(p, dataset, max_items=None):\n",
    "#     refs, preds, lat = [], [], []\n",
    "#     iterable = dataset if max_items is None else dataset[:max_items]\n",
    "#     print(f\"Оценка модели на {p.device}...\") # Добавим лог устройства пайплайна\n",
    "\n",
    "#     for i, sample in enumerate(iterable):\n",
    "#         audio_dict = sample[\"audio\"]\n",
    "\n",
    "#         try:\n",
    "#             wav = audio_dict[\"array\"]\n",
    "#             sr = audio_dict[\"sampling_rate\"]\n",
    "\n",
    "#             if not isinstance(wav, np.ndarray):\n",
    "#                 print(f\"Предупреждение: Ожидался np.ndarray, получен {type(wav)} в sample {i}. Пропускаем.\")\n",
    "#                 continue\n",
    "#             if sr != 16_000:\n",
    "#                  print(f\"Предупреждение: Неожиданная частота дискретизации {sr} != 16000 в sample {i}.\")\n",
    "#                  # Но мы все равно передаем массив как есть, т.к. cast_column должен был отработать\n",
    "\n",
    "#         except KeyError as e:\n",
    "#             print(f\"Ошибка: Отсутствует ключ {e} в audio_dict для sample {i}. Структура данных: {audio_dict.keys()}\")\n",
    "#             continue\n",
    "#         except Exception as e:\n",
    "#             print(f\"Неожиданная ошибка при извлечении аудио из sample {i}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         start = time.time()\n",
    "#         try:\n",
    "#             # --- ИЗМЕНЕНИЕ ЗДЕСЬ ---\n",
    "#             # Убираем sampling_rate=sr, т.к. массив wav уже имеет нужную частоту (16kHz)\n",
    "#             # и пайплайн, похоже, не ожидает этот параметр в данном контексте.\n",
    "#             result = p(wav)\n",
    "#             # --- КОНЕЦ ИЗМЕНЕНИЯ ---\n",
    "#             text = result[\"text\"]\n",
    "#         except Exception as e:\n",
    "#             # Добавим вывод типа пайплайна для диагностики\n",
    "#             print(f\"Ошибка во время выполнения пайплайна ({type(p).__name__}) для sample {i}: {e}\")\n",
    "#             print(f\"Тип wav: {type(wav)}, форма: {wav.shape if isinstance(wav, np.ndarray) else 'N/A'}, sr (из данных): {sr}\")\n",
    "#             # Устанавливаем result в None или пустой dict, чтобы del не падал\n",
    "#             result = None\n",
    "#             text = \"\" # Или другое значение по умолчанию\n",
    "\n",
    "#         lat.append(time.time() - start)\n",
    "\n",
    "#         refs.append(sample[\"text\"].lower())\n",
    "#         preds.append(text.lower())\n",
    "\n",
    "#         # Очистка памяти\n",
    "#         # Проверяем существование result перед удалением\n",
    "#         del wav, audio_dict\n",
    "#         if result is not None:\n",
    "#              del result\n",
    "#         if i % 5 == 0:\n",
    "#              if torch.cuda.is_available() and hasattr(p, 'device') and p.device.type == 'cuda':\n",
    "#                  torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "\n",
    "\n",
    "#     wer_score = None\n",
    "#     if preds and refs:\n",
    "#          valid_preds = [p if isinstance(p, str) else \"\" for p in preds]\n",
    "#          valid_refs = [r if isinstance(r, str) else \"\" for r in refs]\n",
    "#          if valid_preds and valid_refs:\n",
    "#               try:\n",
    "#                   wer_score = wer_metric.compute(predictions=valid_preds, references=valid_refs)\n",
    "#               except Exception as e:\n",
    "#                   print(f\"Ошибка при вычислении WER: {e}\")\n",
    "#                   print(f\"Preds: {valid_preds[:5]}\") # Показать начало списков для отладки\n",
    "#                   print(f\"Refs: {valid_refs[:5]}\")\n",
    "#                   wer_score = -1 # Или другое значение-индикатор ошибки\n",
    "\n",
    "\n",
    "#     return {\n",
    "#         \"wer\": wer_score,\n",
    "#         \"latency_s\": np.mean(lat) if lat else 0,\n",
    "#         \"processed_items\": len(preds)\n",
    "#     }\n",
    "\n",
    "# --- Напоминание ---\n",
    "# После изменения Cell 4, обязательно перезапустите ее,\n",
    "# а затем перезапустите Cell 5 (где создается pipe_fp16 и вызывается evaluate_model).\n",
    "# Затем можно будет переходить к Cell 6 и 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/miniconda3/envs/ort-gpu/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание пайплайна на устройстве: cuda:0\n",
      "Оценка модели (WhisperForConditionalGeneration) на устройстве: cuda:0...\n",
      "Прогревочных запусков: 3, Оценочных запусков: 10\n",
      "Выполнение прогревочных запусков...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прогрев завершен.\n",
      "Начало оценочных запусков (10 сэмплов)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 10/10 оценочных сэмплов...\n",
      "Оценочные запуски завершены.\n",
      "FP16: {'wer': 0.10047846889952153, 'latency_s': 0.6250935133999918, 'latency_std_s': 0.42636415074513684, 'processed_items': 10, 'peak_ram_mb': '796.8', 'peak_vram_mb': '3208.4'}\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 5: FP16 Whisper-large-v3, базовый прогон ─────────────────────────────\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe_fp16 = asr_pipe(model, processor, dtype=torch_dtype)\n",
    "\n",
    "stats_fp16 = evaluate_model(pipe_fp16, ds, max_items=10)\n",
    "print(\"FP16:\", stats_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6 (фикс) ────────────────────────────────────────────────────────────\n",
    "from optimum.onnxruntime import (\n",
    "    ORTModelForSpeechSeq2Seq, ORTQuantizer, AutoQuantizationConfig\n",
    ")\n",
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "export_dir_fp32 = \"whisper_onnx_fp32\"\n",
    "export_dir_int8 = \"whisper_onnx_int8\"\n",
    "\n",
    "# if not Path(export_dir_fp32).exists():\n",
    "#     model_ort = ORTModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)\n",
    "#     shutil.move(model_ort.model_save_dir, export_dir_fp32)\n",
    "# else:\n",
    "#     print(\"ONNX-FP32 уже существует\")\n",
    "\n",
    "# qconfig = AutoQuantizationConfig.avx512_vnni(\n",
    "#     is_static=False,\n",
    "#     per_channel=False,\n",
    "#     operators_to_quantize=[\"MatMul\"]   # ← Квантуем только матмулы\n",
    "# )\n",
    "\n",
    "# onnx_files = list(Path(export_dir_fp32).glob(\"*.onnx\"))\n",
    "# print(\"Квантуем:\", [f.name for f in onnx_files])\n",
    "\n",
    "# os.makedirs(export_dir_int8, exist_ok=True)\n",
    "\n",
    "# for onnx_path in onnx_files:\n",
    "#     quantizer = ORTQuantizer.from_pretrained(export_dir_fp32,\n",
    "#                                              file_name=onnx_path.name)\n",
    "#     quantizer.quantize(\n",
    "#         save_dir            = export_dir_int8,\n",
    "#         quantization_config = qconfig,\n",
    "#     )\n",
    "\n",
    "# for fname in [\"config.json\", \"generation_config.json\"]:\n",
    "#     src = Path(export_dir_fp32) / fname\n",
    "#     if src.exists():\n",
    "#         shutil.copy(src, export_dir_int8)\n",
    "\n",
    "# print(\"INT-8-модель готова →\", export_dir_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели на cpu...\n",
      "INT8-CPU: {'wer': 0.02390438247011952, 'latency_s': 5.581054902076721, 'processed_items': 10}\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 7 (правка): загрузка INT8-модели на CPU и оценка ───────────────────\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "\n",
    "model_int8 = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
    "    export_dir_int8,\n",
    "    provider=\"CPUExecutionProvider\",   # ключевая строка\n",
    ")\n",
    "pipe_int8 = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_int8,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=-1,                         # -1 = CPU\n",
    ")\n",
    "\n",
    "stats_int8 = evaluate_model(pipe_int8, ds, max_items=10)\n",
    "print(\"INT8-CPU:\", stats_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Функция evaluate_model обновлена с прогревом и улучшенным замером времени.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 9: Обновленная evaluate_model с прогревом и точным замером времени ──\n",
    "import time, numpy as np, torch, gc, psutil, os, json\n",
    "from evaluate import load as load_metric\n",
    "from transformers import pipeline, Pipeline # Убедимся, что Pipeline импортирован\n",
    "\n",
    "# --- Функция замера памяти (остается без изменений) ---\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_mb = process.memory_info().rss / (1024**2)\n",
    "    vram_mb = 0\n",
    "    try:\n",
    "        if torch.cuda.is_available() and torch.cuda.is_initialized():\n",
    "            vram_mb = torch.cuda.memory_allocated(0) / (1024**2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return ram_mb, vram_mb\n",
    "\n",
    "# --- Обновленная evaluate_model ---\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def evaluate_model(p: Pipeline, dataset, max_items=None, num_warmup_runs=3): # Добавлен параметр num_warmup_runs\n",
    "    refs, preds = [], []\n",
    "    # Переносим замер задержки внутрь основного цикла, чтобы он был более чистым\n",
    "    pure_latencies = [] # Список для чистых задержек вызова пайплайна\n",
    "\n",
    "    iterable = dataset if max_items is None else dataset\n",
    "    # Если max_items задан, нам нужно взять warmup_runs + max_items из датасета\n",
    "    # чтобы иметь достаточно данных и для прогрева, и для оценки.\n",
    "    total_samples_needed = (num_warmup_runs if max_items is not None else 0) + \\\n",
    "                           (max_items if max_items is not None else len(dataset))\n",
    "\n",
    "    # Ограничиваем iterable, если dataset это список и мы используем max_items\n",
    "    if isinstance(dataset, list) and total_samples_needed < len(dataset):\n",
    "        effective_iterable = dataset[:total_samples_needed]\n",
    "    else:\n",
    "        # Для потоковых датасетов или если нужно меньше, чем есть\n",
    "        effective_iterable = list(iterable.take(total_samples_needed)) if hasattr(iterable, 'take') else list(iterable)[:total_samples_needed]\n",
    "\n",
    "    if len(effective_iterable) < (num_warmup_runs if max_items is not None else 0) + \\\n",
    "                                 (max_items if max_items is not None else 1):\n",
    "        print(f\"Предупреждение: Недостаточно данных для {num_warmup_runs} прогревочных запусков и {max_items} оценочных. \"\n",
    "              f\"Доступно: {len(effective_iterable)}. Прогрев может быть сокращен или отсутствовать.\")\n",
    "        # Корректируем num_warmup_runs, если max_items не задан или данных мало\n",
    "        if max_items is None:\n",
    "             actual_eval_items = max(1, len(effective_iterable) - num_warmup_runs)\n",
    "        else:\n",
    "             actual_eval_items = max_items\n",
    "        # Уменьшаем прогрев, если не хватает данных\n",
    "        effective_num_warmup_runs = max(0, len(effective_iterable) - actual_eval_items)\n",
    "    else:\n",
    "        effective_num_warmup_runs = num_warmup_runs\n",
    "\n",
    "    print(f\"Оценка модели ({type(p.model).__name__}) на устройстве: {p.device}...\")\n",
    "    print(f\"Прогревочных запусков: {effective_num_warmup_runs}, Оценочных запусков: {max_items if max_items is not None else len(effective_iterable) - effective_num_warmup_runs}\")\n",
    "\n",
    "    # --- Замеры памяти (как и ранее) ---\n",
    "    initial_ram, initial_vram = get_memory_usage()\n",
    "    max_ram_seen = initial_ram\n",
    "    peak_vram_torch = 0\n",
    "    is_torch_model_on_gpu = False\n",
    "    if hasattr(p, 'model') and isinstance(p.model, torch.nn.Module) and p.device.type == 'cuda':\n",
    "        try:\n",
    "            torch.cuda.reset_peak_memory_stats(p.device)\n",
    "            is_torch_model_on_gpu = True\n",
    "        except Exception: pass\n",
    "\n",
    "    # --- Прогревочные запуски ---\n",
    "    if effective_num_warmup_runs > 0:\n",
    "        print(\"Выполнение прогревочных запусков...\")\n",
    "        for i in range(effective_num_warmup_runs):\n",
    "            if i >= len(effective_iterable): break # Предохранитель\n",
    "            sample = effective_iterable[i]\n",
    "            audio_dict = sample[\"audio\"]\n",
    "            try:\n",
    "                wav = audio_dict[\"array\"]\n",
    "                sr = audio_dict[\"sampling_rate\"]\n",
    "                if not isinstance(wav, np.ndarray) or sr != 16_000: continue\n",
    "                _ = p(wav) # Вызываем пайплайн, результат не важен\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка на прогревочном запуске {i+1}: {e}\")\n",
    "                continue # Продолжаем, даже если прогрев упал\n",
    "        print(\"Прогрев завершен.\")\n",
    "\n",
    "    # --- Основной цикл оценки (только для замера времени и WER) ---\n",
    "    # Определяем срез данных для реальной оценки\n",
    "    eval_samples_start_index = effective_num_warmup_runs\n",
    "    num_eval_items = max_items if max_items is not None else (len(effective_iterable) - effective_num_warmup_runs)\n",
    "    eval_samples_end_index = eval_samples_start_index + num_eval_items\n",
    "\n",
    "    print(f\"Начало оценочных запусков ({num_eval_items} сэмплов)...\")\n",
    "    for i in range(eval_samples_start_index, eval_samples_end_index):\n",
    "        if i >= len(effective_iterable): break # Предохранитель\n",
    "        sample = effective_iterable[i]\n",
    "        audio_dict = sample[\"audio\"]\n",
    "\n",
    "        try:\n",
    "            wav = audio_dict[\"array\"]\n",
    "            sr = audio_dict[\"sampling_rate\"]\n",
    "            if not isinstance(wav, np.ndarray) or sr != 16_000:\n",
    "                print(f\"Предупреждение: Некорректные данные в оценочном sample {i}. Пропускаем.\")\n",
    "                continue\n",
    "        except KeyError as e:\n",
    "            print(f\"Ошибка: Отсутствует ключ {e} в audio_dict для оценочного sample {i}.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Неожиданная ошибка при извлечении аудио из оценочного sample {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- Точный замер времени выполнения пайплайна ---\n",
    "        torch.cuda.synchronize() if p.device.type == 'cuda' else None # Для точного замера на GPU\n",
    "        start_time = time.perf_counter()\n",
    "        try:\n",
    "            result = p(wav)\n",
    "            text = result[\"text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка во время выполнения пайплайна ({type(p).__name__}) для оценочного sample {i}: {e}\")\n",
    "            result = None\n",
    "            text = \"\"\n",
    "        torch.cuda.synchronize() if p.device.type == 'cuda' else None # Для точного замера на GPU\n",
    "        end_time = time.perf_counter()\n",
    "        pure_latencies.append(end_time - start_time)\n",
    "        # --- Конец точного замера ---\n",
    "\n",
    "        refs.append(sample[\"text\"].lower())\n",
    "        preds.append(text.lower())\n",
    "\n",
    "        # Обновляем пиковые значения памяти (делаем это для каждого сэмпла, включая оценочные)\n",
    "        current_ram, _ = get_memory_usage() # VRAM от PyTorch statistics будет точнее для PyTorch моделей\n",
    "        max_ram_seen = max(max_ram_seen, current_ram)\n",
    "\n",
    "        del wav, audio_dict\n",
    "        if result is not None: del result\n",
    "        if (i - eval_samples_start_index + 1) % 10 == 0 : # Каждые 10 обработанных сэмплов\n",
    "            print(f\"Обработано {i - eval_samples_start_index + 1}/{num_eval_items} оценочных сэмплов...\")\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available() and p.device.type == 'cuda':\n",
    "                 torch.cuda.empty_cache()\n",
    "    print(\"Оценочные запуски завершены.\")\n",
    "\n",
    "    if is_torch_model_on_gpu:\n",
    "        try:\n",
    "            peak_vram_torch = torch.cuda.max_memory_allocated(p.device) / (1024**2)\n",
    "        except Exception: pass\n",
    "\n",
    "    wer_score = None\n",
    "    if preds and refs:\n",
    "         valid_preds = [pred_text if isinstance(pred_text, str) else \"\" for pred_text in preds]\n",
    "         valid_refs = [ref_text if isinstance(ref_text, str) else \"\" for ref_text in refs]\n",
    "         if valid_preds and valid_refs:\n",
    "              try:\n",
    "                  wer_score = wer_metric.compute(predictions=valid_preds, references=valid_refs)\n",
    "              except Exception as e:\n",
    "                  print(f\"Ошибка при вычислении WER: {e}\")\n",
    "                  wer_score = -1\n",
    "\n",
    "    peak_ram_delta_mb = max_ram_seen - initial_ram\n",
    "    peak_vram_mb_final = f\"{peak_vram_torch:.1f}\" if peak_vram_torch > 0 else 'N/A (Не PyTorch/GPU)'\n",
    "    \n",
    "    # Если это не PyTorch модель на GPU (например, ONNX на GPU), VRAM из torch.cuda.max_memory_allocated нерелевантен.\n",
    "    # В этом случае, можно попробовать взять пик от get_memory_usage(), но он будет менее точным для GPU.\n",
    "    if peak_vram_mb_final == 'N/A (Не PyTorch/GPU)' and p.device.type == 'cuda':\n",
    "        # Для ONNX на GPU, пик VRAM от torch.cuda.max_memory_allocated() не работает.\n",
    "        # Используем замер, сделанный после загрузки модели в Ячейке 11 (если доступен)\n",
    "        # или указываем, что это общий пик во время выполнения.\n",
    "        # Это значение уже передается в stats_int8_gpu['peak_vram_mb'] в Ячейке 11.\n",
    "        # Здесь мы просто не переопределяем его, если оно уже установлено.\n",
    "        pass\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer_score,\n",
    "        \"latency_s\": np.mean(pure_latencies) if pure_latencies else 0, # Используем pure_latencies\n",
    "        \"latency_std_s\": np.std(pure_latencies) if pure_latencies else 0, # Стандартное отклонение задержек\n",
    "        \"processed_items\": len(preds),\n",
    "        \"peak_ram_mb\": f\"{peak_ram_delta_mb:.1f}\",\n",
    "        \"peak_vram_mb\": peak_vram_mb_final\n",
    "    }\n",
    "\n",
    "print(\"Функция evaluate_model обновлена с прогревом и улучшенным замером времени.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Запуск FP16 на CPU ---\n",
      "Загрузка openai/whisper-large-v3 (torch.float16) на cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание пайплайна для FP16 на CPU...\n",
      "Запуск оценки FP16 на CPU...\n",
      "Оценка модели (WhisperForConditionalGeneration) на устройстве: cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ort-gpu/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты FP16 CPU: {'wer': 0.07569721115537849, 'latency_s': 72.75857982635497, 'processed_items': 10, 'peak_ram_mb': '2970.9', 'peak_vram_mb': 'N/A (Не PyTorch/GPU)'}\n",
      "Завершено FP16 на CPU.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 10: Запуск FP16 на CPU ──────────────────────────────────────────────\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import gc\n",
    "\n",
    "print(\"\\n--- Запуск FP16 на CPU ---\")\n",
    "device_cpu = \"cpu\"\n",
    "model_id = \"openai/whisper-large-v3\" # Убедимся, что ID модели доступен\n",
    "\n",
    "# FP16 может быть очень медленным или неподдерживаемым на CPU для некоторых операций.\n",
    "# Попробуем FP16, но если будут ошибки или слишком долго, переключимся на FP32.\n",
    "# ВАЖНО: Для CPU обычно используют FP32 для лучшей совместимости и производительности.\n",
    "fp16_on_cpu_dtype = torch.float16\n",
    "# fp16_on_cpu_dtype = torch.float32 # Альтернатива, если FP16 не работает\n",
    "\n",
    "stats_fp16_cpu = {}\n",
    "\n",
    "try:\n",
    "    print(f\"Загрузка {model_id} ({fp16_on_cpu_dtype}) на {device_cpu}...\")\n",
    "    # Загружаем модель сразу на CPU\n",
    "    model_fp16_cpu = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=fp16_on_cpu_dtype,\n",
    "        low_cpu_mem_usage=False # low_cpu_mem_usage=True может потребовать GPU для метаданных\n",
    "    ).to(device_cpu)\n",
    "\n",
    "    # Процессор загружаем как обычно\n",
    "    processor_fp16_cpu = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    print(\"Создание пайплайна для FP16 на CPU...\")\n",
    "    pipe_fp16_cpu = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_fp16_cpu,\n",
    "        tokenizer=processor_fp16_cpu.tokenizer,\n",
    "        feature_extractor=processor_fp16_cpu.feature_extractor,\n",
    "        torch_dtype=fp16_on_cpu_dtype,\n",
    "        device=-1, # Явно указываем CPU для пайплайна\n",
    "    )\n",
    "\n",
    "    print(\"Запуск оценки FP16 на CPU...\")\n",
    "    # Используем обновленную evaluate_model из Cell 9\n",
    "    stats_fp16_cpu = evaluate_model(pipe_fp16_cpu, ds, max_items=SAMPLES) # Используем SAMPLES = 10\n",
    "    print(\"\\nРезультаты FP16 CPU:\", stats_fp16_cpu)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nОшибка при выполнении FP16 на CPU ({fp16_on_cpu_dtype}): {e}\")\n",
    "    print(\"Возможно, стоит попробовать torch_dtype=torch.float32 или уменьшить max_items.\")\n",
    "    stats_fp16_cpu = {\"error\": str(e)} # Сохраняем информацию об ошибке\n",
    "\n",
    "finally:\n",
    "    # Очистка памяти\n",
    "    if 'pipe_fp16_cpu' in locals(): del pipe_fp16_cpu\n",
    "    if 'model_fp16_cpu' in locals(): del model_fp16_cpu\n",
    "    if 'processor_fp16_cpu' in locals(): del processor_fp16_cpu\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Завершено FP16 на CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Запуск INT8 ONNX на GPU ---\n",
      "Доступные провайдеры ONNX Runtime: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Загрузка INT8 ONNX модели из whisper_onnx_int8 на GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-13 02:53:45.542375038 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 768 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:45.566808740 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:45.566835732 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:50.774461247 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 1348 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:50.828900093 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:50.828927518 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:56.442982258 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 1251 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:56.491090169 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-13 02:53:56.491115610 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM занято после загрузки INT8 ONNX модели: 2953.0 MB\n",
      "Создание пайплайна для INT8 ONNX на GPU...\n",
      "Запуск оценки INT8 ONNX на GPU...\n",
      "Оценка модели (_ORTModelForWhisper) на устройстве: cuda:0...\n",
      "Статистика пиковой VRAM PyTorch сброшена.\n",
      "Пиковая VRAM по статистике PyTorch: 3484.3 MB\n",
      "\n",
      "Результаты INT8 ONNX GPU: {'wer': 0.02390438247011952, 'latency_s': 7.1746272325515745, 'processed_items': 10, 'peak_ram_mb': '832.4', 'peak_vram_mb': '3484.3'}\n",
      "Завершено INT8 ONNX на GPU.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 11: Запуск INT8 ONNX на GPU ─────────────────────────────────────────\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from transformers import AutoProcessor, pipeline # Нужен процессор и пайплайн\n",
    "\n",
    "print(\"\\n--- Запуск INT8 ONNX на GPU ---\")\n",
    "stats_int8_gpu = {}\n",
    "onnx_int8_dir = Path(export_dir_int8)\n",
    "gpu_device_index = 0 # Используем GPU 0\n",
    "gpu_device_name = f\"cuda:{gpu_device_index}\"\n",
    "\n",
    "# Проверяем наличие ONNX Runtime GPU и папки с моделью\n",
    "onnxruntime_gpu_available = False\n",
    "try:\n",
    "    import onnxruntime\n",
    "    providers = onnxruntime.get_available_providers()\n",
    "    print(\"Доступные провайдеры ONNX Runtime:\", providers)\n",
    "    if \"CUDAExecutionProvider\" in providers:\n",
    "        onnxruntime_gpu_available = True\n",
    "    else:\n",
    "        print(\"\\nПРЕДУПРЕЖДЕНИЕ: CUDAExecutionProvider не найден.\")\n",
    "        print(\"Убедитесь, что установлен пакет 'onnxruntime-gpu'.\")\n",
    "        print(\"pip install -U onnxruntime-gpu\")\n",
    "except ImportError:\n",
    "    print(\"ПРЕДУПРЕЖДЕНИЕ: Библиотека onnxruntime не найдена.\")\n",
    "\n",
    "if not onnx_int8_dir.exists():\n",
    "    print(f\"ПРЕДУПРЕЖДЕНИЕ: Папка с INT8 ONNX моделью не найдена: {onnx_int8_dir}\")\n",
    "\n",
    "if onnxruntime_gpu_available and onnx_int8_dir.exists():\n",
    "    try:\n",
    "        from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "\n",
    "        print(f\"Загрузка INT8 ONNX модели из {onnx_int8_dir} на GPU...\")\n",
    "        model_int8_gpu = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
    "            onnx_int8_dir,\n",
    "            provider=\"CUDAExecutionProvider\",\n",
    "            provider_options={'device_id': str(gpu_device_index)}, # Указываем ID устройства GPU\n",
    "        )\n",
    "        # Загрузим VRAM после загрузки модели (как прокси)\n",
    "        vram_after_load_mb = 0\n",
    "        if torch.cuda.is_available():\n",
    "             vram_after_load_mb = torch.cuda.memory_allocated(gpu_device_index) / (1024**2)\n",
    "             print(f\"VRAM занято после загрузки INT8 ONNX модели: {vram_after_load_mb:.1f} MB\")\n",
    "\n",
    "\n",
    "        # Используем тот же процессор, что и для других моделей (если он еще существует)\n",
    "        # Если нет, загружаем заново\n",
    "        if 'processor' not in locals():\n",
    "            print(\"Загрузка процессора...\")\n",
    "            processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\") # Исходный ID\n",
    "\n",
    "        print(\"Создание пайплайна для INT8 ONNX на GPU...\")\n",
    "        pipe_int8_gpu = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model_int8_gpu,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            device=gpu_device_index, # Явно указываем GPU для пайплайна\n",
    "        )\n",
    "\n",
    "        print(\"Запуск оценки INT8 ONNX на GPU...\")\n",
    "        # Используем обновленную evaluate_model из Cell 9\n",
    "        stats_int8_gpu = evaluate_model(pipe_int8_gpu, ds, max_items=SAMPLES) # Используем SAMPLES = 10\n",
    "\n",
    "        # Добавим замер VRAM после загрузки в результаты, т.к. evaluate_model не может мерить пик ORT\n",
    "        if 'peak_vram_mb' not in stats_int8_gpu or stats_int8_gpu['peak_vram_mb'] == 'N/A (Не PyTorch/GPU)':\n",
    "             stats_int8_gpu['peak_vram_mb'] = f\"~{vram_after_load_mb:.1f} (После загрузки)\"\n",
    "\n",
    "        print(\"\\nРезультаты INT8 ONNX GPU:\", stats_int8_gpu)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nОшибка при выполнении INT8 ONNX на GPU: {e}\")\n",
    "        stats_int8_gpu = {\"error\": str(e)}\n",
    "\n",
    "    finally:\n",
    "        # Очистка памяти\n",
    "        if 'pipe_int8_gpu' in locals(): del pipe_int8_gpu\n",
    "        if 'model_int8_gpu' in locals(): del model_int8_gpu\n",
    "        # 'processor' лучше оставить, если он нужен дальше\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Пропуск запуска INT8 ONNX на GPU из-за отсутствия провайдера, папки модели или onnxruntime.\")\n",
    "    stats_int8_gpu = {\"error\": \"Пропущено из-за зависимостей\"}\n",
    "\n",
    "print(\"Завершено INT8 ONNX на GPU.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ort-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
