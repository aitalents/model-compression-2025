{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b422a9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-16T06:19:24.568984Z",
     "iopub.status.busy": "2025-05-16T06:19:24.568720Z",
     "iopub.status.idle": "2025-05-16T06:20:57.193642Z",
     "shell.execute_reply": "2025-05-16T06:20:57.192895Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 92.632589,
     "end_time": "2025-05-16T06:20:57.195273",
     "exception": false,
     "start_time": "2025-05-16T06:19:24.562684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eager\r\n",
      "  Downloading eager-0.0.1.tar.gz (2.6 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\r\n",
      "Collecting onnxruntime-gpu\r\n",
      "  Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\r\n",
      "Collecting optimum[onnxruntime]\r\n",
      "  Downloading optimum-1.25.2-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.51.3)\r\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (25.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (0.31.1)\r\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.17.0)\r\n",
      "Requirement already satisfied: datasets>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (3.6.0)\r\n",
      "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (3.20.3)\r\n",
      "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime])\r\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\r\n",
      "Collecting coloredlogs (from onnxruntime-gpu)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.18.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (19.0.1)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.70.16)\r\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime])\r\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum[onnxruntime])\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.21.1)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (3.11.18)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2025.4.26)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum[onnxruntime]) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum[onnxruntime]) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optimum[onnxruntime]) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optimum[onnxruntime]) (2024.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.20.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optimum[onnxruntime]) (2024.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.2.1->optimum[onnxruntime]) (1.17.0)\r\n",
      "Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading optimum-1.25.2-py3-none-any.whl (429 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m429.4/429.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: eager\r\n",
      "  Building wheel for eager (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for eager: filename=eager-0.0.1-py3-none-any.whl size=3768 sha256=2f31f150b2e575fc1e20dc1e0c38d788e052f61283fdbe88ada7b6eed9e93939\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/65/d5/3499ac987526c08fe43c1edc3613267aee1859987739e9eeff\r\n",
      "Successfully built eager\r\n",
      "Installing collected packages: eager, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, humanfriendly, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, nvidia-cusolver-cu12, optimum, onnxruntime, onnxruntime-gpu\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.3.2\r\n",
      "    Uninstalling fsspec-2025.3.2:\r\n",
      "      Successfully uninstalled fsspec-2025.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed coloredlogs-15.0.1 eager-0.0.1 fsspec-2025.3.0 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.0 onnxruntime-gpu-1.22.0 optimum-1.25.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install eager optimum[onnxruntime] timm onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840e1aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:20:57.261779Z",
     "iopub.status.busy": "2025-05-16T06:20:57.261128Z",
     "iopub.status.idle": "2025-05-16T06:21:01.903035Z",
     "shell.execute_reply": "2025-05-16T06:21:01.902279Z"
    },
    "papermill": {
     "duration": 4.677422,
     "end_time": "2025-05-16T06:21:01.904514",
     "exception": false,
     "start_time": "2025-05-16T06:20:57.227092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gputil\r\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: gputil\r\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=3826ee4637902942dd9e78087febf70d03c342e3a201ff5684214cdb17e61634\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\r\n",
      "Successfully built gputil\r\n",
      "Installing collected packages: gputil\r\n",
      "Successfully installed gputil-1.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5459581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:21:01.964963Z",
     "iopub.status.busy": "2025-05-16T06:21:01.964707Z",
     "iopub.status.idle": "2025-05-16T06:21:16.142982Z",
     "shell.execute_reply": "2025-05-16T06:21:16.142386Z"
    },
    "papermill": {
     "duration": 14.20944,
     "end_time": "2025-05-16T06:21:16.144397",
     "exception": false,
     "start_time": "2025-05-16T06:21:01.934957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import psutil\n",
    "import GPUtil\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.amp import autocast\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02bd6fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:21:16.204582Z",
     "iopub.status.busy": "2025-05-16T06:21:16.203960Z",
     "iopub.status.idle": "2025-05-16T06:21:16.221945Z",
     "shell.execute_reply": "2025-05-16T06:21:16.221374Z"
    },
    "papermill": {
     "duration": 0.049132,
     "end_time": "2025-05-16T06:21:16.223007",
     "exception": false,
     "start_time": "2025-05-16T06:21:16.173875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµĞ³Ğ°Ğ±Ğ°Ğ¹Ñ‚Ğ°Ñ…\"\"\"\n",
    "    if label:\n",
    "        print(f\"\\n--- Memory Usage ({label}) ---\")\n",
    "    else:\n",
    "        print(\"\\n--- Memory Usage ---\")\n",
    "        \n",
    "    # CPU RAM Ğ² MB\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_used = process.memory_info().rss / (1024 ** 2)\n",
    "    print(f\"CPU RAM used: {ram_used:.2f} MB\")\n",
    "    \n",
    "    # GPU VRAM Ğ² MB\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        vram_used = gpu.memoryUsed\n",
    "        vram_total = gpu.memoryTotal\n",
    "        print(f\"GPU {gpu.id} VRAM: {vram_used:.2f} MB / {vram_total:.2f} MB\")\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    return (param_size + buffer_size) / 1024**2\n",
    "\n",
    "\n",
    "def calculate_metrics(model, device, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets.numpy())\n",
    "    \n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def load_imagenet_mini(dataset_path, model, split):\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "\n",
    "    print(transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2,\n",
    "                hue=0.2,\n",
    "            ),\n",
    "            transforms.GaussianBlur(\n",
    "                kernel_size=3,\n",
    "            ),\n",
    "            transform,\n",
    "        ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(dataset_path, split),\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def benchmark_model(model, device, input_tensor, num_runs=10, warmup=3, use_amp=False):\n",
    "    model = model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"\\nğŸ”¥ Warming up ({warmup} runs) on {device}...\")\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    _ = model(input_tensor)\n",
    "            else:\n",
    "                _ = model(input_tensor)\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"ğŸš€ Benchmarking ({num_runs} runs) on {device}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    _ = model(input_tensor)\n",
    "            else:\n",
    "                _ = model(input_tensor)\n",
    "    \n",
    "    total_time = (time.time() - start_time) * 1000\n",
    "    avg_time = total_time / num_runs\n",
    "    print(f\"âœ… Average inference: {avg_time:.2f} ms\")\n",
    "    print(f\"ğŸ“Š Total time: {total_time:.2f} ms | FPS: {1000/(avg_time + 1e-9):.1f}\")\n",
    "    \n",
    "    return avg_time\n",
    "\n",
    "\n",
    "def main(model, dataset_path=None):\n",
    "    device_cpu = torch.device('cpu')\n",
    "    device_gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"\\nğŸ” Initial memory state:\")\n",
    "    print_memory_usage(\"Before loading model\")\n",
    "    model.eval()\n",
    "    print(f\"ğŸ“ Model size: {get_model_size(model):.2f} MB\")\n",
    "\n",
    "    print(\"\\nğŸ” Initial memory state:\")\n",
    "    print_memory_usage(\"Model loaded\")\n",
    "    \n",
    "    # Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸\n",
    "    input_tensor = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    print(\"\\nğŸ§ª Benchmarking on CPU:\")\n",
    "    cpu_time = benchmark_model(model, device_cpu, input_tensor)\n",
    "    print_memory_usage(\"After CPU benchmark\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nğŸ® Benchmarking on GPU:\")\n",
    "        gpu_time = benchmark_model(model, device_gpu, input_tensor)\n",
    "        print_memory_usage(\"After GPU test\")\n",
    "        \n",
    "        print(\"\\nâš¡ Benchmarking with AMP:\")\n",
    "        gpu_amp_time = benchmark_model(model, device_gpu, input_tensor, use_amp=True)\n",
    "        print_memory_usage(\"After AMP test\")\n",
    "        \n",
    "        print(\"\\nğŸ“ˆ Results Summary:\")\n",
    "        print(f\"| Device | Inference Time (ms) | Speedup vs CPU |\")\n",
    "        print(\"|--------|---------------------|----------------|\")\n",
    "        print(f\"| CPU    | {cpu_time:19.2f} | {'â€”':^15} |\")\n",
    "        print(f\"| GPU    | {gpu_time:19.2f} | {cpu_time/gpu_time:^15.1f}x |\")\n",
    "        print(f\"| AMP    | {gpu_amp_time:19.2f} | {cpu_time/gpu_amp_time:^15.1f}x |\")\n",
    "    else:\n",
    "        print(\"\\nâŒ CUDA not available\")\n",
    "        print(f\"â±ï¸ CPU inference time: {cpu_time:.2f} ms\")\n",
    "        if dataset_path:\n",
    "            print(\"\\nğŸ¯ Quality Metrics (CPU):\")\n",
    "            print(f\"Precision: {precision_cpu:.4f}\")\n",
    "            print(f\"Recall:    {recall_cpu:.a4f}\")\n",
    "            print(f\"F1-Score:  {f1_cpu:.4f}\")\n",
    "\n",
    "    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°\n",
    "    if dataset_path:\n",
    "        print(\"\\nğŸ“Š Loading ImageNetMini dataset...\")\n",
    "        val_dataset = load_imagenet_mini(dataset_path, model, 'val')\n",
    "        data_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, drop_last=False)\n",
    "        \n",
    "        # print(\"\\nğŸ§® Calculating metrics on CPU:\")\n",
    "        # precision_cpu, recall_cpu, f1_cpu = calculate_metrics(model, device_cpu, data_loader)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\nğŸ§® Calculating metrics on GPU:\")\n",
    "            precision_gpu, recall_gpu, f1_gpu = calculate_metrics(model, device_gpu, data_loader)\n",
    "\n",
    "            print(\"\\nğŸ¯ Quality Metrics Summary:\")\n",
    "            print(\"| Device | Precision | Recall  | F1-Score |\")\n",
    "            print(\"|--------|-----------|---------|----------|\")\n",
    "            # print(f\"| CPU    | {precision_cpu:.4f}  | {recall_cpu:.4f} | {f1_cpu:.4f}  |\")\n",
    "            print(f\"| GPU    | {precision_gpu:.4f}  | {recall_gpu:.4f} | {f1_gpu:.4f}  |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e66f3da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:21:16.329549Z",
     "iopub.status.busy": "2025-05-16T06:21:16.328889Z",
     "iopub.status.idle": "2025-05-16T06:21:16.480583Z",
     "shell.execute_reply": "2025-05-16T06:21:16.479687Z"
    },
    "papermill": {
     "duration": 0.230249,
     "end_time": "2025-05-16T06:21:16.481821",
     "exception": false,
     "start_time": "2025-05-16T06:21:16.251572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/imagenetmini-1000/imagenet-mini\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"ifigotin/imagenetmini-1000\")\n",
    "path += \"/imagenet-mini\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77aaffab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:21:16.541310Z",
     "iopub.status.busy": "2025-05-16T06:21:16.540721Z",
     "iopub.status.idle": "2025-05-16T06:21:17.901050Z",
     "shell.execute_reply": "2025-05-16T06:21:17.900340Z"
    },
    "papermill": {
     "duration": 1.391532,
     "end_time": "2025-05-16T06:21:17.902566",
     "exception": false,
     "start_time": "2025-05-16T06:21:16.511034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a1988c1f5345e0a403f7ba59ca4930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = timm.create_model('efficientvit_b3.r256_in1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced67357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:21:17.972071Z",
     "iopub.status.busy": "2025-05-16T06:21:17.971789Z",
     "iopub.status.idle": "2025-05-16T06:22:37.741675Z",
     "shell.execute_reply": "2025-05-16T06:22:37.740832Z"
    },
    "papermill": {
     "duration": 79.836794,
     "end_time": "2025-05-16T06:22:37.774043",
     "exception": false,
     "start_time": "2025-05-16T06:21:17.937249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Initial memory state:\n",
      "\n",
      "--- Memory Usage (Before loading model) ---\n",
      "CPU RAM used: 908.25 MB\n",
      "GPU 0 VRAM: 3.00 MB / 16384.00 MB\n",
      "ğŸ“ Model size: 185.75 MB\n",
      "\n",
      "ğŸ” Initial memory state:\n",
      "\n",
      "--- Memory Usage (Model loaded) ---\n",
      "CPU RAM used: 908.25 MB\n",
      "GPU 0 VRAM: 3.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ§ª Benchmarking on CPU:\n",
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cpu...\n",
      "ğŸš€ Benchmarking (10 runs) on cpu...\n",
      "âœ… Average inference: 144.25 ms\n",
      "ğŸ“Š Total time: 1442.46 ms | FPS: 6.9\n",
      "\n",
      "--- Memory Usage (After CPU benchmark) ---\n",
      "CPU RAM used: 955.62 MB\n",
      "GPU 0 VRAM: 3.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ® Benchmarking on GPU:\n",
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cuda...\n",
      "ğŸš€ Benchmarking (10 runs) on cuda...\n",
      "âœ… Average inference: 23.16 ms\n",
      "ğŸ“Š Total time: 231.59 ms | FPS: 43.2\n",
      "\n",
      "--- Memory Usage (After GPU test) ---\n",
      "CPU RAM used: 1076.43 MB\n",
      "GPU 0 VRAM: 539.00 MB / 16384.00 MB\n",
      "\n",
      "âš¡ Benchmarking with AMP:\n",
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cuda...\n",
      "ğŸš€ Benchmarking (10 runs) on cuda...\n",
      "âœ… Average inference: 28.60 ms\n",
      "ğŸ“Š Total time: 286.03 ms | FPS: 35.0\n",
      "\n",
      "--- Memory Usage (After AMP test) ---\n",
      "CPU RAM used: 1094.93 MB\n",
      "GPU 0 VRAM: 615.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ“ˆ Results Summary:\n",
      "| Device | Inference Time (ms) | Speedup vs CPU |\n",
      "|--------|---------------------|----------------|\n",
      "| CPU    |              144.25 |        â€”        |\n",
      "| GPU    |               23.16 |       6.2      x |\n",
      "| AMP    |               28.60 |       5.0      x |\n",
      "\n",
      "ğŸ“Š Loading ImageNetMini dataset...\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(256, 256))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "\n",
      "ğŸ§® Calculating metrics on GPU:\n",
      "\n",
      "ğŸ¯ Quality Metrics Summary:\n",
      "| Device | Precision | Recall  | F1-Score |\n",
      "|--------|-----------|---------|----------|\n",
      "| GPU    | 0.8474  | 0.8342 | 0.8239  |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main(base_model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1c3d7",
   "metadata": {
    "papermill": {
     "duration": 0.028696,
     "end_time": "2025-05-16T06:22:37.831842",
     "exception": false,
     "start_time": "2025-05-16T06:22:37.803146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optimum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8597cc95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:22:37.891036Z",
     "iopub.status.busy": "2025-05-16T06:22:37.890785Z",
     "iopub.status.idle": "2025-05-16T06:22:37.904553Z",
     "shell.execute_reply": "2025-05-16T06:22:37.903988Z"
    },
    "papermill": {
     "duration": 0.044831,
     "end_time": "2025-05-16T06:22:37.905560",
     "exception": false,
     "start_time": "2025-05-16T06:22:37.860729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµĞ³Ğ°Ğ±Ğ°Ğ¹Ñ‚Ğ°Ñ…\"\"\"\n",
    "    if label:\n",
    "        print(f\"\\n--- Memory Usage ({label}) ---\")\n",
    "    else:\n",
    "        print(\"\\n--- Memory Usage ---\")\n",
    "        \n",
    "    # CPU RAM Ğ² MB\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_used = process.memory_info().rss / (1024 ** 2)\n",
    "    print(f\"CPU RAM used: {ram_used:.2f} MB\")\n",
    "    \n",
    "    # GPU VRAM Ğ² MB\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        vram_used = gpu.memoryUsed\n",
    "        vram_total = gpu.memoryTotal\n",
    "        print(f\"GPU {gpu.id} VRAM: {vram_used:.2f} MB / {vram_total:.2f} MB\")\n",
    "\n",
    "\n",
    "def calculate_metrics(model, device, data_loader):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).logits\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets.numpy())\n",
    "    \n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def load_imagenet_mini(dataset_path, model, split):\n",
    "    config = resolve_data_config({}, model=base_model)\n",
    "    transform = create_transform(**config)\n",
    "\n",
    "    print(transform)\n",
    "\n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(dataset_path, split),\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def benchmark_model(model, device, input_tensor, num_runs=10, warmup=3, use_amp=False):\n",
    "    model = model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"\\nğŸ”¥ Warming up ({warmup} runs) on {device}...\")\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    _ = model(input_tensor)\n",
    "            else:\n",
    "                _ = model(input_tensor)\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"ğŸš€ Benchmarking ({num_runs} runs) on {device}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    _ = model(input_tensor)\n",
    "            else:\n",
    "                _ = model(input_tensor)\n",
    "    \n",
    "    total_time = (time.time() - start_time) * 1000\n",
    "    avg_time = total_time / num_runs\n",
    "    print(f\"âœ… Average inference: {avg_time:.2f} ms\")\n",
    "    print(f\"ğŸ“Š Total time: {total_time:.2f} ms | FPS: {1000/(avg_time + 1e-9):.1f}\")\n",
    "    \n",
    "    return avg_time\n",
    "\n",
    "\n",
    "def main(model, dataset_path=None):\n",
    "    device_cpu = torch.device('cpu')\n",
    "    device_gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"\\nğŸ” Initial memory state:\")\n",
    "    print_memory_usage(\"Before loading model\")\n",
    "\n",
    "    print(\"\\nğŸ” Initial memory state:\")\n",
    "    print_memory_usage(\"Model loaded\")\n",
    "    \n",
    "    # Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸\n",
    "    input_tensor = torch.randn(1, 3, 256, 256)\n",
    "    \n",
    "    print(\"\\nğŸ§ª Benchmarking on CPU:\")\n",
    "    cpu_time = benchmark_model(model, device_cpu, input_tensor)\n",
    "    print_memory_usage(\"After CPU benchmark\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nğŸ® Benchmarking on GPU:\")\n",
    "        gpu_time = benchmark_model(model, device_gpu, input_tensor)\n",
    "        print_memory_usage(\"After GPU test\")\n",
    "        \n",
    "        print(\"\\nâš¡ Benchmarking with AMP:\")\n",
    "        gpu_amp_time = benchmark_model(model, device_gpu, input_tensor, use_amp=True)\n",
    "        print_memory_usage(\"After AMP test\")\n",
    "        \n",
    "        print(\"\\nğŸ“ˆ Results Summary:\")\n",
    "        print(f\"| Device | Inference Time (ms) | Speedup vs CPU |\")\n",
    "        print(\"|--------|---------------------|----------------|\")\n",
    "        print(f\"| CPU    | {cpu_time:19.2f} | {'â€”':^15} |\")\n",
    "        print(f\"| GPU    | {gpu_time:19.2f} | {cpu_time/gpu_time:^15.1f}x |\")\n",
    "        print(f\"| AMP    | {gpu_amp_time:19.2f} | {cpu_time/gpu_amp_time:^15.1f}x |\")\n",
    "    else:\n",
    "        print(\"\\nâŒ CUDA not available\")\n",
    "        print(f\"â±ï¸ CPU inference time: {cpu_time:.2f} ms\")\n",
    "        if dataset_path:\n",
    "            print(\"\\nğŸ¯ Quality Metrics (CPU):\")\n",
    "            print(f\"Precision: {precision_cpu:.4f}\")\n",
    "            print(f\"Recall:    {recall_cpu:.a4f}\")\n",
    "            print(f\"F1-Score:  {f1_cpu:.4f}\")\n",
    "\n",
    "    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°\n",
    "    if dataset_path:\n",
    "        print(\"\\nğŸ“Š Loading ImageNetMini dataset...\")\n",
    "        val_dataset = load_imagenet_mini(dataset_path, model, 'val')\n",
    "        data_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, drop_last=False)\n",
    "        \n",
    "        # print(\"\\nğŸ§® Calculating metrics on CPU:\")\n",
    "        # precision_cpu, recall_cpu, f1_cpu = calculate_metrics(model, device_cpu, data_loader)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\nğŸ§® Calculating metrics on GPU:\")\n",
    "            precision_gpu, recall_gpu, f1_gpu = calculate_metrics(model, device_gpu, data_loader)\n",
    "\n",
    "            print(\"\\nğŸ¯ Quality Metrics Summary:\")\n",
    "            print(\"| Device | Precision | Recall  | F1-Score |\")\n",
    "            print(\"|--------|-----------|---------|----------|\")\n",
    "            # print(f\"| CPU    | {precision_cpu:.4f}  | {recall_cpu:.4f} | {f1_cpu:.4f}  |\")\n",
    "            print(f\"| GPU    | {precision_gpu:.4f}  | {recall_gpu:.4f} | {f1_gpu:.4f}  |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9adf94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:22:37.964546Z",
     "iopub.status.busy": "2025-05-16T06:22:37.964328Z",
     "iopub.status.idle": "2025-05-16T06:23:01.832452Z",
     "shell.execute_reply": "2025-05-16T06:23:01.831853Z"
    },
    "papermill": {
     "duration": 23.899149,
     "end_time": "2025-05-16T06:23:01.833949",
     "exception": false,
     "start_time": "2025-05-16T06:22:37.934800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 06:22:40.000308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747376560.218091      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747376560.288553      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForImageClassification\n",
    "from transformers import AutoFeatureExtractor\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36817f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:23:01.894321Z",
     "iopub.status.busy": "2025-05-16T06:23:01.893719Z",
     "iopub.status.idle": "2025-05-16T06:23:13.907873Z",
     "shell.execute_reply": "2025-05-16T06:23:13.907193Z"
    },
    "papermill": {
     "duration": 12.045176,
     "end_time": "2025-05-16T06:23:13.909316",
     "exception": false,
     "start_time": "2025-05-16T06:23:01.864140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1df5dfc9c94bdd8f80d44ca1e22a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type timm_wrapper to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "The `save_pretrained` method is disabled for TimmWrapperImageProcessor. The image processor configuration is saved directly in `config.json` when `save_pretrained` is called for saving the model.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"timm/efficientvit_b3.r256_in1k\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "model = ORTModelForImageClassification.from_pretrained(model_id, export=True)\n",
    "model.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7578e46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:23:13.971042Z",
     "iopub.status.busy": "2025-05-16T06:23:13.970799Z",
     "iopub.status.idle": "2025-05-16T06:23:14.202256Z",
     "shell.execute_reply": "2025-05-16T06:23:14.201020Z"
    },
    "papermill": {
     "duration": 0.263679,
     "end_time": "2025-05-16T06:23:14.204119",
     "exception": false,
     "start_time": "2025-05-16T06:23:13.940440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm onnx/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7968e45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:23:14.272102Z",
     "iopub.status.busy": "2025-05-16T06:23:14.271796Z",
     "iopub.status.idle": "2025-05-16T06:23:18.096450Z",
     "shell.execute_reply": "2025-05-16T06:23:18.095604Z"
    },
    "papermill": {
     "duration": 3.861712,
     "end_time": "2025-05-16T06:23:18.098406",
     "exception": false,
     "start_time": "2025-05-16T06:23:14.236694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "base_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        base_model,\n",
    "        torch.rand(1, 3, 256, 256).to('cuda'),\n",
    "        'onnx/model.onnx',\n",
    "        opset_version=15,\n",
    "        input_names=['pixel_values'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={'pixel_values': {0: 'batch_size'}, 'logits': {0: 'batch_size'}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45bcff38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:23:18.166772Z",
     "iopub.status.busy": "2025-05-16T06:23:18.166060Z",
     "iopub.status.idle": "2025-05-16T06:23:18.916373Z",
     "shell.execute_reply": "2025-05-16T06:23:18.915775Z"
    },
    "papermill": {
     "duration": 0.786813,
     "end_time": "2025-05-16T06:23:18.917797",
     "exception": false,
     "start_time": "2025-05-16T06:23:18.130984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type timm_wrapper to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"timm/efficientvit_b3.r256_in1k\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "model = ORTModelForImageClassification.from_pretrained(onnx_path, file_name=\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bffe8e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:23:18.981382Z",
     "iopub.status.busy": "2025-05-16T06:23:18.980906Z",
     "iopub.status.idle": "2025-05-16T06:24:12.640978Z",
     "shell.execute_reply": "2025-05-16T06:24:12.640249Z"
    },
    "papermill": {
     "duration": 53.724732,
     "end_time": "2025-05-16T06:24:12.674077",
     "exception": false,
     "start_time": "2025-05-16T06:23:18.949345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Initial memory state:\n",
      "\n",
      "--- Memory Usage (Before loading model) ---\n",
      "CPU RAM used: 2967.36 MB\n",
      "GPU 0 VRAM: 2199.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ” Initial memory state:\n",
      "\n",
      "--- Memory Usage (Model loaded) ---\n",
      "CPU RAM used: 2967.36 MB\n",
      "GPU 0 VRAM: 2199.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ§ª Benchmarking on CPU:\n",
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cpu...\n",
      "ğŸš€ Benchmarking (10 runs) on cpu...\n",
      "âœ… Average inference: 105.49 ms\n",
      "ğŸ“Š Total time: 1054.92 ms | FPS: 9.5\n",
      "\n",
      "--- Memory Usage (After CPU benchmark) ---\n",
      "CPU RAM used: 2822.61 MB\n",
      "GPU 0 VRAM: 2199.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ® Benchmarking on GPU:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 06:23:21.358165503 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 06:23:21.358220597 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cuda...\n",
      "ğŸš€ Benchmarking (10 runs) on cuda...\n",
      "âœ… Average inference: 20.43 ms\n",
      "ğŸ“Š Total time: 204.34 ms | FPS: 48.9\n",
      "\n",
      "--- Memory Usage (After GPU test) ---\n",
      "CPU RAM used: 2695.33 MB\n",
      "GPU 0 VRAM: 2475.00 MB / 16384.00 MB\n",
      "\n",
      "âš¡ Benchmarking with AMP:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 06:23:22.164885269 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 06:23:22.164914679 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cuda...\n",
      "ğŸš€ Benchmarking (10 runs) on cuda...\n",
      "âœ… Average inference: 20.02 ms\n",
      "ğŸ“Š Total time: 200.24 ms | FPS: 49.9\n",
      "\n",
      "--- Memory Usage (After AMP test) ---\n",
      "CPU RAM used: 2695.45 MB\n",
      "GPU 0 VRAM: 2475.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ“ˆ Results Summary:\n",
      "| Device | Inference Time (ms) | Speedup vs CPU |\n",
      "|--------|---------------------|----------------|\n",
      "| CPU    |              105.49 |        â€”        |\n",
      "| GPU    |               20.43 |       5.2      x |\n",
      "| AMP    |               20.02 |       5.3      x |\n",
      "\n",
      "ğŸ“Š Loading ImageNetMini dataset...\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(256, 256))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "\n",
      "ğŸ§® Calculating metrics on GPU:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 06:23:23.839902991 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 06:23:23.839934448 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Quality Metrics Summary:\n",
      "| Device | Precision | Recall  | F1-Score |\n",
      "|--------|-----------|---------|----------|\n",
      "| GPU    | 0.8474  | 0.8342 | 0.8239  |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677429bd",
   "metadata": {
    "papermill": {
     "duration": 0.030112,
     "end_time": "2025-05-16T06:24:12.734881",
     "exception": false,
     "start_time": "2025-05-16T06:24:12.704769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Quantization with Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bfae71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:24:12.798953Z",
     "iopub.status.busy": "2025-05-16T06:24:12.798362Z",
     "iopub.status.idle": "2025-05-16T06:24:12.944341Z",
     "shell.execute_reply": "2025-05-16T06:24:12.943819Z"
    },
    "papermill": {
     "duration": 0.178689,
     "end_time": "2025-05-16T06:24:12.945771",
     "exception": false,
     "start_time": "2025-05-16T06:24:12.767082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from onnxruntime.quantization import QuantFormat, QuantizationMode\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(\n",
    "    is_static=True,\n",
    "    per_channel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3172ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:24:13.008309Z",
     "iopub.status.busy": "2025-05-16T06:24:13.007759Z",
     "iopub.status.idle": "2025-05-16T06:25:23.798509Z",
     "shell.execute_reply": "2025-05-16T06:25:23.797901Z"
    },
    "papermill": {
     "duration": 70.823127,
     "end_time": "2025-05-16T06:25:23.799989",
     "exception": false,
     "start_time": "2025-05-16T06:24:12.976862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(256, 256))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_imagenet_mini(path, base_model, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb497af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:25:23.867506Z",
     "iopub.status.busy": "2025-05-16T06:25:23.866973Z",
     "iopub.status.idle": "2025-05-16T06:25:49.151946Z",
     "shell.execute_reply": "2025-05-16T06:25:49.151134Z"
    },
    "papermill": {
     "duration": 25.319539,
     "end_time": "2025-05-16T06:25:49.153457",
     "exception": false,
     "start_time": "2025-05-16T06:25:23.833918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from datasets import Dataset\n",
    "\n",
    "calibration_size = 1000\n",
    "calibration_dataset, _ = random_split(train_dataset, [calibration_size, len(train_dataset) - calibration_size])\n",
    "images_list = [image for image, _ in calibration_dataset]\n",
    "optimum_dataset = Dataset.from_dict({\n",
    "    \"pixel_values\": images_list,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f177b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:25:49.218183Z",
     "iopub.status.busy": "2025-05-16T06:25:49.217249Z",
     "iopub.status.idle": "2025-05-16T07:07:51.342831Z",
     "shell.execute_reply": "2025-05-16T07:07:51.341868Z"
    },
    "papermill": {
     "duration": 2522.158469,
     "end_time": "2025-05-16T07:07:51.344252",
     "exception": false,
     "start_time": "2025-05-16T06:25:49.185783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 2 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 3 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 4 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 5 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 6 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 7 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 8 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 9 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 10 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 11 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 12 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 13 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 14 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 15 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 16 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 17 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 18 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 19 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 20 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 21 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 22 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 23 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 24 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 25 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 26 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 27 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 28 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 29 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 30 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 31 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 32 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 33 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 34 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 35 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 36 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 37 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 38 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 39 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 40 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 41 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 42 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 43 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 44 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 45 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 46 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 47 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 48 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 49 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 50 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 51 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 52 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 53 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 54 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 55 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 56 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 57 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 58 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 59 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 60 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 61 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 62 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 63 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 64 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 65 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 66 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 67 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 68 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 69 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 70 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 71 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 72 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 73 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 74 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 75 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 76 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 77 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 78 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 79 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 80 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 81 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 82 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 83 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 84 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 85 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 86 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 87 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 88 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 89 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 90 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 91 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 92 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 93 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 94 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 95 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 96 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 97 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 98 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 99 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Processing 100 shard\n",
      "Collecting tensor data and making histogram ...\n",
      "Finding optimal threshold for each tensor using 'percentile' algorithm ...\n",
      "Number of tensors : 560\n",
      "Number of histogram bins : 2048\n",
      "Percentile : (0.010000000000005116,99.99)\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "\n",
    "calibration_config = AutoCalibrationConfig.percentiles(optimum_dataset, percentile=99.99)\n",
    "\n",
    "# Perform the calibration step: computes the activations quantization ranges\n",
    "batch_size = 10\n",
    "shards = len(optimum_dataset) // batch_size\n",
    "\n",
    "for i in range(shards):\n",
    "    print(f'Processing {i + 1} shard')\n",
    "    shard = optimum_dataset.shard(shards, i)\n",
    "    quantizer.partial_fit(\n",
    "        dataset=shard,\n",
    "        calibration_config=calibration_config,\n",
    "        operators_to_quantize=qconfig.operators_to_quantize,\n",
    "        batch_size=batch_size,\n",
    "        use_external_data_format=False,\n",
    "    )\n",
    "ranges = quantizer.compute_ranges()\n",
    "\n",
    "# remove temp augmented model again\n",
    "os.remove(\"augmented_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39152e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T07:07:51.424651Z",
     "iopub.status.busy": "2025-05-16T07:07:51.424334Z",
     "iopub.status.idle": "2025-05-16T07:08:06.409663Z",
     "shell.execute_reply": "2025-05-16T07:08:06.408830Z"
    },
    "papermill": {
     "duration": 15.027136,
     "end_time": "2025-05-16T07:08:06.411158",
     "exception": false,
     "start_time": "2025-05-16T07:07:51.384022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime.preprocessors import QuantizationPreprocessor\n",
    "from optimum.onnxruntime.preprocessors.passes import (\n",
    "    ExcludeGeLUNodes,\n",
    "    ExcludeLayerNormNodes,\n",
    "    ExcludeNodeAfter,\n",
    "    ExcludeNodeFollowedBy,\n",
    ")\n",
    "\n",
    "\n",
    "def create_quantization_preprocessor():\n",
    "    # Create a quantization preprocessor to determine the nodes to exclude\n",
    "    quantization_preprocessor = QuantizationPreprocessor()\n",
    "\n",
    "    # Exclude the nodes constituting LayerNorm\n",
    "    quantization_preprocessor.register_pass(ExcludeLayerNormNodes())\n",
    "    # Exclude the nodes constituting GELU\n",
    "    quantization_preprocessor.register_pass(ExcludeGeLUNodes())\n",
    "    # Exclude the residual connection Add nodes\n",
    "    quantization_preprocessor.register_pass(ExcludeNodeAfter(\"Add\", \"Add\"))\n",
    "    # Exclude the Add nodes following the Gather operator\n",
    "    quantization_preprocessor.register_pass(ExcludeNodeAfter(\"Gather\", \"Add\"))\n",
    "    # Exclude the Add nodes followed by the Softmax operator\n",
    "    quantization_preprocessor.register_pass(ExcludeNodeFollowedBy(\"Add\", \"Softmax\"))\n",
    "\n",
    "    return quantization_preprocessor\n",
    "\n",
    "\n",
    "# create processor\n",
    "quantization_preprocessor = create_quantization_preprocessor()\n",
    "\n",
    "# Quantize the same way we did for dynamic quantization!\n",
    "quantizer.quantize(\n",
    "    save_dir=onnx_path,\n",
    "    calibration_tensors_range=ranges,\n",
    "    quantization_config=qconfig,\n",
    "    preprocessor=quantization_preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1872cda2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T07:08:06.507571Z",
     "iopub.status.busy": "2025-05-16T07:08:06.507282Z",
     "iopub.status.idle": "2025-05-16T07:08:06.512541Z",
     "shell.execute_reply": "2025-05-16T07:08:06.511650Z"
    },
    "papermill": {
     "duration": 0.054813,
     "end_time": "2025-05-16T07:08:06.513882",
     "exception": false,
     "start_time": "2025-05-16T07:08:06.459069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 185.87 MB\n",
      "Quantized Model file size: 48.90 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "quantized_model = os.path.getsize(onnx_path / \"model_quantized.onnx\")/(1024*1024)\n",
    "\n",
    "print(f\"Model file size: {size:.2f} MB\")\n",
    "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be2081cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T07:08:06.626661Z",
     "iopub.status.busy": "2025-05-16T07:08:06.626304Z",
     "iopub.status.idle": "2025-05-16T07:08:07.351471Z",
     "shell.execute_reply": "2025-05-16T07:08:07.350773Z"
    },
    "papermill": {
     "duration": 0.774373,
     "end_time": "2025-05-16T07:08:07.353197",
     "exception": false,
     "start_time": "2025-05-16T07:08:06.578824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type timm_wrapper to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Too many ONNX model files were found in onnx/model.onnx ,onnx/model_quantized.onnx. specify which one to load by using the `file_name` and/or the `subfolder` arguments. Loading the file model_quantized.onnx in the subfolder onnx.\n"
     ]
    }
   ],
   "source": [
    "model = ORTModelForImageClassification.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74339c3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T07:08:07.435958Z",
     "iopub.status.busy": "2025-05-16T07:08:07.435681Z",
     "iopub.status.idle": "2025-05-16T07:09:13.538738Z",
     "shell.execute_reply": "2025-05-16T07:09:13.537902Z"
    },
    "papermill": {
     "duration": 66.18706,
     "end_time": "2025-05-16T07:09:13.580964",
     "exception": false,
     "start_time": "2025-05-16T07:08:07.393904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Initial memory state:\n",
      "\n",
      "--- Memory Usage (Before loading model) ---\n",
      "CPU RAM used: 8642.28 MB\n",
      "GPU 0 VRAM: 2205.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ” Initial memory state:\n",
      "\n",
      "--- Memory Usage (Model loaded) ---\n",
      "CPU RAM used: 8642.28 MB\n",
      "GPU 0 VRAM: 2205.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ§ª Benchmarking on CPU:\n",
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cpu...\n",
      "ğŸš€ Benchmarking (10 runs) on cpu...\n",
      "âœ… Average inference: 88.46 ms\n",
      "ğŸ“Š Total time: 884.65 ms | FPS: 11.3\n",
      "\n",
      "--- Memory Usage (After CPU benchmark) ---\n",
      "CPU RAM used: 8668.28 MB\n",
      "GPU 0 VRAM: 2205.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ® Benchmarking on GPU:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 07:08:09.935517113 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 116 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-16 07:08:09.960226282 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 07:08:09.960249430 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cuda...\n",
      "ğŸš€ Benchmarking (10 runs) on cuda...\n",
      "âœ… Average inference: 29.34 ms\n",
      "ğŸ“Š Total time: 293.44 ms | FPS: 34.1\n",
      "\n",
      "--- Memory Usage (After GPU test) ---\n",
      "CPU RAM used: 8643.50 MB\n",
      "GPU 0 VRAM: 2347.00 MB / 16384.00 MB\n",
      "\n",
      "âš¡ Benchmarking with AMP:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 07:08:11.224793031 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 116 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-16 07:08:11.248286262 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 07:08:11.248307229 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ Warming up (3 runs) on cuda...\n",
      "ğŸš€ Benchmarking (10 runs) on cuda...\n",
      "âœ… Average inference: 28.77 ms\n",
      "ğŸ“Š Total time: 287.74 ms | FPS: 34.8\n",
      "\n",
      "--- Memory Usage (After AMP test) ---\n",
      "CPU RAM used: 8643.50 MB\n",
      "GPU 0 VRAM: 2347.00 MB / 16384.00 MB\n",
      "\n",
      "ğŸ“ˆ Results Summary:\n",
      "| Device | Inference Time (ms) | Speedup vs CPU |\n",
      "|--------|---------------------|----------------|\n",
      "| CPU    |               88.46 |        â€”        |\n",
      "| GPU    |               29.34 |       3.0      x |\n",
      "| AMP    |               28.77 |       3.1      x |\n",
      "\n",
      "ğŸ“Š Loading ImageNetMini dataset...\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(256, 256))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "\n",
      "ğŸ§® Calculating metrics on GPU:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 07:08:13.849031148 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 116 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-16 07:08:13.872271522 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 07:08:13.872292008 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Quality Metrics Summary:\n",
      "| Device | Precision | Recall  | F1-Score |\n",
      "|--------|-----------|---------|----------|\n",
      "| GPU    | 0.6113  | 0.4248 | 0.4623  |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main(model, path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 547506,
     "sourceId": 998277,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2996.551361,
   "end_time": "2025-05-16T07:09:16.543411",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-16T06:19:19.992050",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0417cd863d4242558ecc253266b90971": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3f1df5dfc9c94bdd8f80d44ca1e22a45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4c55449c388f4a299a968700bd336ab2",
        "IPY_MODEL_e2f9a9a861ab4b5eb12668ff409c389e",
        "IPY_MODEL_dc65c2d0cc274441ae5ddb61c762241a"
       ],
       "layout": "IPY_MODEL_d7ff001d2a264100ad80738c21cfd6f8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "49c83188cbb246f0af44db77cfd463b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4c55449c388f4a299a968700bd336ab2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8b5de22a29240449fa7af89f76f5eee",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b35bb7e8c221451aa955eee29c767bc2",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:â€‡100%"
      }
     },
     "55a1988c1f5345e0a403f7ba59ca4930": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_571a22b815c940a193a2a57891f72f23",
        "IPY_MODEL_5ab6473d6c82446ab0bc3fa1e5c107c5",
        "IPY_MODEL_645d4831c9fd486cb12628b0f3455222"
       ],
       "layout": "IPY_MODEL_8924cccc68924856bb96e8f4d0f1a32e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "571a22b815c940a193a2a57891f72f23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f5cf5c32a75476b8e9bb086a4c5f69e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_49c83188cbb246f0af44db77cfd463b2",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:â€‡100%"
      }
     },
     "5ab6473d6c82446ab0bc3fa1e5c107c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f6f60e7206244655a53daa779d0caeaa",
       "max": 194838984.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0417cd863d4242558ecc253266b90971",
       "tabbable": null,
       "tooltip": null,
       "value": 194838984.0
      }
     },
     "636b7a05ebad403eaf2733cf1f0e5f25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "645d4831c9fd486cb12628b0f3455222": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6eb5a2b4dda34b92964b809bd1f20dd2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a66ff8d24975459195977f5106aa99b0",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡195M/195Mâ€‡[00:00&lt;00:00,â€‡348MB/s]"
      }
     },
     "6eb5a2b4dda34b92964b809bd1f20dd2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8924cccc68924856bb96e8f4d0f1a32e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f21d64a4cc8453c8a5be07e0cd33db2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f5cf5c32a75476b8e9bb086a4c5f69e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a397ee2b3af246a0bce2388165cf2ccd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a66ff8d24975459195977f5106aa99b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b35bb7e8c221451aa955eee29c767bc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b44cf567fffa4e5796e02879a4d52db4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8b5de22a29240449fa7af89f76f5eee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7ff001d2a264100ad80738c21cfd6f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc65c2d0cc274441ae5ddb61c762241a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b44cf567fffa4e5796e02879a4d52db4",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_8f21d64a4cc8453c8a5be07e0cd33db2",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡616/616â€‡[00:00&lt;00:00,â€‡69.8kB/s]"
      }
     },
     "e2f9a9a861ab4b5eb12668ff409c389e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_636b7a05ebad403eaf2733cf1f0e5f25",
       "max": 616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a397ee2b3af246a0bce2388165cf2ccd",
       "tabbable": null,
       "tooltip": null,
       "value": 616.0
      }
     },
     "f6f60e7206244655a53daa779d0caeaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
