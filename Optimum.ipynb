{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31f6419-fa61-4c04-a420-f79e2a34b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ab1b69-761b-4684-973e-2e758e5f6ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset clinc_oos (/root/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75153390cc6f45a2897423f36a52f7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a6d29f-65b5-4038-ae0d-065f3d6fc7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'what expression would i use to say i love you if i were an italian', 'intent': 61}\n"
     ]
    }
   ],
   "source": [
    "sample = clinc[\"train\"][0]\n",
    "print(sample)\n",
    "\n",
    "# Каждый объект датасета содержит текст и соответствующее ему намерение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8c51c-7f5d-4763-9c6a-90da0247552e",
   "metadata": {},
   "source": [
    "Намерения предоставляются в виде идентификаторов, но мы можем легко получить его значение (и наоборот), вызвав функцию int2str:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4896b70-0197-473c-a267-d657ac507b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate\n"
     ]
    }
   ],
   "source": [
    "intents = clinc[\"train\"].features[\"intent\"]\n",
    "intent = intents.int2str(sample[\"intent\"])\n",
    "print(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bee3af5-33bf-4a5f-8be8-923fbcb67716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1651/3849015595.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_score = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  predictions, labels = pred\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db4a8e0-544e-4802-a81f-24dc5503f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_labels = intents.num_classes\n",
    "\n",
    "checkpoint = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd15498-38db-451d-8c6a-4c71d545d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:  109598359\n"
     ]
    }
   ],
   "source": [
    "print(\"Base Model: \", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f929a6-157c-490d-b4e7-1841388c676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete a transaction from savings to checking of $20000\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "sample_input = clinc['train']['text'][101]\n",
    "\n",
    "print(clinc['train']['text'][101])\n",
    "print(clinc['train']['intent'][101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30f1068-5644-40f0-81a5-9f06f93f006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model, tokenizer='bert-base-uncased', device=0)\n",
    "\n",
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec269c2-35e8-4915-b4b4-3c29f206a175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее время обработки 100 запросов базовой моделью: 0.6840972900390625\n"
     ]
    }
   ],
   "source": [
    "#WARMUP\n",
    "for _ in range(10):\n",
    "  _ = pipe(sample_input)\n",
    "\n",
    "#INFERENCE\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "  _ = pipe(sample_input)\n",
    "total_time_model = time.time() - start\n",
    "print(\"Общее время обработки 100 запросов базовой моделью:\", total_time_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "005fdf50-b516-4c38-8b6b-7ac7d1da962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "data_test_X = clinc['test']['text'][::50]\n",
    "data_test_y = clinc['test']['intent'][::50]\n",
    "\n",
    "def show_accuracy(model: Callable):\n",
    "    model_preds = []\n",
    "    for i in tqdm(data_test_X):\n",
    "        model_preds.append(label2id[model(i)[0]['label']])\n",
    "\n",
    "    print(accuracy_score.compute(predictions=model_preds, references=data_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb660de2-37c3-4469-ad7d-601b8e2cf76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01533a3e704f47af9ba11856a81d24a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8363636363636363}\n"
     ]
    }
   ],
   "source": [
    "show_accuracy(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bec4e96-3020-4116-ac1c-d69a20e1377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"transformersbook/bert-base-uncased-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "127dc637-f414-4fd4-a540-1d723e4bfeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "from optimum.onnxruntime import ORTOptimizer, ORTModelForSequenceClassification\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a47afa8-565b-4e4a-b179-86ae3e804379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "sess = onnxruntime.InferenceSession(\"optimized_model/model_optimized.onnx\", providers=['CUDAExecutionProvider'])\n",
    "session_options = onnxruntime.SessionOptions()\n",
    "session_options.log_severity_level = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07043fd3-e24d-4aee-beba-7f10a3eb93c3",
   "metadata": {},
   "source": [
    "### Using optimum pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "242b22d7-3511-4b8e-bf2a-9fd61397d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.0.0+cu117\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n"
     ]
    }
   ],
   "source": [
    "optimum_model1 = pipeline(\n",
    "        task=\"text-classification\", \n",
    "        model=checkpoint, \n",
    "        tokenizer='bert-base-uncased', \n",
    "        accelerator=\"ort\",\n",
    "        device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f06b279b-176b-4ac4-95bc-70e963036017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее время обработки 100 запросов базовой моделью из optimum: 0.29105353355407715\n"
     ]
    }
   ],
   "source": [
    "#WARMUP\n",
    "for _ in range(10):\n",
    "  _ = optimum_model1(sample_input)\n",
    "\n",
    "#INFERENCE\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "  _ = optimum_model1(sample_input)\n",
    "total_time_model = time.time() - start\n",
    "print(\"Общее время обработки 100 запросов базовой моделью из optimum:\", total_time_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c74da-ce4a-4ea1-a2be-50e478487aee",
   "metadata": {},
   "source": [
    "### Using ORTModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eaebb37c-cb07-4c41-98fe-3d2d1ffde199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.0.0+cu117\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = checkpoint\n",
    "save_dir = \"optimized_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "optimum_model2 = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    export=True,\n",
    "    provider=\"CUDAExecutionProvider\",\n",
    "    session_options=session_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9106d4ab-5960-419b-8b2a-e66a47f082c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Optimizing model...\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n",
      "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed in shape inference <class 'AssertionError'>\n",
      "failed in shape inference <class 'AssertionError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in optimized_model/ort_config.json\n",
      "Optimized model saved at: optimized_model (external data format: False; saved all tensor to one file: True)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the optimization configuration detailing the optimization we wish to apply\n",
    "# optimization_config = AutoOptimizationConfig.O3()\n",
    "optimization_config = OptimizationConfig(\n",
    "    # optimize_for_gpu=True,\n",
    "    optimization_level=99\n",
    ")\n",
    "optimizer = ORTOptimizer.from_pretrained(optimum_model2)\n",
    "\n",
    "optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config)\n",
    "# Load the optimized model from a local repository\n",
    "optimized_model1 = ORTModelForSequenceClassification.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e53b466d-4423-4b36-a629-72e87418df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n"
     ]
    }
   ],
   "source": [
    "# Create the transformers pipeline\n",
    "onnx_clx = pipeline(\"text-classification\", model=optimized_model1, accelerator=\"ort\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "471143d0-3c1c-4ba5-b531-b73de75474bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее время обработки 100 запросов базовой моделью: 0.2588021755218506\n"
     ]
    }
   ],
   "source": [
    "#WARMUP\n",
    "for _ in range(10):\n",
    "  _ = onnx_clx(sample_input)\n",
    "\n",
    "#INFERENCE\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "  _ = onnx_clx(sample_input)\n",
    "total_time_model = time.time() - start\n",
    "print(\"Общее время обработки 100 запросов базовой моделью:\", total_time_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcaadddd-285d-49af-80d3-e3cc0322c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19783ac0f6dc41d49e199d3c1edcab76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8363636363636363}\n"
     ]
    }
   ],
   "source": [
    "show_accuracy(onnx_clx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6aee0-74f3-438e-9a74-0f6ba49cf06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
