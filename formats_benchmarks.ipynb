{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":998277,"datasetId":547506,"databundleVersionId":1026923}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gputil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:32:18.441691Z","iopub.execute_input":"2025-04-24T12:32:18.442032Z","iopub.status.idle":"2025-04-24T12:32:24.452838Z","shell.execute_reply.started":"2025-04-24T12:32:18.442015Z","shell.execute_reply":"2025-04-24T12:32:24.452130Z"}},"outputs":[{"name":"stdout","text":"Collecting gputil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: gputil\n  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=9f5a3ca7b72a57d0f3db23f00606b24aa628e53a28150c9967f4817ac7d61aa7\n  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\nSuccessfully built gputil\nInstalling collected packages: gputil\nSuccessfully installed gputil-1.4.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall onnxruntime -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:21:15.170034Z","iopub.execute_input":"2025-04-24T11:21:15.170417Z","iopub.status.idle":"2025-04-24T11:21:16.502660Z","shell.execute_reply.started":"2025-04-24T11:21:15.170387Z","shell.execute_reply":"2025-04-24T11:21:16.501761Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip uninstall onnxruntime-gpu -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:21:33.891044Z","iopub.execute_input":"2025-04-24T11:21:33.891722Z","iopub.status.idle":"2025-04-24T11:21:35.092985Z","shell.execute_reply.started":"2025-04-24T11:21:33.891694Z","shell.execute_reply":"2025-04-24T11:21:35.092278Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: onnxruntime-gpu 1.21.1\nUninstalling onnxruntime-gpu-1.21.1:\n  Successfully uninstalled onnxruntime-gpu-1.21.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install optimum[onnxruntime-gpu]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"ifigotin/imagenetmini-1000\")\npath += \"/imagenet-mini\"\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:32:24.453789Z","iopub.execute_input":"2025-04-24T12:32:24.453997Z","iopub.status.idle":"2025-04-24T12:32:24.716030Z","shell.execute_reply.started":"2025-04-24T12:32:24.453980Z","shell.execute_reply":"2025-04-24T12:32:24.715405Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/imagenetmini-1000/imagenet-mini\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport onnxruntime as ort\nimport onnx\nimport time\nimport timm\nimport psutil\nimport os\nimport GPUtil\nimport numpy as np\nfrom torch.amp import autocast\nfrom torchvision import datasets, transforms\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef print_memory_usage(label=\"\"):\n    \"\"\"–í—ã–≤–æ–¥–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –º–µ–≥–∞–±–∞–π—Ç–∞—Ö\"\"\"\n    if label:\n        print(f\"\\n--- Memory Usage ({label}) ---\")\n    else:\n        print(\"\\n--- Memory Usage ---\")\n        \n    # CPU RAM –≤ MB\n    process = psutil.Process(os.getpid())\n    ram_used = process.memory_info().rss / (1024 ** 2)\n    print(f\"CPU RAM used: {ram_used:.2f} MB\")\n    \n    # GPU VRAM –≤ MB\n    gpus = GPUtil.getGPUs()\n    for gpu in gpus:\n        vram_used = gpu.memoryUsed\n        vram_total = gpu.memoryTotal\n        print(f\"GPU {gpu.id} VRAM: {vram_used:.2f} MB / {vram_total:.2f} MB\")\n\ndef get_model_size(model):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    return (param_size + buffer_size) / 1024**2\n\ndef calculate_metrics(model, device, data_loader):\n    model.eval()\n    all_preds = []\n    all_targets = []\n\n    model.to(device)\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n            \n            all_preds.extend(preds)\n            all_targets.extend(targets.numpy())\n    \n    precision = precision_score(all_targets, all_preds, average='macro')\n    recall = recall_score(all_targets, all_preds, average='macro')\n    f1 = f1_score(all_targets, all_preds, average='macro')\n    \n    return precision, recall, f1\n\ndef load_imagenet_mini(dataset_path, model):\n    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏\n    config = resolve_data_config({}, model=model)\n    transform = create_transform(**config)\n    \n    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n    dataset = datasets.ImageFolder(\n        root=os.path.join(dataset_path, 'val'),\n        transform=transform\n    )\n    \n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=64,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    return data_loader\n\ndef benchmark_model(model, device, input_tensor, num_runs=10, warmup=3, use_amp=False):\n    model = model.to(device)\n    input_tensor = input_tensor.to(device)\n    \n    # Warmup\n    print(f\"\\nüî• Warming up ({warmup} runs) on {device}...\")\n    for _ in range(warmup):\n        with torch.no_grad():\n            if use_amp and device.type == 'cuda':\n                with autocast(device_type='cuda', dtype=torch.float16):\n                    _ = model(input_tensor)\n            else:\n                _ = model(input_tensor)\n    \n    # Benchmark\n    print(f\"üöÄ Benchmarking ({num_runs} runs) on {device}...\")\n    start_time = time.time()\n    \n    for _ in range(num_runs):\n        with torch.no_grad():\n            if use_amp and device.type == 'cuda':\n                with autocast(device_type='cuda', dtype=torch.float16):\n                    _ = model(input_tensor)\n            else:\n                _ = model(input_tensor)\n    \n    total_time = (time.time() - start_time) * 1000\n    avg_time = total_time / num_runs\n    print(f\"‚úÖ Average inference: {avg_time:.2f} ms\")\n    print(f\"üìä Total time: {total_time:.2f} ms | FPS: {1000/(avg_time + 1e-9):.1f}\")\n    \n    return avg_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:32:39.727686Z","iopub.execute_input":"2025-04-24T12:32:39.728181Z","iopub.status.idle":"2025-04-24T12:32:40.268929Z","shell.execute_reply.started":"2025-04-24T12:32:39.728156Z","shell.execute_reply":"2025-04-24T12:32:40.268330Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model = timm.create_model('efficientvit_b3.r256_in1k', pretrained=True).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:32:57.399478Z","iopub.execute_input":"2025-04-24T12:32:57.400302Z","iopub.status.idle":"2025-04-24T12:33:00.057544Z","shell.execute_reply.started":"2025-04-24T12:32:57.400278Z","shell.execute_reply":"2025-04-24T12:33:00.056914Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/195M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"287b734dd8d64a418f22f3e072346efd"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Baseline","metadata":{}},{"cell_type":"code","source":"print(f\"üìè Model size: {get_model_size(model):.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:27:43.352561Z","iopub.execute_input":"2025-04-24T12:27:43.352885Z","iopub.status.idle":"2025-04-24T12:27:43.360273Z","shell.execute_reply.started":"2025-04-24T12:27:43.352846Z","shell.execute_reply":"2025-04-24T12:27:43.359641Z"}},"outputs":[{"name":"stdout","text":"üìè Model size: 185.75 MB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print_memory_usage(\"Model loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:27:46.929036Z","iopub.execute_input":"2025-04-24T12:27:46.929923Z","iopub.status.idle":"2025-04-24T12:27:46.956829Z","shell.execute_reply.started":"2025-04-24T12:27:46.929880Z","shell.execute_reply":"2025-04-24T12:27:46.956197Z"}},"outputs":[{"name":"stdout","text":"\n--- Memory Usage (Model loaded) ---\nCPU RAM used: 932.78 MB\nGPU 0 VRAM: 0.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"device_cpu = torch.device('cpu')\ndevice_gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:20.056608Z","iopub.execute_input":"2025-04-24T12:33:20.056876Z","iopub.status.idle":"2025-04-24T12:33:20.126913Z","shell.execute_reply.started":"2025-04-24T12:33:20.056857Z","shell.execute_reply":"2025-04-24T12:33:20.126128Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"input_tensor = torch.randn(1, 3, 224, 224)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:35.418602Z","iopub.execute_input":"2025-04-24T12:33:35.419132Z","iopub.status.idle":"2025-04-24T12:33:35.423804Z","shell.execute_reply.started":"2025-04-24T12:33:35.419088Z","shell.execute_reply":"2025-04-24T12:33:35.423146Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(\"\\nBenchmarking on CPU:\")\ncpu_time = benchmark_model(model, device_cpu, input_tensor)\nprint_memory_usage(\"After CPU benchmark\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:27:48.998544Z","iopub.execute_input":"2025-04-24T12:27:48.998811Z","iopub.status.idle":"2025-04-24T12:27:50.791202Z","shell.execute_reply.started":"2025-04-24T12:27:48.998792Z","shell.execute_reply":"2025-04-24T12:27:50.790499Z"}},"outputs":[{"name":"stdout","text":"\nBenchmarking on CPU:\n\nüî• Warming up (3 runs) on cpu...\nüöÄ Benchmarking (10 runs) on cpu...\n‚úÖ Average inference: 122.18 ms\nüìä Total time: 1221.78 ms | FPS: 8.2\n\n--- Memory Usage (After CPU benchmark) ---\nCPU RAM used: 975.90 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"gpu_time = benchmark_model(model, device_gpu, input_tensor)\nprint_memory_usage(\"After GPU test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:27:50.792421Z","iopub.execute_input":"2025-04-24T12:27:50.792676Z","iopub.status.idle":"2025-04-24T12:27:52.041247Z","shell.execute_reply.started":"2025-04-24T12:27:50.792659Z","shell.execute_reply":"2025-04-24T12:27:52.040646Z"}},"outputs":[{"name":"stdout","text":"\nüî• Warming up (3 runs) on cuda...\nüöÄ Benchmarking (10 runs) on cuda...\n‚úÖ Average inference: 22.45 ms\nüìä Total time: 224.53 ms | FPS: 44.5\n\n--- Memory Usage (After GPU test) ---\nCPU RAM used: 1098.04 MB\nGPU 0 VRAM: 539.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"gpu_amp_time = benchmark_model(model, device_gpu, input_tensor, use_amp=True)\nprint_memory_usage(\"After AMP test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:27:52.041870Z","iopub.execute_input":"2025-04-24T12:27:52.042105Z","iopub.status.idle":"2025-04-24T12:27:52.508892Z","shell.execute_reply.started":"2025-04-24T12:27:52.042088Z","shell.execute_reply":"2025-04-24T12:27:52.508235Z"}},"outputs":[{"name":"stdout","text":"\nüî• Warming up (3 runs) on cuda...\nüöÄ Benchmarking (10 runs) on cuda...\n‚úÖ Average inference: 27.16 ms\nüìä Total time: 271.59 ms | FPS: 36.8\n\n--- Memory Usage (After AMP test) ---\nCPU RAM used: 1116.79 MB\nGPU 0 VRAM: 615.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"data_loader = load_imagenet_mini(path, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:04.405921Z","iopub.execute_input":"2025-04-24T12:33:04.406242Z","iopub.status.idle":"2025-04-24T12:33:10.299826Z","shell.execute_reply.started":"2025-04-24T12:33:04.406220Z","shell.execute_reply":"2025-04-24T12:33:10.299081Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"precision_gpu, recall_gpu, f1_gpu = calculate_metrics(model, device_gpu, data_loader)\nprint(f\"| {precision_gpu:.4f}  | {recall_gpu:.4f} | {f1_gpu:.4f}  |\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:24:05.574277Z","iopub.execute_input":"2025-04-24T11:24:05.574493Z","iopub.status.idle":"2025-04-24T11:24:25.364152Z","shell.execute_reply.started":"2025-04-24T11:24:05.574477Z","shell.execute_reply":"2025-04-24T11:24:25.363188Z"}},"outputs":[{"name":"stdout","text":"| 0.8474  | 0.8342 | 0.8239  |\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Torchscript","metadata":{}},{"cell_type":"code","source":"model = model.to(device_cpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:25.251486Z","iopub.execute_input":"2025-04-24T12:33:25.252061Z","iopub.status.idle":"2025-04-24T12:33:25.264773Z","shell.execute_reply.started":"2025-04-24T12:33:25.252025Z","shell.execute_reply":"2025-04-24T12:33:25.263932Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"traiced_model = torch.jit.trace(model, input_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:43.717349Z","iopub.execute_input":"2025-04-24T12:33:43.717628Z","iopub.status.idle":"2025-04-24T12:33:48.992481Z","shell.execute_reply.started":"2025-04-24T12:33:43.717610Z","shell.execute_reply":"2025-04-24T12:33:48.991691Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"torch.jit.save(traiced_model, \"traiced_model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:48.995761Z","iopub.execute_input":"2025-04-24T12:33:48.995985Z","iopub.status.idle":"2025-04-24T12:33:49.706135Z","shell.execute_reply.started":"2025-04-24T12:33:48.995968Z","shell.execute_reply":"2025-04-24T12:33:49.705309Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print_memory_usage(\"Before model loaded\")\nmodel_ts = torch.jit.load(\"traiced_model.pt\")\nprint_memory_usage(\"After model loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:49.707904Z","iopub.execute_input":"2025-04-24T12:33:49.708256Z","iopub.status.idle":"2025-04-24T12:33:50.339051Z","shell.execute_reply.started":"2025-04-24T12:33:49.708227Z","shell.execute_reply":"2025-04-24T12:33:50.338443Z"}},"outputs":[{"name":"stdout","text":"\n--- Memory Usage (Before model loaded) ---\nCPU RAM used: 1291.32 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n\n--- Memory Usage (After model loaded) ---\nCPU RAM used: 1366.32 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(f\"üìè Model size: {get_model_size(traiced_model):.2f} MB\")\ncpu_time = benchmark_model(traiced_model, device_cpu, input_tensor)\nprint_memory_usage(\"After CPU benchmark\")\ngpu_time = benchmark_model(traiced_model, device_gpu, input_tensor)\nprint_memory_usage(\"After GPU test\")\ngpu_amp_time = benchmark_model(traiced_model, device_gpu, input_tensor, use_amp=True)\nprint_memory_usage(\"After AMP test\")\nprecision_gpu, recall_gpu, f1_gpu = calculate_metrics(traiced_model, device_gpu, data_loader)\nprint(f\"| {precision_gpu:.4f}  | {recall_gpu:.4f} | {f1_gpu:.4f}  |\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:33:50.340421Z","iopub.execute_input":"2025-04-24T12:33:50.341126Z","iopub.status.idle":"2025-04-24T12:34:19.626745Z","shell.execute_reply.started":"2025-04-24T12:33:50.341077Z","shell.execute_reply":"2025-04-24T12:34:19.625921Z"}},"outputs":[{"name":"stdout","text":"üìè Model size: 185.75 MB\n\nüî• Warming up (3 runs) on cpu...\nüöÄ Benchmarking (10 runs) on cpu...\n‚úÖ Average inference: 100.95 ms\nüìä Total time: 1009.47 ms | FPS: 9.9\n\n--- Memory Usage (After CPU benchmark) ---\nCPU RAM used: 1390.82 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n\nüî• Warming up (3 runs) on cuda...\nüöÄ Benchmarking (10 runs) on cuda...\n‚úÖ Average inference: 15.92 ms\nüìä Total time: 159.17 ms | FPS: 62.8\n\n--- Memory Usage (After GPU test) ---\nCPU RAM used: 1481.03 MB\nGPU 0 VRAM: 537.00 MB / 16384.00 MB\n\nüî• Warming up (3 runs) on cuda...\nüöÄ Benchmarking (10 runs) on cuda...\n‚úÖ Average inference: 19.36 ms\nüìä Total time: 193.58 ms | FPS: 51.7\n\n--- Memory Usage (After AMP test) ---\nCPU RAM used: 1601.78 MB\nGPU 0 VRAM: 537.00 MB / 16384.00 MB\n| 0.8474  | 0.8342 | 0.8239  |\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# ONNX","metadata":{}},{"cell_type":"code","source":"opset_version = max(int(k[14:]) for k in vars(torch.onnx) if \"symbolic_opset\" in k) - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:24:55.803424Z","iopub.execute_input":"2025-04-24T11:24:55.804191Z","iopub.status.idle":"2025-04-24T11:24:55.808147Z","shell.execute_reply.started":"2025-04-24T11:24:55.804163Z","shell.execute_reply":"2025-04-24T11:24:55.807498Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"torch.onnx.export(\n    model.to(device_cpu), input_tensor, \"model.onnx\",\n    export_params=True,\n    opset_version=opset_version,\n    do_constant_folding=True,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\n        \"input\":  {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n        \"output\": {0: \"batch_size\"},\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:00.011512Z","iopub.execute_input":"2025-04-24T11:25:00.012093Z","iopub.status.idle":"2025-04-24T11:25:03.673669Z","shell.execute_reply.started":"2025-04-24T11:25:00.012070Z","shell.execute_reply":"2025-04-24T11:25:03.673099Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:663: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n  _C._jit_pass_onnx_graph_shape_type_inference(\n/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n  _C._jit_pass_onnx_graph_shape_type_inference(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"model_onnx = onnx.load(\"model.onnx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:03.675610Z","iopub.execute_input":"2025-04-24T11:25:03.675803Z","iopub.status.idle":"2025-04-24T11:25:04.089378Z","shell.execute_reply.started":"2025-04-24T11:25:03.675788Z","shell.execute_reply":"2025-04-24T11:25:04.088817Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def get_onnx_model_size(path: str) -> float:\n    size_bytes = os.path.getsize(path)\n    return size_bytes / (1024 ** 2)\ndef load_onnx_session(path: str, use_gpu: bool = True) -> ort.InferenceSession:\n    providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if use_gpu else [\"CPUExecutionProvider\"]\n    return ort.InferenceSession(path, providers=providers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:09.298369Z","iopub.execute_input":"2025-04-24T11:25:09.298653Z","iopub.status.idle":"2025-04-24T11:25:09.304210Z","shell.execute_reply.started":"2025-04-24T11:25:09.298632Z","shell.execute_reply":"2025-04-24T11:25:09.303467Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def benchmark_onnx(session, input_array: np.ndarray, num_runs=10, warmup=3):\n    input_name = session.get_inputs()[0].name\n    # warmup\n    for _ in range(warmup):\n        session.run(None, {input_name: input_array})\n\n    start = time.time()\n    for _ in range(num_runs):\n        session.run(None, {input_name: input_array})\n    return (time.time() - start) * 1000 / num_runs  # ms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:13.665615Z","iopub.execute_input":"2025-04-24T11:25:13.666282Z","iopub.status.idle":"2025-04-24T11:25:13.670582Z","shell.execute_reply.started":"2025-04-24T11:25:13.666257Z","shell.execute_reply":"2025-04-24T11:25:13.669842Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def calculate_metrics_onnx(session, data_loader):\n    all_preds, all_targets = [], []\n    in_name = session.get_inputs()[0].name\n    for images, targets in data_loader:\n        arr = images.cpu().numpy()\n        out = session.run(None, {in_name: arr})[0]\n        preds = out.argmax(axis=1)\n        all_preds.extend(preds)\n        all_targets.extend(targets.numpy())\n    return (\n        precision_score(all_targets, all_preds, average='macro'),\n        recall_score(all_targets, all_preds, average='macro'),\n        f1_score(all_targets, all_preds, average='macro'),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:14.882688Z","iopub.execute_input":"2025-04-24T11:25:14.883272Z","iopub.status.idle":"2025-04-24T11:25:14.887949Z","shell.execute_reply.started":"2025-04-24T11:25:14.883246Z","shell.execute_reply":"2025-04-24T11:25:14.887365Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"onnx_path = \"model.onnx\"\nprint(f\"üìè ONNX model size: {get_onnx_model_size(onnx_path):.2f} MB\")\nprint_memory_usage(\"Before ONNX load\")\n\nsess_cpu = load_onnx_session(onnx_path, use_gpu=False)\nprint_memory_usage(\"ONNX session (CPU) loaded\")\ncpu_time = benchmark_onnx(sess_cpu, input_tensor.cpu().numpy())\nprint(f\"‚úÖ CPU inference avg: {cpu_time:.2f} ms\")\nprint_memory_usage(\"After ONNX CPU benchmark\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:24.952446Z","iopub.execute_input":"2025-04-24T11:25:24.953056Z","iopub.status.idle":"2025-04-24T11:25:26.698482Z","shell.execute_reply.started":"2025-04-24T11:25:24.953030Z","shell.execute_reply":"2025-04-24T11:25:26.697809Z"}},"outputs":[{"name":"stdout","text":"üìè ONNX model size: 185.87 MB\n\n--- Memory Usage (Before ONNX load) ---\nCPU RAM used: 2590.98 MB\nGPU 0 VRAM: 2201.00 MB / 16384.00 MB\n\n--- Memory Usage (ONNX session (CPU) loaded) ---\nCPU RAM used: 2776.54 MB\nGPU 0 VRAM: 2201.00 MB / 16384.00 MB\n‚úÖ CPU inference avg: 74.61 ms\n\n--- Memory Usage (After ONNX CPU benchmark) ---\nCPU RAM used: 2776.54 MB\nGPU 0 VRAM: 2201.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"sess_gpu = load_onnx_session(onnx_path, use_gpu=True)\nprint_memory_usage(\"ONNX session (GPU) loaded\")\ngpu_time = benchmark_onnx(sess_gpu, input_tensor.cpu().numpy())\nprint(f\"‚úÖ GPU inference avg: {gpu_time:.2f} ms\")\nprint_memory_usage(\"After ONNX GPU benchmark\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:29.669745Z","iopub.execute_input":"2025-04-24T11:25:29.670297Z","iopub.status.idle":"2025-04-24T11:25:30.486460Z","shell.execute_reply.started":"2025-04-24T11:25:29.670273Z","shell.execute_reply":"2025-04-24T11:25:30.485373Z"}},"outputs":[{"name":"stdout","text":"\n--- Memory Usage (ONNX session (GPU) loaded) ---\nCPU RAM used: 2791.92 MB\nGPU 0 VRAM: 2469.00 MB / 16384.00 MB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[0;93m2025-04-24 11:25:29.863311055 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 30 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ GPU inference avg: 19.50 ms\n\n--- Memory Usage (After ONNX GPU benchmark) ---\nCPU RAM used: 2807.17 MB\nGPU 0 VRAM: 2479.00 MB / 16384.00 MB\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"precision_onnx, recall_onnx, f1_onnx = calculate_metrics_onnx(sess_gpu, data_loader)\nprint(f\"| {precision_onnx:.4f} | {recall_onnx:.4f} | {f1_onnx:.4f} |\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:25:42.450756Z","iopub.execute_input":"2025-04-24T11:25:42.451342Z","iopub.status.idle":"2025-04-24T11:26:13.945709Z","shell.execute_reply.started":"2025-04-24T11:25:42.451318Z","shell.execute_reply":"2025-04-24T11:26:13.944732Z"}},"outputs":[{"name":"stdout","text":"| 0.8474 | 0.8342 | 0.8239 |\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# OpenVINO","metadata":{}},{"cell_type":"code","source":"!pip install openvino-dev[onnx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from openvino.tools.mo import convert_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:11.231849Z","iopub.execute_input":"2025-04-24T11:56:11.232464Z","iopub.status.idle":"2025-04-24T11:56:11.236065Z","shell.execute_reply.started":"2025-04-24T11:56:11.232439Z","shell.execute_reply":"2025-04-24T11:56:11.235421Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"from openvino.runtime import serialize\nfrom openvino.runtime import Core","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:11.412311Z","iopub.execute_input":"2025-04-24T11:56:11.412527Z","iopub.status.idle":"2025-04-24T11:56:11.415872Z","shell.execute_reply.started":"2025-04-24T11:56:11.412511Z","shell.execute_reply":"2025-04-24T11:56:11.415181Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"converted = convert_model(\n    \"model.onnx\",\n    output_dir=\"openvino_model\",\n    input_shape=[1, 3, 224, 224],\n    compress_to_fp16=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:11.812835Z","iopub.execute_input":"2025-04-24T11:56:11.813120Z","iopub.status.idle":"2025-04-24T11:56:13.767468Z","shell.execute_reply.started":"2025-04-24T11:56:11.813089Z","shell.execute_reply":"2025-04-24T11:56:13.633064Z"}},"outputs":[{"name":"stdout","text":"[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\nIn 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \nFind more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"serialize(\n    converted,\n    \"openvino_model/model.xml\",\n    \"openvino_model/model.bin\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:22.583144Z","iopub.execute_input":"2025-04-24T11:56:22.583649Z","iopub.status.idle":"2025-04-24T11:56:23.033669Z","shell.execute_reply.started":"2025-04-24T11:56:22.583615Z","shell.execute_reply":"2025-04-24T11:56:23.032734Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"!ls ./openvino_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:26.081059Z","iopub.execute_input":"2025-04-24T11:56:26.081675Z","iopub.status.idle":"2025-04-24T11:56:26.299996Z","shell.execute_reply.started":"2025-04-24T11:56:26.081654Z","shell.execute_reply":"2025-04-24T11:56:26.299067Z"}},"outputs":[{"name":"stdout","text":"model.bin  model.xml  openvino_model\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"def print_memory_usage(label=\"\"):\n    if label:\n        print(f\"\\n--- Memory Usage ({label}) ---\")\n    else:\n        print(\"\\n--- Memory Usage ---\")\n    process = psutil.Process(os.getpid())\n    ram_used = process.memory_info().rss / (1024 ** 2)\n    print(f\"CPU RAM used: {ram_used:.2f} MB\")\n\ndef get_openvino_model_size(ir_dir: str) -> float:\n    xml = os.path.join(ir_dir, \"model.xml\")\n    binf = os.path.join(ir_dir, \"model.bin\")\n    total = os.path.getsize(xml) + os.path.getsize(binf)\n    return total / (1024 ** 2)\n\ndef load_openvino_model(ir_dir: str, device: str = \"CPU\"):\n    \"\"\"\n    –°–æ–∑–¥–∞—ë—Ç –∏ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç OpenVINO-–º–æ–¥–µ–ª—å.\n    \"\"\"\n    ie = Core()\n    model = ie.read_model(model=os.path.join(ir_dir, \"model.xml\"))\n    compiled = ie.compile_model(model=model, device_name=device)\n    input_name = compiled.input(0).get_any_name()\n    output_name = compiled.output(0).get_any_name()\n    return compiled, input_name, output_name\n\ndef benchmark_openvino(compiled, input_name: str, input_np: np.ndarray,\n                      num_runs: int = 10, warmup: int = 3) -> float:\n    \"\"\"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤ –º—Å.\"\"\"\n    # warmup\n    for _ in range(warmup):\n        compiled([input_np])\n\n    start = time.time()\n    for _ in range(num_runs):\n        compiled([input_np])\n    avg_ms = (time.time() - start) * 1000 / num_runs\n    return avg_ms\n\ndef calculate_metrics_openvino(compiled, input_name: str, output_name: str,\n                               data_loader) -> tuple:\n    \"\"\"Precision, recall, f1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ.\"\"\"\n    all_preds, all_targets = [], []\n    for images, targets in data_loader:\n        # –ü—Ä–∏–≤–æ–¥–∏–º –±–∞—Ç—á –∫ numpy –∏ –¥–µ–ª–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n        inp = images.cpu().numpy()\n        res = compiled([inp])[output_name]\n        preds = np.argmax(res, axis=1)\n        all_preds.extend(preds)\n        all_targets.extend(targets.numpy())\n    return (\n        precision_score(all_targets, all_preds, average='macro'),\n        recall_score(all_targets, all_preds, average='macro'),\n        f1_score(all_targets, all_preds, average='macro'),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:28.967358Z","iopub.execute_input":"2025-04-24T11:56:28.967672Z","iopub.status.idle":"2025-04-24T11:56:28.978063Z","shell.execute_reply.started":"2025-04-24T11:56:28.967644Z","shell.execute_reply":"2025-04-24T11:56:28.977277Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"ir_dir = \"./openvino_model\"\ninput_np = input_tensor.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:30.622367Z","iopub.execute_input":"2025-04-24T11:56:30.622652Z","iopub.status.idle":"2025-04-24T11:56:30.626768Z","shell.execute_reply.started":"2025-04-24T11:56:30.622631Z","shell.execute_reply":"2025-04-24T11:56:30.626190Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"size_mb = get_openvino_model_size(ir_dir)\nprint(f\"üìè OpenVINO IR size: {size_mb:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:33.302498Z","iopub.execute_input":"2025-04-24T11:56:33.303238Z","iopub.status.idle":"2025-04-24T11:56:33.307515Z","shell.execute_reply.started":"2025-04-24T11:56:33.303212Z","shell.execute_reply":"2025-04-24T11:56:33.306705Z"}},"outputs":[{"name":"stdout","text":"üìè OpenVINO IR size: 186.31 MB\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"print_memory_usage(\"before load\")\ncompiled_cpu, in_name, out_name = load_openvino_model(ir_dir, device=\"CPU\")\nprint_memory_usage(\"after load (CPU)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:34.362731Z","iopub.execute_input":"2025-04-24T11:56:34.363489Z","iopub.status.idle":"2025-04-24T11:56:34.957996Z","shell.execute_reply.started":"2025-04-24T11:56:34.363463Z","shell.execute_reply":"2025-04-24T11:56:34.957293Z"}},"outputs":[{"name":"stdout","text":"\n--- Memory Usage (before load) ---\nCPU RAM used: 4284.18 MB\n\n--- Memory Usage (after load (CPU)) ---\nCPU RAM used: 4675.43 MB\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"cpu_ms = benchmark_openvino(compiled_cpu, in_name, input_np)\nprint(f\"‚úÖ CPU inference avg: {cpu_ms:.2f} ms\")\nprint_memory_usage(\"after CPU benchmark\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:37.306858Z","iopub.execute_input":"2025-04-24T11:56:37.307189Z","iopub.status.idle":"2025-04-24T11:56:37.985003Z","shell.execute_reply.started":"2025-04-24T11:56:37.307166Z","shell.execute_reply":"2025-04-24T11:56:37.984362Z"}},"outputs":[{"name":"stdout","text":"‚úÖ CPU inference avg: 50.52 ms\n\n--- Memory Usage (after CPU benchmark) ---\nCPU RAM used: 4684.55 MB\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"compiled_gpu, in_name, out_name = load_openvino_model(ir_dir, device=\"GPU\")\nprint_memory_usage(\"after load (GPU)\")\ngpu_ms = benchmark_openvino(compiled_gpu, in_name, input_np)\nprint(f\"‚úÖ GPU inference avg: {gpu_ms:.2f} ms\")\nprint_memory_usage(\"after GPU benchmark\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:56:53.361723Z","iopub.execute_input":"2025-04-24T11:56:53.362425Z","iopub.status.idle":"2025-04-24T11:56:54.203641Z","shell.execute_reply.started":"2025-04-24T11:56:53.362392Z","shell.execute_reply":"2025-04-24T11:56:54.202722Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_513/774144953.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompiled_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_openvino_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after load (GPU)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgpu_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_openvino\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ GPU inference avg: {gpu_ms:.2f} ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after GPU benchmark\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_513/2357172922.py\u001b[0m in \u001b[0;36mload_openvino_model\u001b[0;34m(ir_dir, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcompiled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0minput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_any_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moutput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_any_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(self, model, device_name, config, weights)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 )\n\u001b[1;32m    542\u001b[0m             return CompiledModel(\n\u001b[0;32m--> 543\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m             )\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/cpp/core.cpp:104:\nCheck '!m_device_map.empty()' failed at src/plugins/intel_gpu/src/plugin/plugin.cpp:419:\n[GPU] Can't get PERFORMANCE_HINT property as no supported devices found or an error happened during devices query.\n[GPU] Please check OpenVINO documentation for GPU drivers setup guide.\n\n\n"],"ename":"RuntimeError","evalue":"Exception from src/inference/src/cpp/core.cpp:104:\nCheck '!m_device_map.empty()' failed at src/plugins/intel_gpu/src/plugin/plugin.cpp:419:\n[GPU] Can't get PERFORMANCE_HINT property as no supported devices found or an error happened during devices query.\n[GPU] Please check OpenVINO documentation for GPU drivers setup guide.\n\n\n","output_type":"error"}],"execution_count":91},{"cell_type":"code","source":"precision, recall, f1 = calculate_metrics_openvino(compiled_cpu, in_name, out_name, data_loader)\nprint(f\"| {precision:.4f} | {recall:.4f} | {f1:.4f} |\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T12:25:28.882383Z","iopub.execute_input":"2025-04-24T12:25:28.882655Z","iopub.status.idle":"2025-04-24T12:25:28.887572Z","shell.execute_reply.started":"2025-04-24T12:25:28.882639Z","shell.execute_reply":"2025-04-24T12:25:28.886910Z"}},"outputs":[{"name":"stdout","text":"| 0.8474 | 0.8342 | 0.8239 |\n","output_type":"stream"}],"execution_count":95}]}