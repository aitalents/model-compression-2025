{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf \n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "from datasets import load_dataset \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: CUDA\n",
      "ONNX Runtime будет использовать CUDAExecutionProvider (GPU)\n",
      "ID модели: openai/whisper-large-v3\n",
      "Папка для ONNX: ./whisper-large-v3-onnx\n",
      "Путь к аудиофайлу: sample.wav\n"
     ]
    }
   ],
   "source": [
    "model_id = \"openai/whisper-large-v3\"\n",
    "onnx_output_dir = \"./whisper-large-v3-onnx\" \n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Используемое устройство: {device.upper()}\")\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    onnx_provider = \"CUDAExecutionProvider\"\n",
    "    print(\"ONNX Runtime будет использовать CUDAExecutionProvider (GPU)\")\n",
    "else:\n",
    "    onnx_provider = \"CPUExecutionProvider\"\n",
    "    print(\"ONNX Runtime будет использовать CPUExecutionProvider (CPU)\")\n",
    "\n",
    "audio_path = \"sample.wav\"\n",
    "\n",
    "n_warmup_runs = 1 \n",
    "n_timed_runs = 3  \n",
    "\n",
    "print(f\"ID модели: {model_id}\")\n",
    "print(f\"Папка для ONNX: {onnx_output_dir}\")\n",
    "print(f\"Путь к аудиофайлу: {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Загрузка данных для WER ВРУЧНУЮ из ЛОКАЛЬНОЙ ПАПКИ ---\n",
      "Поиск данных в директории: /home/ubuntu/model-compression-2025/data/LibriSpeech/test-clean\n",
      "Начинаем обход директорий и чтение файлов...\n",
      "\n",
      "Собрано 15 сэмплов.\n",
      "\n",
      "Успешно загружено и подготовлено 15 аудио и 15 эталонных текстов.\n",
      "Пример эталонного текста (0): WHAT THE LATTER DAY SAINTS CALL CELESTIAL MARRIAGE IS CHARACTERISTIC OF THE CHURCH AND IS IN VERY GENERAL PRACTISE BUT OF CELESTIAL MARRIAGE PLURALITY OF WIVES WAS AN INCIDENT NEVER AN ESSENTIAL\n",
      "Пример формы аудио (0): (203999,), Тип: float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import soundfile as sf \n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "print(\"\\n--- Загрузка данных для WER ВРУЧНУЮ из ЛОКАЛЬНОЙ ПАПКИ ---\")\n",
    "\n",
    "data_base_path = \"/home/ubuntu/model-compression-2025/data\"\n",
    "\n",
    "\n",
    "num_samples_for_wer = 15 \n",
    "target_sampling_rate = 16000 \n",
    "\n",
    "\n",
    "test_clean_path = os.path.join(data_base_path, \"LibriSpeech\", \"test-clean\")\n",
    "print(f\"Поиск данных в директории: {test_clean_path}\")\n",
    "\n",
    "wer_audio_samples = []\n",
    "wer_reference_texts = []\n",
    "\n",
    "\n",
    "if not os.path.isdir(test_clean_path):\n",
    "    print(f\"\\n!!! ОШИБКА: Директория '{test_clean_path}' не найдена.\")\n",
    "    print(f\"!!! Убедитесь, что вы правильно указали 'data_base_path' и папка 'LibriSpeech/test-clean' существует внутри нее.\")\n",
    "else:\n",
    "    print(\"Начинаем обход директорий и чтение файлов...\")\n",
    "    samples_collected = 0\n",
    "    try:\n",
    "        # Обходим все поддиректории test-clean рекурсивно\n",
    "        for root, dirs, files in os.walk(test_clean_path):\n",
    "            # Ищем файл транскрипции в текущей директории\n",
    "            transcription_file = None\n",
    "            for file in files:\n",
    "                if file.endswith(\".trans.txt\"):\n",
    "                    transcription_file = os.path.join(root, file)\n",
    "                    break \n",
    "\n",
    "            if transcription_file:\n",
    "\n",
    "                chapter_transcripts = {}\n",
    "                try:\n",
    "                    with open(transcription_file, 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            # Разделяем по первому пробелу: ID и Текст\n",
    "                            match = re.match(r'^(\\S+)\\s+(.*)$', line)\n",
    "                            if match:\n",
    "                                file_id = match.group(1)\n",
    "                                text = match.group(2)\n",
    "                                chapter_transcripts[file_id] = text.upper() \n",
    "                           \n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при чтении файла транскрипции {transcription_file}: {e}\")\n",
    "                    continue \n",
    "\n",
    "               \n",
    "                for file in files:\n",
    "                    if file.endswith(\".flac\"):\n",
    "                        file_id = os.path.splitext(file)[0] # Получаем ID файла без расширения\n",
    "\n",
    "                        if file_id in chapter_transcripts:\n",
    "                            audio_file_path = os.path.join(root, file)\n",
    "                            try:\n",
    "                                # Читаем аудиофайл\n",
    "                                audio_data, sr = sf.read(audio_file_path, dtype='float32')\n",
    "\n",
    "                                # Проверяем частоту дискретизации\n",
    "                                if sr != target_sampling_rate:\n",
    "                                    print(f\"Предупреждение: Частота дискретизации файла {audio_file_path} ({sr}Hz) отличается от целевой ({target_sampling_rate}Hz). Пропускаем.\")\n",
    "                                   \n",
    "                                    continue # Пропускаем этот файл, так как он не 16кГц\n",
    "\n",
    "                               \n",
    "                                wer_audio_samples.append(audio_data)\n",
    "                                wer_reference_texts.append(chapter_transcripts[file_id])\n",
    "                                samples_collected += 1\n",
    "\n",
    "\n",
    "                                if samples_collected >= num_samples_for_wer:\n",
    "                                    print(f\"\\nСобрано {samples_collected} сэмплов.\")\n",
    "                                    raise StopIteration # Используем исключение для выхода из всех циклов\n",
    "\n",
    "                            except StopIteration: \n",
    "                                raise\n",
    "                            except Exception as e:\n",
    "                                print(f\"Ошибка при чтении или обработке аудиофайла {audio_file_path}: {e}\")\n",
    "\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    except Exception as e: \n",
    "        print(f\"Произошла ошибка во время обхода директорий: {e}\")\n",
    "\n",
    "    # --- Финальная проверка ---\n",
    "    if samples_collected == num_samples_for_wer:\n",
    "        print(f\"\\nУспешно загружено и подготовлено {len(wer_audio_samples)} аудио и {len(wer_reference_texts)} эталонных текстов.\")\n",
    "        if wer_reference_texts:\n",
    "            print(f\"Пример эталонного текста (0): {wer_reference_texts[0]}\")\n",
    "            print(f\"Пример формы аудио (0): {wer_audio_samples[0].shape}, Тип: {wer_audio_samples[0].dtype}\")\n",
    "    elif samples_collected > 0:\n",
    "         print(f\"\\nПредупреждение: Удалось собрать только {samples_collected} из {num_samples_for_wer} запрошенных сэмплов.\")\n",
    "    else:\n",
    "        print(\"\\n!!! Не удалось собрать ни одного сэмпла. Проверьте структуру папок и наличие файлов в {test_clean_path}.\")\n",
    "\n",
    "\n",
    "# Если списки остались пустыми (из-за ошибок выше), присваиваем пустые списки\n",
    "if not wer_audio_samples:\n",
    "    wer_audio_samples = []\n",
    "if not wer_reference_texts:\n",
    "    wer_reference_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Загрузка оригинальной модели openai/whisper-large-v3 (PyTorch) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальная модель и процессор загружены за 5.33 сек.\n",
      "Модель PyTorch размещена на: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Загрузка оригинальной модели {model_id} (PyTorch) ---\")\n",
    "start_load_torch = time.time()\n",
    "\n",
    "original_processor = WhisperProcessor.from_pretrained(model_id)\n",
    "original_model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "original_model.to(device)\n",
    "original_model.eval()\n",
    "\n",
    "end_load_torch = time.time()\n",
    "print(f\"Оригинальная модель и процессор загружены за {end_load_torch - start_load_torch:.2f} сек.\")\n",
    "print(f\"Модель PyTorch размещена на: {next(original_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Экспорт модели в ONNX (в папку ./whisper-large-v3-onnx) ---\n",
      "ONNX модель не найдена или неполная, запускаю экспорт...\n",
      "ВНИМАНИЕ: Экспорт может занять ОЧЕНЬ много времени (десятки минут и более) и потребовать много RAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1016: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n",
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:334: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1476: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/cache_utils.py:460: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/cache_utils.py:444: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  len(self.key_cache[layer_idx]) == 0\n",
      "Moving the following attributes in the config to the generation config: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Экспорт завершен за 125.85 секунд.\n",
      "Файлы ONNX модели должны находиться в: /home/ubuntu/model-compression-2025/whisper-large-v3-onnx\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Экспорт модели в ONNX (в папку {onnx_output_dir}) ---\")\n",
    "\n",
    "\n",
    "encoder_path = os.path.join(onnx_output_dir, \"encoder_model.onnx\")\n",
    "decoder_path = os.path.join(onnx_output_dir, \"decoder_model.onnx\")\n",
    "\n",
    "if not (os.path.exists(encoder_path) and os.path.exists(decoder_path)):\n",
    "    print(\"ONNX модель не найдена или неполная, запускаю экспорт...\")\n",
    "    start_export_time = time.time()\n",
    "\n",
    "\n",
    "    model_for_export = ORTModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)\n",
    "\n",
    "    model_for_export.save_pretrained(onnx_output_dir)\n",
    "\n",
    "\n",
    "    original_processor.save_pretrained(onnx_output_dir)\n",
    "\n",
    "    end_export_time = time.time()\n",
    "    print(f\"Экспорт завершен за {end_export_time - start_export_time:.2f} секунд.\")\n",
    "else:\n",
    "    print(\"Найдены существующие ONNX файлы в папке, пропускаю экспорт.\")\n",
    "    if not os.path.exists(os.path.join(onnx_output_dir, \"preprocessor_config.json\")):\n",
    "         original_processor.save_pretrained(onnx_output_dir)\n",
    "         print(\"Скопирован файл конфигурации процессора в папку ONNX.\")\n",
    "\n",
    "print(f\"Файлы ONNX модели должны находиться в: {os.path.abspath(onnx_output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Загрузка ONNX модели из ./whisper-large-v3-onnx ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-12 22:45:55.495893053 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-12 22:45:55.495954926 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-05-12 22:45:56.753307239 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-12 22:45:56.753352621 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX модель и процессор загружены за 4.89 сек.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-12 22:45:58.351383530 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-12 22:45:58.351429273 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Загрузка ONNX модели из {onnx_output_dir} ---\")\n",
    "start_load_onnx = time.time()\n",
    "\n",
    "onnx_processor = WhisperProcessor.from_pretrained(onnx_output_dir)\n",
    "\n",
    "onnx_model = ORTModelForSpeechSeq2Seq.from_pretrained(onnx_output_dir, provider=onnx_provider)\n",
    "\n",
    "end_load_onnx = time.time()\n",
    "print(f\"ONNX модель и процессор загружены за {end_load_onnx - start_load_onnx:.2f} сек.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Запуск инференса (PyTorch на cuda) для 15 сэмплов ---\n",
      "Прогрев PyTorch модели...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прогрев завершен.\n",
      "Запуск инференса и замера времени для 15 сэмплов...\n",
      "Инференс PyTorch завершен за 14.99 сек (общее время цикла).\n",
      "\n",
      "Среднее время инференса на сэмпл (PyTorch): 0.9807 сек\n",
      "Приблизительное изменение RAM во время инференса PyTorch: 265.25 MB\n",
      "Пиковое потребление VRAM (отслеженное PyTorch): 6536.38 MB\n",
      "\n",
      "Пример предсказания (PyTorch, 0):  WHAT THE LATTER-DAY SAINTS CALL CELESTIAL MARRIAGE IS CHARACTERISTIC OF THE CHURCH AND IS IN VERY GENERAL PRACTICE BUT OF CELESTIAL MARRIAGE PLURALITY OF WIVES WAS AN INCIDENT NEVER AN ESSENTIAL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import psutil # Для замера RAM\n",
    "import torch.cuda # Для замера VRAM\n",
    "\n",
    "print(f\"\\n--- Запуск инференса (PyTorch на {device}) для {len(wer_audio_samples)} сэмплов ---\")\n",
    "\n",
    "torch_total_time = 0.0\n",
    "torch_predictions = []\n",
    "process = psutil.Process(os.getpid()) # Получаем текущий процесс Python\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.empty_cache() # Попытка очистить кэш перед замером\n",
    "\n",
    "ram_before_torch = process.memory_info().rss # RAM до начала цикла\n",
    "\n",
    "if wer_audio_samples:\n",
    "    print(\"Прогрев PyTorch модели...\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "\n",
    "             inputs_warmup = original_processor(wer_audio_samples[0], sampling_rate=16000, return_tensors=\"pt\")\n",
    "             input_features_warmup = inputs_warmup.input_features.to(device)\n",
    "             _ = original_model.generate(input_features_warmup) \n",
    "             if device == \"cuda\":\n",
    "                 torch.cuda.synchronize()\n",
    "        print(\"Прогрев завершен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка во время прогрева PyTorch: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Запуск инференса и замера времени для {len(wer_audio_samples)} сэмплов...\")\n",
    "start_full_torch_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for i, audio_sample in enumerate(wer_audio_samples):\n",
    "\n",
    "        inputs = original_processor(audio_sample, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "        input_features_torch = inputs.input_features.to(device)\n",
    "\n",
    "        iter_start_time = time.time()\n",
    "        \n",
    "        predicted_ids = original_model.generate(input_features_torch)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        iter_end_time = time.time()\n",
    "\n",
    "        run_time = iter_end_time - iter_start_time\n",
    "        torch_total_time += run_time\n",
    "\n",
    "        predicted_ids_cpu = predicted_ids.to(\"cpu\")\n",
    "        transcription = original_processor.batch_decode(predicted_ids_cpu, skip_special_tokens=True)[0]\n",
    "        torch_predictions.append(transcription.upper()) # Сохраняем в верхнем регистре\n",
    "\n",
    "\n",
    "ram_after_torch = process.memory_info().rss\n",
    "vram_peak_torch = torch.cuda.max_memory_allocated(device) if device == \"cuda\" else 0\n",
    "end_full_torch_time = time.time()\n",
    "print(f\"Инференс PyTorch завершен за {end_full_torch_time - start_full_torch_time:.2f} сек (общее время цикла).\")\n",
    "\n",
    "\n",
    "\n",
    "avg_torch_time_per_sample = torch_total_time / len(wer_audio_samples) if wer_audio_samples else 0\n",
    "print(f\"\\nСреднее время инференса на сэмпл (PyTorch): {avg_torch_time_per_sample:.4f} сек\")\n",
    "\n",
    "\n",
    "torch_ram_usage = ram_after_torch - ram_before_torch\n",
    "print(f\"Приблизительное изменение RAM во время инференса PyTorch: {torch_ram_usage / (1024**2):.2f} MB\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"Пиковое потребление VRAM (отслеженное PyTorch): {vram_peak_torch / (1024**2):.2f} MB\")\n",
    "\n",
    "if torch_predictions:\n",
    "    print(f\"\\nПример предсказания (PyTorch, 0): {torch_predictions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Запуск инференса (ONNX на CUDAExecutionProvider) для 15 сэмплов ---\n",
      "Прогрев ONNX модели...\n",
      "Прогрев завершен.\n",
      "Запуск инференса и замера времени для 15 сэмплов...\n",
      "Инференс ONNX завершен за 11.81 сек (общее время цикла).\n",
      "\n",
      "Среднее время инференса на сэмпл (ONNX): 0.7684 сек\n",
      "Приблизительное изменение RAM во время инференса ONNX: 113.38 MB\n",
      "Пиковое потребление VRAM (отслеженное PyTorch*): 6562.14 MB\n",
      "*Примечание: Замер VRAM для ONNX через torch.cuda может быть неполным, т.к. ONNX RT может управлять памятью GPU самостоятельно.\n",
      "\n",
      "Пример предсказания (ONNX, 0):  WHAT THE LATTER-DAY SAINTS CALL CELESTIAL MARRIAGE IS CHARACTERISTIC OF THE CHURCH AND IS IN VERY GENERAL PRACTICE BUT OF CELESTIAL MARRIAGE PLURALITY OF WIVES WAS AN INCIDENT NEVER AN ESSENTIAL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import psutil # Для замера RAM\n",
    "import torch.cuda # Для замера VRAM\n",
    "\n",
    "print(f\"\\n--- Запуск инференса (ONNX на {onnx_provider}) для {len(wer_audio_samples)} сэмплов ---\")\n",
    "\n",
    "onnx_total_time = 0.0\n",
    "onnx_predictions = []\n",
    "process = psutil.Process(os.getpid()) # Обновляем ссылку на процесс\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.empty_cache() # Попытка очистить кэш\n",
    "\n",
    "ram_before_onnx = process.memory_info().rss # RAM до начала цикла\n",
    "\n",
    "# Прогрев (опционально, на первом сэмпле)\n",
    "if wer_audio_samples:\n",
    "    print(\"Прогрев ONNX модели...\")\n",
    "    try:\n",
    "\n",
    "        inputs_warmup = onnx_processor(wer_audio_samples[0], sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "        input_features_warmup_onnx = inputs_warmup.input_features.to(device)\n",
    "        _ = onnx_model.generate(input_features_warmup_onnx)\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "             torch.cuda.synchronize()\n",
    "        print(\"Прогрев завершен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка во время прогрева ONNX: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Запуск инференса и замера времени для {len(wer_audio_samples)} сэмплов...\")\n",
    "start_full_onnx_time = time.time()\n",
    "for i, audio_sample in enumerate(wer_audio_samples):\n",
    "\n",
    "    inputs = onnx_processor(audio_sample, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    input_features_for_onnx_generate = inputs.input_features.to(device)\n",
    "\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    predicted_ids = onnx_model.generate(input_features_for_onnx_generate)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    iter_end_time = time.time()\n",
    "\n",
    "    run_time = iter_end_time - iter_start_time\n",
    "    onnx_total_time += run_time\n",
    "\n",
    "    predicted_ids_cpu = predicted_ids.to(\"cpu\") \n",
    "    transcription = onnx_processor.batch_decode(predicted_ids_cpu, skip_special_tokens=True)[0]\n",
    "    onnx_predictions.append(transcription.upper())\n",
    "\n",
    "\n",
    "ram_after_onnx = process.memory_info().rss\n",
    "vram_peak_onnx = torch.cuda.max_memory_allocated(device) if device == \"cuda\" else 0\n",
    "end_full_onnx_time = time.time()\n",
    "print(f\"Инференс ONNX завершен за {end_full_onnx_time - start_full_onnx_time:.2f} сек (общее время цикла).\")\n",
    "\n",
    "avg_onnx_time_per_sample = onnx_total_time / len(wer_audio_samples) if wer_audio_samples else 0\n",
    "print(f\"\\nСреднее время инференса на сэмпл (ONNX): {avg_onnx_time_per_sample:.4f} сек\")\n",
    "\n",
    "onnx_ram_usage = ram_after_onnx - ram_before_onnx\n",
    "print(f\"Приблизительное изменение RAM во время инференса ONNX: {onnx_ram_usage / (1024**2):.2f} MB\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"Пиковое потребление VRAM (отслеженное PyTorch*): {vram_peak_onnx / (1024**2):.2f} MB\")\n",
    "    print(f\"*Примечание: Замер VRAM для ONNX через torch.cuda может быть неполным, т.к. ONNX RT может управлять памятью GPU самостоятельно.\")\n",
    "\n",
    "if onnx_predictions:\n",
    "    print(f\"\\nПример предсказания (ONNX, 0): {onnx_predictions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Сравнительный анализ (Автоматизированный) ---\n",
      "\n",
      "1. Скорость инференса (средняя на сэмпл):\n",
      "   Среднее время (PyTorch): 0.9807 сек\n",
      "   Среднее время (ONNX):    0.7684 сек\n",
      "\n",
      "   Ускорение ONNX по сравнению с PyTorch: 1.28x\n",
      "   ONNX быстрее PyTorch на: 21.65%\n",
      "\n",
      "2. Размер модели:\n",
      "   Размер папки с ONNX моделью: 9460.61 МБ\n",
      "   (Размер оригинальной PyTorch модели large-v3 обычно ~3-6 ГБ, в зависимости от точности)\n",
      "   Примечание: Значительное уменьшение размера достигается квантованием.\n",
      "\n",
      "3. Потребление памяти (Приблизительные замеры):\n",
      "   Изменение RAM (PyTorch): ~ 265.25 MB\n",
      "   Изменение RAM (ONNX):    ~ 113.38 MB\n",
      "   Примечание RAM: Замер показывает разницу RSS процесса до и после цикла инференса, может включать сторонние факторы.\n",
      "\n",
      "   Пик VRAM (PyTorch, отслечено): 6536.38 MB\n",
      "   Пик VRAM (ONNX, отслечено*): 6562.14 MB\n",
      "   *Примечание VRAM: Замер для ONNX через torch.cuda может быть неполным.\n",
      "\n",
      "4. Качество (Word Error Rate):\n",
      "   WER (PyTorch): 6.73%\n",
      "   WER (ONNX):    6.73%\n",
      "   (Чем ниже WER, тем лучше)\n",
      "   Разница в WER: 0.00%\n",
      "   Качество распознавания практически идентично.\n",
      "\n",
      "   Пример сравнения (сэмпл 0):\n",
      "   Эталон:  \"WHAT THE LATTER DAY SAINTS CALL CELESTIAL MARRIAGE IS CHARACTERISTIC OF THE CHURCH AND IS IN VERY GENERAL PRACTISE BUT OF CELESTIAL MARRIAGE PLURALITY OF WIVES WAS AN INCIDENT NEVER AN ESSENTIAL\"\n",
      "   PyTorch: \" WHAT THE LATTER-DAY SAINTS CALL CELESTIAL MARRIAGE IS CHARACTERISTIC OF THE CHURCH AND IS IN VERY GENERAL PRACTICE BUT OF CELESTIAL MARRIAGE PLURALITY OF WIVES WAS AN INCIDENT NEVER AN ESSENTIAL\"\n",
      "   ONNX:    \" WHAT THE LATTER-DAY SAINTS CALL CELESTIAL MARRIAGE IS CHARACTERISTIC OF THE CHURCH AND IS IN VERY GENERAL PRACTICE BUT OF CELESTIAL MARRIAGE PLURALITY OF WIVES WAS AN INCIDENT NEVER AN ESSENTIAL\"\n",
      "\n",
      "--- Анализ завершен ---\n"
     ]
    }
   ],
   "source": [
    "import jiwer \n",
    "import os\n",
    "\n",
    "print(\"\\n\\n--- Сравнительный анализ (Автоматизированный) ---\")\n",
    "\n",
    "# --- 1. Скорость инференса (средняя на сэмпл) ---\n",
    "print(\"\\n1. Скорость инференса (средняя на сэмпл):\")\n",
    "# Используем переменные из обновленных Ячеек 8 и 9\n",
    "print(f\"   Среднее время (PyTorch): {avg_torch_time_per_sample:.4f} сек\")\n",
    "print(f\"   Среднее время (ONNX):    {avg_onnx_time_per_sample:.4f} сек\")\n",
    "\n",
    "if avg_torch_time_per_sample > 0 and avg_onnx_time_per_sample > 0:\n",
    "    speedup = avg_torch_time_per_sample / avg_onnx_time_per_sample\n",
    "    improvement = ((avg_torch_time_per_sample - avg_onnx_time_per_sample) / avg_torch_time_per_sample) * 100\n",
    "    print(f\"\\n   Ускорение ONNX по сравнению с PyTorch: {speedup:.2f}x\")\n",
    "    print(f\"   ONNX быстрее PyTorch на: {improvement:.2f}%\")\n",
    "elif avg_onnx_time_per_sample == 0:\n",
    "     print(\"\\n   Не удалось рассчитать ускорение (время ONNX равно нулю).\")\n",
    "else:\n",
    "    print(\"\\n   Не удалось рассчитать ускорение.\")\n",
    "\n",
    "\n",
    "# --- 2. Размер модели ---\n",
    "print(\"\\n2. Размер модели:\")\n",
    "\n",
    "def get_folder_size_mb(folder_path):\n",
    "    total_size = 0\n",
    "    try:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            for f in filenames:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                if not os.path.islink(fp):\n",
    "                    total_size += os.path.getsize(fp)\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "    return total_size / (1024 * 1024) # в МБ\n",
    "\n",
    "# Переменная onnx_output_dir должна быть определена в Ячейке 2\n",
    "onnx_model_size_mb = get_folder_size_mb(onnx_output_dir)\n",
    "print(f\"   Размер папки с ONNX моделью: {onnx_model_size_mb:.2f} МБ\")\n",
    "print(\"   (Размер оригинальной PyTorch модели large-v3 обычно ~3-6 ГБ, в зависимости от точности)\")\n",
    "print(\"   Примечание: Значительное уменьшение размера достигается квантованием.\")\n",
    "\n",
    "\n",
    "# --- 3. Потребление памяти (RAM/VRAM) ---\n",
    "print(\"\\n3. Потребление памяти (Приблизительные замеры):\")\n",
    "# Используем переменные из обновленных Ячеек 8 и 9\n",
    "print(f\"   Изменение RAM (PyTorch): ~ {torch_ram_usage / (1024**2):.2f} MB\")\n",
    "print(f\"   Изменение RAM (ONNX):    ~ {onnx_ram_usage / (1024**2):.2f} MB\")\n",
    "print(\"   Примечание RAM: Замер показывает разницу RSS процесса до и после цикла инференса, может включать сторонние факторы.\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    # Используем переменные из обновленных Ячеек 8 и 9\n",
    "    print(f\"\\n   Пик VRAM (PyTorch, отслечено): {vram_peak_torch / (1024**2):.2f} MB\")\n",
    "    print(f\"   Пик VRAM (ONNX, отслечено*): {vram_peak_onnx / (1024**2):.2f} MB\")\n",
    "    print(f\"   *Примечание VRAM: Замер для ONNX через torch.cuda может быть неполным.\")\n",
    "else:\n",
    "    print(\"\\n   VRAM: Не измерялось (выполнение на CPU).\")\n",
    "\n",
    "\n",
    "# --- 4. Качество (Word Error Rate - WER) ---\n",
    "print(\"\\n4. Качество (Word Error Rate):\")\n",
    "\n",
    "wer_calculated = False\n",
    "# Проверяем наличие всех необходимых списков из Ячеек 3, 8, 9\n",
    "if 'wer_reference_texts' in globals() and 'torch_predictions' in globals() and 'onnx_predictions' in globals() and \\\n",
    "   wer_reference_texts and torch_predictions and onnx_predictions and \\\n",
    "   (len(wer_reference_texts) == len(torch_predictions) == len(onnx_predictions)):\n",
    "    try:\n",
    "        # Расчет WER для PyTorch\n",
    "        wer_torch_info = jiwer.compute_measures(wer_reference_texts, torch_predictions)\n",
    "        wer_torch = wer_torch_info['wer'] * 100 # В процентах\n",
    "\n",
    "        # Расчет WER для ONNX\n",
    "        wer_onnx_info = jiwer.compute_measures(wer_reference_texts, onnx_predictions)\n",
    "        wer_onnx = wer_onnx_info['wer'] * 100 # В процентах\n",
    "\n",
    "        print(f\"   WER (PyTorch): {wer_torch:.2f}%\")\n",
    "        print(f\"   WER (ONNX):    {wer_onnx:.2f}%\")\n",
    "        print(f\"   (Чем ниже WER, тем лучше)\")\n",
    "\n",
    "        # Сравнение\n",
    "        wer_diff = abs(wer_torch - wer_onnx)\n",
    "        print(f\"   Разница в WER: {wer_diff:.2f}%\")\n",
    "        if wer_diff < 0.1:\n",
    "            print(\"   Качество распознавания практически идентично.\")\n",
    "        else:\n",
    "            print(\"   Есть заметная разница в качестве распознавания.\")\n",
    "\n",
    "        # Вывод примера для визуальной проверки (первый сэмпл)\n",
    "        print(\"\\n   Пример сравнения (сэмпл 0):\")\n",
    "        print(f\"   Эталон:  \\\"{wer_reference_texts[0]}\\\"\")\n",
    "        print(f\"   PyTorch: \\\"{torch_predictions[0]}\\\"\")\n",
    "        print(f\"   ONNX:    \\\"{onnx_predictions[0]}\\\"\")\n",
    "        wer_calculated = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n   Ошибка при расчете WER: {e}\")\n",
    "        print(f\"   Убедитесь, что библиотека jiwer установлена: pip install jiwer\")\n",
    "\n",
    "# Диагностика, если WER не рассчитан\n",
    "if not wer_calculated:\n",
    "    if 'wer_reference_texts' not in globals() or not wer_reference_texts:\n",
    "         print(\"\\n   Не удалось рассчитать WER: Нет эталонных текстов (ошибка в Ячейке 3?).\")\n",
    "    elif 'torch_predictions' not in globals() or not torch_predictions:\n",
    "         print(\"\\n   Не удалось рассчитать WER: Отсутствуют предсказания PyTorch (ошибка в Ячейке 8?).\")\n",
    "    elif 'onnx_predictions' not in globals() or not onnx_predictions:\n",
    "         print(\"\\n   Не удалось рассчитать WER: Отсутствуют предсказания ONNX (ошибка в Ячейке 9?).\")\n",
    "    elif not (len(wer_reference_texts) == len(torch_predictions) == len(onnx_predictions)):\n",
    "        print(\"\\n   Не удалось рассчитать WER: Не совпадает количество эталонов и предсказаний.\")\n",
    "        print(f\"     Эталонов: {len(wer_reference_texts) if 'wer_reference_texts' in globals() else 'N/A'}\")\n",
    "        print(f\"     PyTorch предсказаний: {len(torch_predictions) if 'torch_predictions' in globals() else 'N/A'}\")\n",
    "        print(f\"     ONNX предсказаний: {len(onnx_predictions) if 'onnx_predictions' in globals() else 'N/A'}\")\n",
    "    else:\n",
    "         print(\"\\n   Не удалось рассчитать WER по неизвестной причине.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Анализ завершен ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сохранение оригинальной PyTorch модели в папку: ./whisper-large-v3-original-pytorch\n",
      "Это может занять некоторое время...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена за 12.51 секунд.\n",
      "\n",
      "Точный размер сохраненной оригинальной PyTorch модели: 5888.20 МБ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Функция для измерения размера папки (если ее нет выше) ---\n",
    "def get_folder_size_mb(folder_path):\n",
    "    total_size = 0\n",
    "    try:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            for f in filenames:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                # Проверяем, что это не символическая ссылка, чтобы избежать двойного подсчета\n",
    "                if not os.path.islink(fp):\n",
    "                    total_size += os.path.getsize(fp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Предупреждение: Папка {folder_path} не найдена.\")\n",
    "        return 0\n",
    "    return total_size / (1024 * 1024) # в МБ\n",
    "\n",
    "\n",
    "if 'original_model' in globals():\n",
    "    temp_save_dir = \"./whisper-large-v3-original-pytorch\" # Имя временной папки\n",
    "    print(f\"\\nСохранение оригинальной PyTorch модели в папку: {temp_save_dir}\")\n",
    "    print(\"Это может занять некоторое время...\")\n",
    "    start_save_time = time.time()\n",
    "\n",
    "\n",
    "    if os.path.exists(temp_save_dir):\n",
    "        import shutil\n",
    "        print(f\"Удаление существующей папки {temp_save_dir}...\")\n",
    "        shutil.rmtree(temp_save_dir)\n",
    "\n",
    "\n",
    "    original_model.save_pretrained(temp_save_dir)\n",
    "\n",
    "\n",
    "    end_save_time = time.time()\n",
    "    print(f\"Модель сохранена за {end_save_time - start_save_time:.2f} секунд.\")\n",
    "\n",
    "    pytorch_model_size_mb = get_folder_size_mb(temp_save_dir)\n",
    "    print(f\"\\nТочный размер сохраненной оригинальной PyTorch модели: {pytorch_model_size_mb:.2f} МБ\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nОшибка: Переменная 'original_model' не найдена. Убедитесь, что Ячейка 4 была выполнена.\")\n",
    "    pytorch_model_size_mb = 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
