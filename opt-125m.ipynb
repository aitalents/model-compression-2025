{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 1/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 1/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 1/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 1/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 1/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 1/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 1/12...\n",
      "Quantizing model.decoder.layers blocks :   8%|▊         | 1/12 [00:03<00:35,  3.20s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 2/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 2/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 2/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 2/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 2/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 2/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 2/12...\n",
      "Quantizing model.decoder.layers blocks :  17%|█▋        | 2/12 [00:05<00:26,  2.68s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 3/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 3/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 3/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 3/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 3/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 3/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 3/12...\n",
      "Quantizing model.decoder.layers blocks :  25%|██▌       | 3/12 [00:07<00:22,  2.52s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 4/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 4/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 4/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 4/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 4/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 4/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 4/12...\n",
      "Quantizing model.decoder.layers blocks :  33%|███▎      | 4/12 [00:10<00:19,  2.43s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 5/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 5/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 5/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 5/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 5/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 5/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 5/12...\n",
      "Quantizing model.decoder.layers blocks :  42%|████▏     | 5/12 [00:12<00:16,  2.40s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 6/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 6/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 6/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 6/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 6/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 6/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 6/12...\n",
      "Quantizing model.decoder.layers blocks :  50%|█████     | 6/12 [00:14<00:14,  2.38s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 7/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 7/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 7/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 7/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 7/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 7/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 7/12...\n",
      "Quantizing model.decoder.layers blocks :  58%|█████▊    | 7/12 [00:17<00:11,  2.35s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 8/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 8/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 8/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 8/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 8/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 8/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 8/12...\n",
      "Quantizing model.decoder.layers blocks :  67%|██████▋   | 8/12 [00:19<00:09,  2.34s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 9/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 9/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 9/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 9/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 9/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 9/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 9/12...\n",
      "Quantizing model.decoder.layers blocks :  75%|███████▌  | 9/12 [00:21<00:07,  2.34s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 10/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 10/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 10/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 10/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 10/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 10/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 10/12...\n",
      "Quantizing model.decoder.layers blocks :  83%|████████▎ | 10/12 [00:24<00:04,  2.35s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 11/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 11/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 11/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 11/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 11/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 11/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 11/12...\n",
      "Quantizing model.decoder.layers blocks :  92%|█████████▏| 11/12 [00:26<00:02,  2.34s/it]INFO:optimum.gptq.quantizer:Start quantizing block model.decoder.layers 12/12\n",
      "INFO:optimum.gptq.quantizer:Module to quantize [['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.q_proj'], ['self_attn.out_proj'], ['fc1'], ['fc2']]\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.k_proj in block 12/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.v_proj in block 12/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.q_proj in block 12/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing self_attn.out_proj in block 12/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc1 in block 12/12...\n",
      "INFO:optimum.gptq.quantizer:Quantizing fc2 in block 12/12...\n",
      "Quantizing model.decoder.layers blocks : 100%|██████████| 12/12 [00:28<00:00,  2.40s/it]\n",
      "INFO:optimum.gptq.quantizer:Packing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.0.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.1.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.2.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.3.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.4.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.5.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.6.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.7.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.8.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.9.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.10.fc2\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.k_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.out_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.q_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.self_attn.v_proj\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.fc1\n",
      "INFO:optimum.gptq.quantizer:model.decoder.layers.11.fc2\n",
      "INFO:optimum.gptq.quantizer:Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n",
      "✓ Модель отквантована\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('opt-125m-gptq/tokenizer_config.json',\n",
       " 'opt-125m-gptq/special_tokens_map.json',\n",
       " 'opt-125m-gptq/vocab.json',\n",
       " 'opt-125m-gptq/merges.txt',\n",
       " 'opt-125m-gptq/added_tokens.json',\n",
       " 'opt-125m-gptq/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPTIMUM_GPTQ_DISABLE_TRITON_KERNEL\"] = \"1\"  \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "gptq_config = GPTQConfig(\n",
    "    bits=4, group_size=128, dataset=\"c4\", tokenizer=tokenizer,\n",
    "    use_triton=False         \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-125m\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=gptq_config\n",
    ")\n",
    "\n",
    "print(\"✓ Модель отквантована\")\n",
    "model.save_pretrained(\"opt-125m-gptq\")\n",
    "tokenizer.save_pretrained(\"opt-125m-gptq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import math \n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 'opt-125m-gptq' loaded.\n"
     ]
    }
   ],
   "source": [
    "quantized_model_dir = \"opt-125m-gptq\" \n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(\"Tokenizer pad_token set to eos_token.\")\n",
    "\n",
    "print(f\"Tokenizer '{quantized_model_dir}' loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size on disk: 123.91 MB\n"
     ]
    }
   ],
   "source": [
    "def get_folder_size_mb(folder_path):\n",
    "    total_size_bytes = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if not os.path.islink(fp):\n",
    "                total_size_bytes += os.path.getsize(fp)\n",
    "    return total_size_bytes / (1024 * 1024) \n",
    "\n",
    "model_disk_size_mb = get_folder_size_mb(quantized_model_dir)\n",
    "print(f\"Quantized model size on disk: {model_disk_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Perplexity on GPU ---\n",
      "Loading model for GPU perplexity onto device: cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quantized_model_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model for GPU perplexity onto device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_gpu_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# from transformers import AutoModelForCausalLM \u001b[39;00m\n\u001b[32m     72\u001b[39m model_gpu_ppl = AutoModelForCausalLM.from_pretrained(\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mquantized_model_dir\u001b[49m,\n\u001b[32m     74\u001b[39m     device_map=target_gpu_device, \n\u001b[32m     75\u001b[39m     torch_dtype=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m )\n\u001b[32m     78\u001b[39m start_time_gpu = time.time()\n\u001b[32m     79\u001b[39m perplexity_gpu = calculate_perplexity(model_gpu_ppl, tokenizer, device=target_gpu_device)\n",
      "\u001b[31mNameError\u001b[39m: name 'quantized_model_dir' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, tokenizer, dataset_name=\"wikitext\", dataset_config=\"wikitext-2-raw-v1\", split=\"test\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculates perplexity of a model on a given dataset using a sliding window approach.\n",
    "    Based on Hugging Face's perplexity calculation tutorial.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(model.config, 'n_positions'): # OPT\n",
    "        max_length = model.config.n_positions\n",
    "    elif hasattr(model.config, 'max_position_embeddings'):\n",
    "        max_length = model.config.max_position_embeddings\n",
    "    else:\n",
    "        max_length = 1024\n",
    "    \n",
    "    stride = 512 \n",
    "\n",
    "    try:\n",
    "        print(f\"Loading dataset: {dataset_name} ({dataset_config}), split: {split}\")\n",
    "        dataset = load_dataset(dataset_name, dataset_config, split=split)\n",
    "        encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "        print(f\"Dataset loaded and tokenized. Total tokens: {encodings.input_ids.size(1)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset {dataset_name}/{dataset_config}: {e}\")\n",
    "        print(\"Using a short sample text for perplexity calculation as a fallback.\")\n",
    "        sample_text = \"This is a sample text to calculate perplexity. Language models are fascinating because they can generate human-like text based on the patterns they learned from vast amounts of data during their training process.\"\n",
    "        encodings = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    nlls = [] \n",
    "    \n",
    "    print(f\"Calculating perplexity on {device} with max_length={max_length}, stride={stride}\")\n",
    "    for i in tqdm(range(0, encodings.input_ids.size(1), stride), desc=\"Perplexity Batches\"):\n",
    "        begin_loc = i\n",
    "        end_loc = min(i + max_length, encodings.input_ids.size(1))\n",
    "        \n",
    "        if end_loc - begin_loc < 2: \n",
    "            continue\n",
    "\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone() \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            if outputs.loss is not None and not torch.isnan(outputs.loss):\n",
    "                nlls.append(outputs.loss)\n",
    "            else:\n",
    "                print(f\"Warning: NaN or None loss detected for batch starting at token {i}. Skipping this batch.\")\n",
    "    \n",
    "    if not nlls:\n",
    "        print(\"No valid NLLs collected. Cannot compute perplexity.\")\n",
    "        return float('nan')\n",
    "\n",
    "    perplexity = torch.exp(torch.stack(nlls).mean())\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "perplexity_gpu = float('nan')\n",
    "perplexity_cpu = float('nan') \n",
    "time_perplexity_gpu = float('nan')\n",
    "time_perplexity_cpu = float('nan') \n",
    "vram_perplexity_gpu_mb = float('nan')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n--- Calculating Perplexity on GPU ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats() \n",
    "    \n",
    "    target_gpu_device = \"cuda:0\" \n",
    "    print(f\"Loading model for GPU perplexity onto device: {target_gpu_device}\")\n",
    "    model_gpu_ppl = AutoModelForCausalLM.from_pretrained(\n",
    "        quantized_model_dir,\n",
    "        device_map=target_gpu_device, \n",
    "        torch_dtype=\"auto\"\n",
    "    )\n",
    "    \n",
    "    start_time_gpu = time.time()\n",
    "    perplexity_gpu = calculate_perplexity(model_gpu_ppl, tokenizer, device=target_gpu_device)\n",
    "    time_perplexity_gpu = time.time() - start_time_gpu\n",
    "    \n",
    "    vram_perplexity_gpu_mb = torch.cuda.max_memory_allocated(target_gpu_device) / (1024**2) \n",
    "    \n",
    "    print(f\"Perplexity (GPU): {perplexity_gpu:.4f}\")\n",
    "    print(f\"Perplexity calculation time (GPU): {time_perplexity_gpu:.2f} seconds\")\n",
    "    print(f\"Peak VRAM for perplexity (GPU): {vram_perplexity_gpu_mb:.2f} MB\")\n",
    "    \n",
    "    del model_gpu_ppl \n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nGPU not available, skipping GPU perplexity calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Measuring Inference Speed on GPU ---\n",
      "Loading model for GPU inference onto device: cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quantized_model_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model for GPU inference onto device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_gpu_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# from transformers import AutoModelForCausalLM\u001b[39;00m\n\u001b[32m     48\u001b[39m model_gpu_inf = AutoModelForCausalLM.from_pretrained(\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mquantized_model_dir\u001b[49m,\n\u001b[32m     50\u001b[39m     device_map=target_gpu_device,\n\u001b[32m     51\u001b[39m     torch_dtype=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m inf_time_gpu_ms = measure_inference_speed(model_gpu_inf, tokenizer, device=target_gpu_device, num_tokens_to_generate=\u001b[32m50\u001b[39m, num_runs=\u001b[32m20\u001b[39m)\n\u001b[32m     55\u001b[39m vram_inference_gpu_mb = torch.cuda.max_memory_allocated(target_gpu_device) / (\u001b[32m1024\u001b[39m**\u001b[32m2\u001b[39m) \n",
      "\u001b[31mNameError\u001b[39m: name 'quantized_model_dir' is not defined"
     ]
    }
   ],
   "source": [
    "def measure_inference_speed(model, tokenizer, device=\"cpu\", num_tokens_to_generate=50, num_runs=10, prompt=\"The future of AI is\"):\n",
    "    \"\"\"\n",
    "    Measures inference speed for text generation.\n",
    "    Returns average time in milliseconds per generation call.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    total_time_seconds = 0\n",
    "    \n",
    "    print(f\"Performing warm-up run on {device}...\")\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_ids, max_new_tokens=num_tokens_to_generate, pad_token_id=tokenizer.eos_token_id, do_sample=False) \n",
    "\n",
    "    print(f\"Measuring inference speed on {device} for {num_runs} runs, generating {num_tokens_to_generate} new tokens each.\")\n",
    "    for i in tqdm(range(num_runs), desc=f\"Inference Runs ({device})\"):\n",
    "        start_run_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids, \n",
    "                max_new_tokens=num_tokens_to_generate, \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False \n",
    "            )\n",
    "        end_run_time = time.time()\n",
    "        total_time_seconds += (end_run_time - start_run_time)\n",
    "        \n",
    "    avg_time_ms_per_call = (total_time_seconds / num_runs) * 1000\n",
    "    \n",
    "    print(f\"Average time per generation call ({num_tokens_to_generate} new tokens) on {device}: {avg_time_ms_per_call:.2f} ms\")\n",
    "    return avg_time_ms_per_call\n",
    "\n",
    "\n",
    "inf_time_gpu_ms = float('nan')\n",
    "inf_time_cpu_ms = float('nan') \n",
    "vram_inference_gpu_mb = float('nan')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n--- Measuring Inference Speed on GPU ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    target_gpu_device = \"cuda:0\" \n",
    "    print(f\"Loading model for GPU inference onto device: {target_gpu_device}\")\n",
    "\n",
    "    model_gpu_inf = AutoModelForCausalLM.from_pretrained(\n",
    "        quantized_model_dir,\n",
    "        device_map=target_gpu_device,\n",
    "        torch_dtype=\"auto\" \n",
    "    )\n",
    "    \n",
    "    inf_time_gpu_ms = measure_inference_speed(model_gpu_inf, tokenizer, device=target_gpu_device, num_tokens_to_generate=50, num_runs=20)\n",
    "    vram_inference_gpu_mb = torch.cuda.max_memory_allocated(target_gpu_device) / (1024**2) \n",
    "    \n",
    "    print(f\"Final Inference Time (GPU, for 50 new tokens): {inf_time_gpu_ms:.2f} ms\")\n",
    "    print(f\"Peak VRAM during inference (GPU): {vram_inference_gpu_mb:.2f} MB\")\n",
    "\n",
    "    del model_gpu_inf\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nGPU not available, skipping GPU inference speed test.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель без GTPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/model-compression/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import math # or from numpy import exp\n",
    "import os\n",
    "from tqdm import tqdm # For progress bars\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # Убедитесь, что это импортировано\n",
    "import shutil # Для удаления временной папки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tokenizer for baseline evaluation (from 'facebook/opt-125m').\n"
     ]
    }
   ],
   "source": [
    "baseline_model_id = \"facebook/opt-125m\"\n",
    "baseline_model_save_dir = \"opt-125m-fp16-baseline\" \n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(baseline_model_id)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load tokenizer for {baseline_model_id}, trying from 'opt-125m-gptq'. Error: {e}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"opt-125m-gptq\") \n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(\"Tokenizer pad_token set to eos_token.\")\n",
    "\n",
    "print(f\"Using Tokenizer for baseline evaluation (from '{tokenizer.name_or_path}').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline model: facebook/opt-125m with torch.float16 for size measurement...\n",
      "Saving baseline model to opt-125m-fp16-baseline to measure disk size...\n",
      "Baseline model (facebook/opt-125m in FP16) size on disk: 238.90 MB\n",
      "Cleaning up temporary directory: opt-125m-fp16-baseline\n"
     ]
    }
   ],
   "source": [
    "def get_folder_size_mb(folder_path):\n",
    "    total_size_bytes = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if not os.path.islink(fp):\n",
    "                total_size_bytes += os.path.getsize(fp)\n",
    "    return total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Loading baseline model: {baseline_model_id} with torch.float16 for size measurement...\")\n",
    "\n",
    "temp_model_for_saving = AutoModelForCausalLM.from_pretrained(\n",
    "    baseline_model_id,\n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True \n",
    ")\n",
    "\n",
    "print(f\"Saving baseline model to {baseline_model_save_dir} to measure disk size...\")\n",
    "if os.path.exists(baseline_model_save_dir):\n",
    "    shutil.rmtree(baseline_model_save_dir) \n",
    "os.makedirs(baseline_model_save_dir, exist_ok=True)\n",
    "\n",
    "temp_model_for_saving.save_pretrained(baseline_model_save_dir)\n",
    "baseline_model_disk_size_mb = get_folder_size_mb(baseline_model_save_dir)\n",
    "print(f\"Baseline model ({baseline_model_id} in FP16) size on disk: {baseline_model_disk_size_mb:.2f} MB\")\n",
    "\n",
    "# Очистка\n",
    "del temp_model_for_saving\n",
    "if os.path.exists(baseline_model_save_dir):\n",
    "    print(f\"Cleaning up temporary directory: {baseline_model_save_dir}\")\n",
    "    shutil.rmtree(baseline_model_save_dir)\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Perplexity for Baseline Model (facebook/opt-125m FP16) on GPU ---\n",
      "Loading baseline model for GPU perplexity onto device: cuda:0\n",
      "Loading dataset: wikitext (wikitext-2-raw-v1), split: test\n",
      "Dataset loaded and tokenized. Total tokens: 287645\n",
      "Calculating perplexity on cuda:0 with max_length=2048, stride=512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity Batches: 100%|██████████| 562/562 [00:04<00:00, 117.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Perplexity (GPU, FP16): 27.6175\n",
      "Baseline Perplexity calculation time (GPU, FP16): 12.70 seconds\n",
      "Baseline Peak VRAM for perplexity (GPU, FP16): 1579.18 MB\n",
      "\n",
      "--- Skipping Perplexity Calculation on CPU for Baseline ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline_perplexity_gpu = float('nan')\n",
    "baseline_time_perplexity_gpu = float('nan')\n",
    "baseline_vram_perplexity_gpu_mb = float('nan')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n--- Calculating Perplexity for Baseline Model ({baseline_model_id} FP16) on GPU ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats() \n",
    "    \n",
    "    target_gpu_device = \"cuda:0\" \n",
    "    print(f\"Loading baseline model for GPU perplexity onto device: {target_gpu_device}\")\n",
    "    \n",
    "    model_gpu_ppl_baseline = AutoModelForCausalLM.from_pretrained(\n",
    "        baseline_model_id,\n",
    "        device_map=target_gpu_device, \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    start_time_gpu = time.time()\n",
    "    baseline_perplexity_gpu = calculate_perplexity(model_gpu_ppl_baseline, tokenizer, device=target_gpu_device)\n",
    "    baseline_time_perplexity_gpu = time.time() - start_time_gpu\n",
    "    \n",
    "    baseline_vram_perplexity_gpu_mb = torch.cuda.max_memory_allocated(target_gpu_device) / (1024**2) \n",
    "    \n",
    "    print(f\"Baseline Perplexity (GPU, FP16): {baseline_perplexity_gpu:.4f}\")\n",
    "    print(f\"Baseline Perplexity calculation time (GPU, FP16): {baseline_time_perplexity_gpu:.2f} seconds\")\n",
    "    print(f\"Baseline Peak VRAM for perplexity (GPU, FP16): {baseline_vram_perplexity_gpu_mb:.2f} MB\")\n",
    "    \n",
    "    del model_gpu_ppl_baseline \n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nGPU not available, skipping GPU perplexity calculation for baseline.\")\n",
    "\n",
    "print(\"\\n--- Skipping Perplexity Calculation on CPU for Baseline ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Measuring Inference Speed for Baseline Model (facebook/opt-125m FP16) on GPU ---\n",
      "Loading baseline model for GPU inference onto device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing warm-up run on cuda:0...\n",
      "Measuring inference speed on cuda:0 for 20 runs, generating 50 new tokens each.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Runs (cuda:0): 100%|██████████| 20/20 [00:05<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per generation call (50 new tokens) on cuda:0: 291.26 ms\n",
      "Baseline Final Inference Time (GPU, FP16, for 50 new tokens): 291.26 ms\n",
      "Baseline Peak VRAM during inference (GPU, FP16): 574.84 MB\n",
      "\n",
      "--- Skipping Inference Speed Calculation on CPU for Baseline ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline_inf_time_gpu_ms = float('nan')\n",
    "baseline_vram_inference_gpu_mb = float('nan')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n--- Measuring Inference Speed for Baseline Model ({baseline_model_id} FP16) on GPU ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    target_gpu_device = \"cuda:0\" \n",
    "    print(f\"Loading baseline model for GPU inference onto device: {target_gpu_device}\")\n",
    "\n",
    "    model_gpu_inf_baseline = AutoModelForCausalLM.from_pretrained(\n",
    "        baseline_model_id,\n",
    "        device_map=target_gpu_device,\n",
    "        torch_dtype=torch.float16 \n",
    "    )\n",
    "    \n",
    "    baseline_inf_time_gpu_ms = measure_inference_speed(model_gpu_inf_baseline, tokenizer, device=target_gpu_device, num_tokens_to_generate=50, num_runs=20)\n",
    "    baseline_vram_inference_gpu_mb = torch.cuda.max_memory_allocated(target_gpu_device) / (1024**2) \n",
    "    \n",
    "    print(f\"Baseline Final Inference Time (GPU, FP16, for 50 new tokens): {baseline_inf_time_gpu_ms:.2f} ms\")\n",
    "    print(f\"Baseline Peak VRAM during inference (GPU, FP16): {baseline_vram_inference_gpu_mb:.2f} MB\")\n",
    "\n",
    "    del model_gpu_inf_baseline\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nGPU not available, skipping GPU inference speed test for baseline.\")\n",
    "\n",
    "print(\"\\n--- Skipping Inference Speed Calculation on CPU for Baseline ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Baseline Validation Summary for facebook/opt-125m (FP16) ---\n",
      "--------------------------------------------------\n",
      "Размер весов (прибл. MB): 238.90\n",
      "Качество (Perplexity GPU): 27.62 (calc time: 12.70s)\n",
      "Время инференса (GPU, ms, 50 токенов): 291.26\n",
      "VRAM (Perplexity, MB): 1579.18\n",
      "VRAM (Inference, MB): 574.84\n",
      "--------------------------------------------------\n",
      "\n",
      "Baseline validation results dictionary:\n",
      "{\n",
      "  \"model_id\": \"facebook/opt-125m\",\n",
      "  \"method\": \"FP16\",\n",
      "  \"disk_size_mb\": 238.9,\n",
      "  \"perplexity_gpu\": 27.62,\n",
      "  \"inference_time_gpu_ms_50_tokens\": 291.26,\n",
      "  \"vram_peak_perplexity_mb\": 1579.18,\n",
      "  \"vram_peak_inference_mb\": 574.84\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n--- Baseline Validation Summary for {baseline_model_id} (FP16) ---\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "print(f\"Размер весов (прибл. MB): {baseline_model_disk_size_mb:.2f}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not math.isnan(baseline_perplexity_gpu):\n",
    "        print(f\"Качество (Perplexity GPU): {baseline_perplexity_gpu:.2f} (calc time: {baseline_time_perplexity_gpu:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"Качество (Perplexity GPU): Not calculated or error.\")\n",
    "    \n",
    "    if not math.isnan(baseline_inf_time_gpu_ms):\n",
    "        print(f\"Время инференса (GPU, ms, 50 токенов): {baseline_inf_time_gpu_ms:.2f}\")\n",
    "    else:\n",
    "        print(f\"Время инференса (GPU, ms, 50 токенов): Not calculated or error.\")\n",
    "\n",
    "    if not math.isnan(baseline_vram_perplexity_gpu_mb):\n",
    "         print(f\"VRAM (Perplexity, MB): {baseline_vram_perplexity_gpu_mb:.2f}\")\n",
    "    else:\n",
    "        print(f\"VRAM (Perplexity, MB): Not calculated or error.\")\n",
    "    \n",
    "    if not math.isnan(baseline_vram_inference_gpu_mb):\n",
    "         print(f\"VRAM (Inference, MB): {baseline_vram_inference_gpu_mb:.2f}\")\n",
    "    else:\n",
    "        print(f\"VRAM (Inference, MB): Not calculated or error.\")\n",
    "else:\n",
    "    print(f\"GPU метрики не доступны.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "# Сохраняем результаты в словарь для удобства\n",
    "baseline_validation_results = {\n",
    "    \"model_id\": baseline_model_id,\n",
    "    \"method\": \"FP16\",\n",
    "    \"disk_size_mb\": round(baseline_model_disk_size_mb, 2) if not math.isnan(baseline_model_disk_size_mb) else 'N/A',\n",
    "    \"perplexity_gpu\": round(baseline_perplexity_gpu, 2) if torch.cuda.is_available() and not math.isnan(baseline_perplexity_gpu) else 'N/A',\n",
    "    \"inference_time_gpu_ms_50_tokens\": round(baseline_inf_time_gpu_ms, 2) if torch.cuda.is_available() and not math.isnan(baseline_inf_time_gpu_ms) else 'N/A',\n",
    "    \"vram_peak_perplexity_mb\": round(baseline_vram_perplexity_gpu_mb, 2) if torch.cuda.is_available() and not math.isnan(baseline_vram_perplexity_gpu_mb) else 'N/A',\n",
    "    \"vram_peak_inference_mb\": round(baseline_vram_inference_gpu_mb, 2) if torch.cuda.is_available() and not math.isnan(baseline_vram_inference_gpu_mb) else 'N/A',\n",
    "}\n",
    "print(\"\\nBaseline validation results dictionary:\")\n",
    "import json\n",
    "print(json.dumps(baseline_validation_results, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 78398.21it/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating validation split: 100%|██████████| 214670/214670 [00:11<00:00, 18266.06 examples/s]\n",
      "AWQ:   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OPTModel' object has no attribute 'rotary_emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tok.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     25\u001b[39m     tok.pad_token = tok.eos_token\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_cfg\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# ⏳ ~1–2 мин\u001b[39;00m\n\u001b[32m     28\u001b[39m model.save_quantized(awq_dir)\n\u001b[32m     29\u001b[39m tok.save_pretrained(awq_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/model-compression/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/model-compression/lib/python3.12/site-packages/awq/models/base.py:245\u001b[39m, in \u001b[36mBaseAWQForCausalLM.quantize\u001b[39m\u001b[34m(self, tokenizer, quant_config, calib_data, split, text_column, duo_scaling, export_compatible, apply_clip, n_parallel_calib_samples, max_calib_samples, max_calib_seq_len, max_chunk_memory, quantizer_cls, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mself\u001b[39m.quant_config.modules_to_not_convert = \u001b[38;5;28mself\u001b[39m.modules_to_not_convert\n\u001b[32m    224\u001b[39m \u001b[38;5;28mself\u001b[39m.quantizer = quantizer_cls(\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.is_quantized = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/model-compression/lib/python3.12/site-packages/awq/quantize/quantizer.py:162\u001b[39m, in \u001b[36mAwqQuantizer.quantize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Transformers >= 4.48.0 requires positional embeddings should be computed before forward pass\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    159\u001b[39m     transformers.__version__ >= \u001b[33m\"\u001b[39m\u001b[33m4.48.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mposition_embeddings\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    161\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28mself\u001b[39m.module_kwargs[\u001b[33m\"\u001b[39m\u001b[33mposition_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m(\n\u001b[32m    163\u001b[39m         \u001b[38;5;28mself\u001b[39m.inps, \u001b[38;5;28mself\u001b[39m.module_kwargs[\u001b[33m\"\u001b[39m\u001b[33mposition_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (transformers.__version__ >= \u001b[33m\"\u001b[39m\u001b[33m4.48.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module_kwargs.get(\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.module_kwargs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/model-compression/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'OPTModel' object has no attribute 'rotary_emb'"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import time, os, torch, shutil, psutil, gc\n",
    "\n",
    "base_model  = \"facebook/opt-125m\"\n",
    "awq_dir     = \"opt-125m-awq\"\n",
    "\n",
    "quant_cfg = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\" \n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        torch_dtype=\"float16\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        safetensors=False\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(base_model)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model.quantize(tok, quant_config=quant_cfg) \n",
    "model.save_quantized(awq_dir)\n",
    "tok.save_pretrained(awq_dir)\n",
    "print(f\"AWQ-модель сохранена в {awq_dir}, время: {time.time()-tic:.1f} c\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
