{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision psutil humanize gputil scikit-learn torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:16:05.850862Z","iopub.execute_input":"2025-04-01T12:16:05.851234Z","iopub.status.idle":"2025-04-01T12:16:12.562871Z","shell.execute_reply.started":"2025-04-01T12:16:05.851204Z","shell.execute_reply":"2025-04-01T12:16:12.561821Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (4.11.0)\nCollecting gputil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nBuilding wheels for collected packages: gputil\n  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=eed8b9be88b2d37a889548cba133830264dc30dc7622a33cdc51dd9b692f57f4\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built gputil\nInstalling collected packages: gputil\nSuccessfully installed gputil-1.4.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"ifigotin/imagenetmini-1000\")\npath += \"/imagenet-mini\"\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:16:12.564115Z","iopub.execute_input":"2025-04-01T12:16:12.564365Z","iopub.status.idle":"2025-04-01T12:16:13.060759Z","shell.execute_reply.started":"2025-04-01T12:16:12.564323Z","shell.execute_reply":"2025-04-01T12:16:13.060085Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/imagenetmini-1000/imagenet-mini\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport time\nimport timm\nimport psutil\nimport os\nimport GPUtil\nimport numpy as np\nfrom torch.amp import autocast\nfrom torchvision import datasets, transforms\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef print_memory_usage(label=\"\"):\n    \"\"\"–í—ã–≤–æ–¥–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –º–µ–≥–∞–±–∞–π—Ç–∞—Ö\"\"\"\n    if label:\n        print(f\"\\n--- Memory Usage ({label}) ---\")\n    else:\n        print(\"\\n--- Memory Usage ---\")\n        \n    # CPU RAM –≤ MB\n    process = psutil.Process(os.getpid())\n    ram_used = process.memory_info().rss / (1024 ** 2)\n    print(f\"CPU RAM used: {ram_used:.2f} MB\")\n    \n    # GPU VRAM –≤ MB\n    gpus = GPUtil.getGPUs()\n    for gpu in gpus:\n        vram_used = gpu.memoryUsed\n        vram_total = gpu.memoryTotal\n        print(f\"GPU {gpu.id} VRAM: {vram_used:.2f} MB / {vram_total:.2f} MB\")\n\ndef get_model_size(model):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    return (param_size + buffer_size) / 1024**2\n\ndef calculate_metrics(model, device, data_loader):\n    model.eval()\n    all_preds = []\n    all_targets = []\n\n    model.to(device)\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n            \n            all_preds.extend(preds)\n            all_targets.extend(targets.numpy())\n    \n    precision = precision_score(all_targets, all_preds, average='macro')\n    recall = recall_score(all_targets, all_preds, average='macro')\n    f1 = f1_score(all_targets, all_preds, average='macro')\n    \n    return precision, recall, f1\n\ndef load_imagenet_mini(dataset_path, model):\n    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏\n    config = resolve_data_config({}, model=model)\n    transform = create_transform(**config)\n    \n    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n    dataset = datasets.ImageFolder(\n        root=os.path.join(dataset_path, 'val'),\n        transform=transform\n    )\n    \n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=64,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    return data_loader\n\ndef benchmark_model(model, device, input_tensor, num_runs=10, warmup=3, use_amp=False):\n    model = model.to(device)\n    input_tensor = input_tensor.to(device)\n    \n    # Warmup\n    print(f\"\\nüî• Warming up ({warmup} runs) on {device}...\")\n    for _ in range(warmup):\n        with torch.no_grad():\n            if use_amp and device.type == 'cuda':\n                with autocast(device_type='cuda', dtype=torch.float16):\n                    _ = model(input_tensor)\n            else:\n                _ = model(input_tensor)\n    \n    # Benchmark\n    print(f\"üöÄ Benchmarking ({num_runs} runs) on {device}...\")\n    start_time = time.time()\n    \n    for _ in range(num_runs):\n        with torch.no_grad():\n            if use_amp and device.type == 'cuda':\n                with autocast(device_type='cuda', dtype=torch.float16):\n                    _ = model(input_tensor)\n            else:\n                _ = model(input_tensor)\n    \n    total_time = (time.time() - start_time) * 1000\n    avg_time = total_time / num_runs\n    print(f\"‚úÖ Average inference: {avg_time:.2f} ms\")\n    print(f\"üìä Total time: {total_time:.2f} ms | FPS: {1000/(avg_time + 1e-9):.1f}\")\n    \n    return avg_time\n\ndef main(dataset_path=None):\n    device_cpu = torch.device('cpu')\n    device_gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    print(\"\\nüîç Initial memory state:\")\n    print_memory_usage(\"Before loading model\")\n\n    print(\"\\nüì¶ Loading EfficientViT_3b model...\")\n    model = timm.create_model('efficientvit_b3.r256_in1k', pretrained=True)\n    model.eval()\n    print(f\"üìè Model size: {get_model_size(model):.2f} MB\")\n\n    print(\"\\nüîç Initial memory state:\")\n    print_memory_usage(\"Model loaded\")\n    \n    # –ë–µ–Ω—á–º–∞—Ä–∫–∏\n    input_tensor = torch.randn(1, 3, 224, 224)\n    \n    print(\"\\nüß™ Benchmarking on CPU:\")\n    cpu_time = benchmark_model(model, device_cpu, input_tensor)\n    print_memory_usage(\"After CPU benchmark\")\n    \n    if torch.cuda.is_available():\n        print(\"\\nüéÆ Benchmarking on GPU:\")\n        gpu_time = benchmark_model(model, device_gpu, input_tensor)\n        print_memory_usage(\"After GPU test\")\n        \n        print(\"\\n‚ö° Benchmarking with AMP:\")\n        gpu_amp_time = benchmark_model(model, device_gpu, input_tensor, use_amp=True)\n        print_memory_usage(\"After AMP test\")\n        \n        print(\"\\nüìà Results Summary:\")\n        print(f\"| Device | Inference Time (ms) | Speedup vs CPU |\")\n        print(\"|--------|---------------------|----------------|\")\n        print(f\"| CPU    | {cpu_time:19.2f} | {'‚Äî':^15} |\")\n        print(f\"| GPU    | {gpu_time:19.2f} | {cpu_time/gpu_time:^15.1f}x |\")\n        print(f\"| AMP    | {gpu_amp_time:19.2f} | {cpu_time/gpu_amp_time:^15.1f}x |\")\n    else:\n        print(\"\\n‚ùå CUDA not available\")\n        print(f\"‚è±Ô∏è CPU inference time: {cpu_time:.2f} ms\")\n        if dataset_path:\n            print(\"\\nüéØ Quality Metrics (CPU):\")\n            print(f\"Precision: {precision_cpu:.4f}\")\n            print(f\"Recall:    {recall_cpu:.4f}\")\n            print(f\"F1-Score:  {f1_cpu:.4f}\")\n\n    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞\n    if dataset_path:\n        print(\"\\nüìä Loading ImageNetMini dataset...\")\n        data_loader = load_imagenet_mini(dataset_path, model)\n        \n        # print(\"\\nüßÆ Calculating metrics on CPU:\")\n        # precision_cpu, recall_cpu, f1_cpu = calculate_metrics(model, device_cpu, data_loader)\n        \n        if torch.cuda.is_available():\n            print(\"\\nüßÆ Calculating metrics on GPU:\")\n            precision_gpu, recall_gpu, f1_gpu = calculate_metrics(model, device_gpu, data_loader)\n\n            print(\"\\nüéØ Quality Metrics Summary:\")\n            print(\"| Device | Precision | Recall  | F1-Score |\")\n            print(\"|--------|-----------|---------|----------|\")\n            # print(f\"| CPU    | {precision_cpu:.4f}  | {recall_cpu:.4f} | {f1_cpu:.4f}  |\")\n            print(f\"| GPU    | {precision_gpu:.4f}  | {recall_gpu:.4f} | {f1_gpu:.4f}  |\")\n\nif __name__ == \"__main__\":\n    main(dataset_path=path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:16:13.062134Z","iopub.execute_input":"2025-04-01T12:16:13.062444Z","iopub.status.idle":"2025-04-01T12:17:00.074329Z","shell.execute_reply.started":"2025-04-01T12:16:13.062420Z","shell.execute_reply":"2025-04-01T12:17:00.073323Z"}},"outputs":[{"name":"stdout","text":"\nüîç Initial memory state:\n\n--- Memory Usage (Before loading model) ---\nCPU RAM used: 569.39 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n\nüì¶ Loading EfficientViT_3b model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/195M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ebf4a2c8174d0fa8f98644cf993e34"}},"metadata":{}},{"name":"stdout","text":"üìè Model size: 185.75 MB\n\nüîç Initial memory state:\n\n--- Memory Usage (Model loaded) ---\nCPU RAM used: 764.75 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n\nüß™ Benchmarking on CPU:\n\nüî• Warming up (3 runs) on cpu...\nüöÄ Benchmarking (10 runs) on cpu...\n‚úÖ Average inference: 113.27 ms\nüìä Total time: 1132.66 ms | FPS: 8.8\n\n--- Memory Usage (After CPU benchmark) ---\nCPU RAM used: 805.37 MB\nGPU 0 VRAM: 3.00 MB / 16384.00 MB\n\nüéÆ Benchmarking on GPU:\n\nüî• Warming up (3 runs) on cuda...\nüöÄ Benchmarking (10 runs) on cuda...\n‚úÖ Average inference: 21.26 ms\nüìä Total time: 212.57 ms | FPS: 47.0\n\n--- Memory Usage (After GPU test) ---\nCPU RAM used: 934.02 MB\nGPU 0 VRAM: 539.00 MB / 16384.00 MB\n\n‚ö° Benchmarking with AMP:\n\nüî• Warming up (3 runs) on cuda...\nüöÄ Benchmarking (10 runs) on cuda...\n‚úÖ Average inference: 26.91 ms\nüìä Total time: 269.12 ms | FPS: 37.2\n\n--- Memory Usage (After AMP test) ---\nCPU RAM used: 952.39 MB\nGPU 0 VRAM: 615.00 MB / 16384.00 MB\n\nüìà Results Summary:\n| Device | Inference Time (ms) | Speedup vs CPU |\n|--------|---------------------|----------------|\n| CPU    |              113.27 |        ‚Äî        |\n| GPU    |               21.26 |       5.3      x |\n| AMP    |               26.91 |       4.2      x |\n\nüìä Loading ImageNetMini dataset...\n\nüßÆ Calculating metrics on GPU:\n\nüéØ Quality Metrics Summary:\n| Device | Precision | Recall  | F1-Score |\n|--------|-----------|---------|----------|\n| GPU    | 0.8474  | 0.8342 | 0.8239  |\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":3}]}